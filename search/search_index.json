{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Ultralytics YOLOv8 documentation landing page! Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. This page serves as the starting point for exploring the various resources available to help you get started with YOLOv8 and understand its features and capabilities.</p> <p>The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.</p> <p>Whether you are a seasoned machine learning practitioner or new to the field, we hope that the resources on this page will help you get the most out of YOLOv8. For any bugs and feature requests please visit GitHub Issues. For professional support please Contact Us.</p>"},{"location":"#a-brief-history-of-yolo","title":"A Brief History of YOLO","text":"<p>YOLO (You Only Look Once) is a popular object detection and image segmentation model developed by Joseph Redmon and Ali Farhadi at the University of Washington. The first version of YOLO was released in 2015 and quickly gained popularity due to its high speed and accuracy.</p> <p>YOLOv2 was released in 2016 and improved upon the original model by incorporating batch normalization, anchor boxes, and dimension clusters. YOLOv3 was released in 2018 and further improved the model's performance by using a more efficient backbone network, adding a feature pyramid, and making use of focal loss.</p> <p>In 2020, YOLOv4 was released which introduced a number of innovations such as the use of Mosaic data augmentation, a new anchor-free detection head, and a new loss function.</p> <p>In 2021, Ultralytics released YOLOv5, which further improved the model's performance and added new features such as support for panoptic segmentation and object tracking.</p> <p>YOLO has been widely used in a variety of applications, including autonomous vehicles, security and surveillance, and medical imaging. It has also been used to win several competitions, such as the COCO Object Detection Challenge and the DOTA Object Detection Challenge.</p> <p>For more information about the history and development of YOLO, you can refer to the following references:</p> <ul> <li>Redmon, J., &amp; Farhadi, A. (2015). You only look once: Unified, real-time object detection. In Proceedings of the IEEE   conference on computer vision and pattern recognition (pp. 779-788).</li> <li>Redmon, J., &amp; Farhadi, A. (2016). YOLO9000: Better, faster, stronger. In Proceedings</li> </ul>"},{"location":"#ultralytics-yolov8","title":"Ultralytics YOLOv8","text":"<p>Ultralytics YOLOv8 is the latest version of the YOLO object detection and image segmentation model developed by Ultralytics. YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility.</p> <p>One key feature of YOLOv8 is its extensibility. It is designed as a framework that supports all previous versions of YOLO, making it easy to switch between different versions and compare their performance. This makes YOLOv8 an ideal choice for users who want to take advantage of the latest YOLO technology while still being able to use their existing YOLO models.</p> <p>In addition to its extensibility, YOLOv8 includes a number of other innovations that make it an appealing choice for a wide range of object detection and image segmentation tasks. These include a new backbone network, a new anchor-free detection head, and a new loss function. YOLOv8 is also highly efficient and can be run on a variety of hardware platforms, from CPUs to GPUs.</p> <p>Overall, YOLOv8 is a powerful and flexible tool for object detection and image segmentation that offers the best of both worlds: the latest SOTA technology and the ability to use and compare all previous YOLO versions.</p>"},{"location":"app/","title":"Ultralytics HUB App for YOLOv8","text":"<p>Welcome to the Ultralytics HUB app for demonstrating YOLOv5 and YOLOv8 models! In this app, available on the Apple App Store and the  Google Play Store, you will be able to see the power and capabilities of YOLOv5, a state-of-the-art object detection model developed by Ultralytics.</p> <p>To install simply scan the QR code above. The App currently features YOLOv5 models, with YOLOv8 models coming soon.</p> <p>With YOLOv5, you can detect and classify objects in images and videos with high accuracy and speed. The model has been trained on a large dataset and is able to detect a wide range of objects, including cars, pedestrians, and traffic signs.</p> <p>In this app, you will be able to try out YOLOv5 on your own images and videos, and see the model in action. You can also learn more about how YOLOv5 works and how it can be used in real-world applications.</p> <p>We hope you enjoy using YOLOv5 and seeing its capabilities firsthand. Thank you for choosing Ultralytics for your object detection needs!</p>"},{"location":"cfg/","title":"Configuration","text":"<p>YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings and hyperparameters can affect the model's behavior at various stages of the model development process, including training, validation, and prediction.</p> <p>YOLOv8 'yolo' CLI commands use the following syntax:</p> CLI <pre><code>yolo TASK MODE ARGS\n</code></pre> <p>Where:</p> <ul> <li><code>TASK</code> (optional) is one of <code>[detect, segment, classify]</code>. If it is not passed explicitly YOLOv8 will try to guess   the <code>TASK</code> from the model type.</li> <li><code>MODE</code> (required) is one of <code>[train, val, predict, export]</code></li> <li><code>ARGS</code> (optional) are any number of custom <code>arg=value</code> pairs like <code>imgsz=320</code> that override defaults.   For a full list of available <code>ARGS</code> see the Configuration page and <code>defaults.yaml</code>   GitHub source.</li> </ul>"},{"location":"cfg/#tasks","title":"Tasks","text":"<p>YOLO models can be used for a variety of tasks, including detection, segmentation, and classification. These tasks differ in the type of output they produce and the specific problem they are designed to solve.</p> <ul> <li>Detect: Detection tasks involve identifying and localizing objects or regions of interest in an image or video.   YOLO models can be used for object detection tasks by predicting the bounding boxes and class labels of objects in an   image.</li> <li>Segment: Segmentation tasks involve dividing an image or video into regions or pixels that correspond to   different objects or classes. YOLO models can be used for image segmentation tasks by predicting a mask or label for   each pixel in an image.</li> <li>Classify: Classification tasks involve assigning a class label to an input, such as an image or text. YOLO   models can be used for image classification tasks by predicting the class label of an input image.</li> </ul>"},{"location":"cfg/#modes","title":"Modes","text":"<p>YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes include train, val, and predict.</p> <ul> <li>Train: The train mode is used to train the model on a dataset. This mode is typically used during the development   and   testing phase of a model.</li> <li>Val: The val mode is used to evaluate the model's performance on a validation dataset. This mode is typically used   to   tune the model's hyperparameters and detect overfitting.</li> <li>Predict: The predict mode is used to make predictions with the model on new data. This mode is typically used in   production or when deploying the model to users.</li> </ul> Key Value Description task 'detect' inference task, i.e. detect, segment, or classify mode 'train' YOLO mode, i.e. train, val, predict, or export resume False resume training from last checkpoint or custom checkpoint if passed as resume=path/to/best.pt model null path to model file, i.e. yolov8n.pt, yolov8n.yaml data null path to data file, i.e. i.e. coco128.yaml"},{"location":"cfg/#training","title":"Training","text":"<p>Training settings for YOLO models refer to the various hyperparameters and configurations used to train the model on a dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO training settings include the batch size, learning rate, momentum, and weight decay. Other factors that may affect the training process include the choice of optimizer, the choice of loss function, and the size and composition of the training dataset. It is important to carefully tune and experiment with these settings to achieve the best possible performance for a given task.</p> Key Value Description model null path to model file, i.e. yolov8n.pt, yolov8n.yaml data null path to data file, i.e. i.e. coco128.yaml epochs 100 number of epochs to train for patience 50 epochs to wait for no observable improvement for early stopping of training batch 16 number of images per batch (-1 for AutoBatch) imgsz 640 size of input images as integer or w,h save True save train checkpoints and predict results cache False True/ram, disk or False. Use cache for data loading device null device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu workers 8 number of worker threads for data loading (per RANK if DDP) project null project name name null experiment name exist_ok False whether to overwrite existing experiment pretrained False whether to use a pretrained model optimizer 'SGD' optimizer to use, choices=['SGD', 'Adam', 'AdamW', 'RMSProp'] verbose False whether to print verbose output seed 0 random seed for reproducibility deterministic True whether to enable deterministic mode single_cls False train multi-class data as single-class image_weights False use weighted image selection for training rect False support rectangular training cos_lr False use cosine learning rate scheduler close_mosaic 10 disable mosaic augmentation for final 10 epochs resume False resume training from last checkpoint lr0 0.01 initial learning rate (i.e. SGD=1E-2, Adam=1E-3) lrf 0.01 final learning rate (lr0 * lrf) momentum 0.937 SGD momentum/Adam beta1 weight_decay 0.0005 optimizer weight decay 5e-4 warmup_epochs 3.0 warmup epochs (fractions ok) warmup_momentum 0.8 warmup initial momentum warmup_bias_lr 0.1 warmup initial bias lr box 7.5 box loss gain cls 0.5 cls loss gain (scale with pixels) dfl 1.5 dfl loss gain fl_gamma 0.0 focal loss gamma (efficientDet default gamma=1.5) label_smoothing 0.0 label smoothing (fraction) nbs 64 nominal batch size overlap_mask True masks should overlap during training (segment train only) mask_ratio 4 mask downsample ratio (segment train only) dropout 0.0 use dropout regularization (classify train only) val True validate/test during training min_memory False minimize memory footprint loss function, choices=[False, True, ]"},{"location":"cfg/#prediction","title":"Prediction","text":"<p>Prediction settings for YOLO models refer to the various hyperparameters and configurations used to make predictions with the model on new data. These settings can affect the model's performance, speed, and accuracy. Some common YOLO prediction settings include the confidence threshold, non-maximum suppression (NMS) threshold, and the number of classes to consider. Other factors that may affect the prediction process include the size and format of the input data, the presence of additional features such as masks or multiple labels per box, and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to achieve the best possible performance for a given task.</p> Key Value Description source 'ultralytics/assets' source directory for images or videos conf 0.25 object confidence threshold for detection iou 0.7 intersection over union (IoU) threshold for NMS half False use half precision (FP16) device null device to run on, i.e. cuda device=0/1/2/3 or device=cpu show False show results if possible save_txt False save results as .txt file save_conf False save results with confidence scores save_crop False save cropped images with results hide_labels False hide labels hide_conf False hide confidence scores max_det 300 maximum number of detections per image vid_stride False video frame-rate stride line_thickness 3 bounding box thickness (pixels) visualize False visualize model features augment False apply image augmentation to prediction sources agnostic_nms False class-agnostic NMS retina_masks False use high-resolution segmentation masks classes null filter results by class, i.e. class=0, or class=[0,2,3] box True Show boxes in segmentation predictions"},{"location":"cfg/#validation","title":"Validation","text":"<p>Validation settings for YOLO models refer to the various hyperparameters and configurations used to evaluate the model's performance on a validation dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO validation settings include the batch size, the frequency with which validation is performed during training, and the metrics used to evaluate the model's performance. Other factors that may affect the validation process include the size and composition of the validation dataset and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to ensure that the model is performing well on the validation dataset and to detect and prevent overfitting.</p> Key Value Description save_json False save results to JSON file save_hybrid False save hybrid version of labels (labels + additional predictions) conf 0.001 object confidence threshold for detection iou 0.6 intersection over union (IoU) threshold for NMS max_det 300 maximum number of detections per image half True use half precision (FP16) device null device to run on, i.e. cuda device=0/1/2/3 or device=cpu dnn False use OpenCV DNN for ONNX inference plots False show plots during training rect False support rectangular evaluation"},{"location":"cfg/#export","title":"Export","text":"<p>Export settings for YOLO models refer to the various configurations and options used to save or export the model for use in other environments or platforms. These settings can affect the model's performance, size, and compatibility with different systems. Some common YOLO export settings include the format of the exported model file (e.g. ONNX, TensorFlow SavedModel), the device on which the model will be run (e.g. CPU, GPU), and the presence of additional features such as masks or multiple labels per box. Other factors that may affect the export process include the specific task the model is being used for and the requirements or constraints of the target environment or platform. It is important to carefully consider and configure these settings to ensure that the exported model is optimized for the intended use case and can be used effectively in the target environment.</p>"},{"location":"cfg/#augmentation","title":"Augmentation","text":"<p>Augmentation settings for YOLO models refer to the various transformations and modifications applied to the training data to increase the diversity and size of the dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO augmentation settings include the type and intensity of the transformations applied (e.g. random flips, rotations, cropping, color changes), the probability with which each transformation is applied, and the presence of additional features such as masks or multiple labels per box. Other factors that may affect the augmentation process include the size and composition of the original dataset and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to ensure that the augmented dataset is diverse and representative enough to train a high-performing model.</p> Key Value Description hsv_h 0.015 image HSV-Hue augmentation (fraction) hsv_s 0.7 image HSV-Saturation augmentation (fraction) hsv_v 0.4 image HSV-Value augmentation (fraction) degrees 0.0 image rotation (+/- deg) translate 0.1 image translation (+/- fraction) scale 0.5 image scale (+/- gain) shear 0.0 image shear (+/- deg) perspective 0.0 image perspective (+/- fraction), range 0-0.001 flipud 0.0 image flip up-down (probability) fliplr 0.5 image flip left-right (probability) mosaic 1.0 image mosaic (probability) mixup 0.0 image mixup (probability) copy_paste 0.0 segment copy-paste (probability)"},{"location":"cfg/#logging-checkpoints-plotting-and-file-management","title":"Logging, checkpoints, plotting and file management","text":"<p>Logging, checkpoints, plotting, and file management are important considerations when training a YOLO model.</p> <ul> <li>Logging: It is often helpful to log various metrics and statistics during training to track the model's progress and   diagnose any issues that may arise. This can be done using a logging library such as TensorBoard or by writing log   messages to a file.</li> <li>Checkpoints: It is a good practice to save checkpoints of the model at regular intervals during training. This allows   you to resume training from a previous point if the training process is interrupted or if you want to experiment with   different training configurations.</li> <li>Plotting: Visualizing the model's performance and training progress can be helpful for understanding how the model is   behaving and identifying potential issues. This can be done using a plotting library such as matplotlib or by   generating plots using a logging library such as TensorBoard.</li> <li>File management: Managing the various files generated during the training process, such as model checkpoints, log   files, and plots, can be challenging. It is important to have a clear and organized file structure to keep track of   these files and make it easy to access and analyze them as needed.</li> </ul> <p>Effective logging, checkpointing, plotting, and file management can help you keep track of the model's progress and make it easier to debug and optimize the training process.</p> Key Value Description project 'runs' project name name 'exp' experiment name. <code>exp</code> gets automatically incremented if not specified, i.e, <code>exp</code>, <code>exp2</code> ... exist_ok False whether to overwrite existing experiment plots False save plots during train/val save False save train checkpoints and predict results"},{"location":"cli/","title":"CLI","text":"<p>The YOLO Command Line Interface (CLI) is the easiest way to get started training, validating, predicting and exporting YOLOv8 models.</p> <p>The <code>yolo</code> command is used for all actions:</p> CLI <pre><code>yolo TASK MODE ARGS\n</code></pre> <p>Where:</p> <ul> <li><code>TASK</code> (optional) is one of <code>[detect, segment, classify]</code>. If it is not passed explicitly YOLOv8 will try to guess   the <code>TASK</code> from the model type.</li> <li><code>MODE</code> (required) is one of <code>[train, val, predict, export]</code></li> <li><code>ARGS</code> (optional) are any number of custom <code>arg=value</code> pairs like <code>imgsz=320</code> that override defaults.   For a full list of available <code>ARGS</code> see the Configuration page and <code>defaults.yaml</code>   GitHub source.</li> </ul> <p>Note: Arguments MUST be passed as <code>arg=val</code> with an equals sign and a space between <code>arg=val</code> pairs</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul>"},{"location":"cli/#train","title":"Train","text":"<p>Train YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see the Configuration page.</p> <pre><code>yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\nyolo detect train resume model=last.pt  # resume training\n</code></pre>"},{"location":"cli/#val","title":"Val","text":"<p>Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> <pre><code>yolo detect val model=yolov8n.pt  # val official model\nyolo detect val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"cli/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n model to run predictions on images.</p> <pre><code>yolo detect predict model=yolov8n.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with official model\nyolo detect predict model=path/to/best.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with custom model\n</code></pre>"},{"location":"cli/#export","title":"Export","text":"<p>Export a YOLOv8n model to a different format like ONNX, CoreML, etc.</p> <pre><code>yolo export model=yolov8n.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8 export formats include:</p> Format <code>format=</code> Model PyTorch - <code>yolov8n.pt</code> TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> CoreML <code>coreml</code> <code>yolov8n.mlmodel</code> TensorFlow SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> TensorFlow GraphDef <code>pb</code> <code>yolov8n.pb</code> TensorFlow Lite <code>tflite</code> <code>yolov8n.tflite</code> TensorFlow Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> TensorFlow.js <code>tfjs</code> <code>yolov8n_web_model/</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code>"},{"location":"cli/#overriding-default-arguments","title":"Overriding default arguments","text":"<p>Default arguments can be overriden by simply passing them as arguments in the CLI in <code>arg=value</code> pairs.</p> Example 1Example 2Example 3 <p>Train a detection model for <code>10 epochs</code> with <code>learning_rate</code> of <code>0.01</code> <pre><code>yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Predict a YouTube video using a pretrained segmentation model at image size 320: <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n</code></pre></p> <p>Validate a pretrained detection model at batch-size 1 and image size 640: <pre><code>yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p>"},{"location":"cli/#overriding-default-config-file","title":"Overriding default config file","text":"<p>You can override the <code>default.yaml</code> config file entirely by passing a new file with the <code>cfg</code> arguments, i.e. <code>cfg=custom.yaml</code>.</p> <p>To do this first create a copy of <code>default.yaml</code> in your current working dir with the <code>yolo copy-cfg</code> command.</p> <p>This will create <code>default_copy.yaml</code>, which you can then pass as <code>cfg=default_copy.yaml</code> along with any additional args, like <code>imgsz=320</code> in this example:</p> CLI <pre><code>yolo copy-cfg\nyolo cfg=default_copy.yaml imgsz=320\n</code></pre>"},{"location":"engine/","title":"Customization Guide","text":"<p>Both the Ultralytics YOLO command-line and python interfaces are simply a high-level abstraction on the base engine executors. Let's take a look at the Trainer engine.</p>"},{"location":"engine/#basetrainer","title":"BaseTrainer","text":"<p>BaseTrainer contains the generic boilerplate training routine. It can be customized for any task based over overriding the required functions or operations as long the as correct formats are followed. For example, you can support your own custom model and dataloader by just overriding these functions:</p> <ul> <li><code>get_model(cfg, weights)</code> - The function that builds the model to be trained</li> <li><code>get_dataloder()</code> - The function that builds the dataloader   More details and source code can be found in <code>BaseTrainer</code> Reference</li> </ul>"},{"location":"engine/#detectiontrainer","title":"DetectionTrainer","text":"<p>Here's how you can use the YOLOv8 <code>DetectionTrainer</code> and customize it.</p> <pre><code>from ultralytics.yolo.v8.detect import DetectionTrainer\ntrainer = DetectionTrainer(overrides={...})\ntrainer.train()\ntrained_model = trainer.best  # get best model\n</code></pre>"},{"location":"engine/#customizing-the-detectiontrainer","title":"Customizing the DetectionTrainer","text":"<p>Let's customize the trainer to train a custom detection model that is not supported directly. You can do this by simply overloading the existing the <code>get_model</code> functionality:</p> <pre><code>from ultralytics.yolo.v8.detect import DetectionTrainer\nclass CustomTrainer(DetectionTrainer):\ndef get_model(self, cfg, weights):\n...\ntrainer = CustomTrainer(overrides={...})\ntrainer.train()\n</code></pre> <p>You now realize that you need to customize the trainer further to:</p> <ul> <li>Customize the <code>loss function</code>.</li> <li>Add <code>callback</code> that uploads model to your Google Drive after every 10 <code>epochs</code>   Here's how you can do it:</li> </ul> <pre><code>from ultralytics.yolo.v8.detect import DetectionTrainer\nclass CustomTrainer(DetectionTrainer):\ndef get_model(self, cfg, weights):\n...\ndef criterion(self, preds, batch):\n# get ground truth\nimgs = batch[\"imgs\"]\nbboxes = batch[\"bboxes\"]\n...\nreturn loss, loss_items  # see Reference-&gt; Trainer for details on the expected format\n# callback to upload model weights\ndef log_model(trainer):\nlast_weight_path = trainer.last\n...\ntrainer = CustomTrainer(overrides={...})\ntrainer.add_callback(\"on_train_epoch_end\", log_model)  # Adds to existing callback\ntrainer.train()\n</code></pre> <p>To know more about Callback triggering events and entry point, checkout our Callbacks guide # TODO</p>"},{"location":"engine/#other-engine-components","title":"Other engine components","text":"<p>There are other components that can be customized similarly like <code>Validators</code> and <code>Predictors</code> See Reference section for more information on these.</p>"},{"location":"hub/","title":"Ultralytics HUB","text":"<p>Ultralytics HUB is a new no-code online tool developed by Ultralytics, the creators of the popular YOLOv5 object detection and image segmentation models. With Ultralytics HUB, users can easily train and deploy YOLO models without any coding or technical expertise.</p> <p>Ultralytics HUB is designed to be user-friendly and intuitive, with a drag-and-drop interface that allows users to easily upload their data and select their model configurations. It also offers a range of pre-trained models and templates to choose from, making it easy for users to get started with training their own models. Once a model is trained, it can be easily deployed and used for real-time object detection and image segmentation tasks. Overall, Ultralytics HUB is an essential tool for anyone looking to use YOLO for their object detection and image segmentation projects.</p> <p>Get started now and experience the power and simplicity of Ultralytics HUB for yourself. Sign up for a free account and start building, training, and deploying YOLOv5 and YOLOv8 models today.</p>"},{"location":"hub/#1-upload-a-dataset","title":"1. Upload a Dataset","text":"<p>Ultralytics HUB datasets are just like YOLOv5 \ud83d\ude80 datasets, they use the same structure and the same label formats to keep everything simple.</p> <p>When you upload a dataset to Ultralytics HUB, make sure to place your dataset YAML inside the dataset root directory as in the example shown below, and then zip for upload to https://hub.ultralytics.com/. Your dataset YAML, directory and zip should all share the same name. For example, if your dataset is called 'coco6' as in our example ultralytics/hub/coco6.zip, then you should have a coco6.yaml inside your coco6/ directory, which should zip to create coco6.zip for upload:</p> <pre><code>zip -r coco6.zip coco6\n</code></pre> <p>The example coco6.zip dataset in this repository can be downloaded and unzipped to see exactly how to structure your custom dataset.</p> <p> </p> <p>The dataset YAML is the same standard YOLOv5 YAML format. See the YOLOv5 Train Custom Data tutorial for full details.</p> <pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath:  # dataset root dir (leave empty for HUB)\ntrain: images/train  # train images (relative to 'path') 8 images\nval: images/val  # val images (relative to 'path') 8 images\ntest:  # test images (optional)\n# Classes\nnames:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n...\n</code></pre> <p>After zipping your dataset, sign in to Ultralytics HUB and click the Datasets tab. Click 'Upload Dataset' to upload, scan and visualize your new dataset before training new YOLOv5 models on it!</p> <p></p>"},{"location":"hub/#2-train-a-model","title":"2. Train a Model","text":"<p>Connect to the Ultralytics HUB notebook and use your model API key to begin training! </p> <p> </p>"},{"location":"hub/#3-deploy-to-real-world","title":"3. Deploy to Real World","text":"<p>Export your model to 13 different formats, including TensorFlow, ONNX, OpenVINO, CoreML, Paddle and many others. Run models directly on your iOS or  Android mobile device by downloading  the Ultralytics App!</p>"},{"location":"hub/#issues","title":"\u2753 Issues","text":"<p>If you are a new Ultralytics HUB user and have questions or comments, you are in the right place! Please raise a New Issue and let us know what we can do to make your life better \ud83d\ude03!</p>"},{"location":"predict/","title":"Predict","text":"<p>Inference or prediction of a task returns a list of <code>Results</code> objects. Alternatively, in the streaming mode, it returns a generator of <code>Results</code> objects which is memory efficient. Streaming mode can be enabled by passing <code>stream=True</code> in predictor's call method.</p> <p>Predict</p> Getting a List <pre><code>inputs = [img, img]  # list of np arrays\nresults = model(inputs)  # List of Results objects\nfor result in results:\nboxes = result.boxes  # Boxes object for bbox outputs\nmasks = result.masks  # Masks object for segmenation masks outputs\nprobs = result.probs  # Class probabilities for classification outputs\n</code></pre> Getting a Generator <pre><code>inputs = [img, img]  # list of numpy arrays\nresults = model(inputs, stream=True)  # generator of Results objects\nfor r in results:\nboxes = r.boxes  # Boxes object for bbox outputs\nmasks = r.masks  # Masks object for segmenation masks outputs\nprobs = r.probs  # Class probabilities for classification outputs\n</code></pre>"},{"location":"predict/#working-with-results","title":"Working with Results","text":"<p>Results object consists of these component objects:</p> <ul> <li><code>Results.boxes</code> : <code>Boxes</code> object with properties and methods for manipulating bboxes</li> <li><code>Results.masks</code> : <code>Masks</code> object used to index masks or to get segment coordinates.</li> <li><code>Results.prob</code>  : <code>torch.Tensor</code> containing the class probabilities/logits.</li> </ul> <p>Each result is composed of torch.Tensor by default, in which you can easily use following functionality:</p> <pre><code>results = results.cuda()\nresults = results.cpu()\nresults = results.to(\"cpu\")\nresults = results.numpy()\n</code></pre>"},{"location":"predict/#boxes","title":"Boxes","text":"<p><code>Boxes</code> object can be used index, manipulate and convert bboxes to different formats. The box format conversion operations are cached, which means they're only calculated once per object and those values are reused for future calls.</p> <ul> <li>Indexing a <code>Boxes</code> objects returns a <code>Boxes</code> object</li> </ul> <pre><code>results = model(inputs)\nboxes = results[0].boxes\nbox = boxes[0]  # returns one box\nbox.xyxy \n</code></pre> <ul> <li>Properties and conversions</li> </ul> <pre><code>boxes.xyxy  # box with xyxy format, (N, 4)\nboxes.xywh  # box with xywh format, (N, 4)\nboxes.xyxyn  # box with xyxy format but normalized, (N, 4)\nboxes.xywhn  # box with xywh format but normalized, (N, 4)\nboxes.conf  # confidence score, (N, 1)\nboxes.cls  # cls, (N, 1)\nboxes.data  # raw bboxes tensor, (N, 6) or boxes.boxes .\n</code></pre>"},{"location":"predict/#masks","title":"Masks","text":"<p><code>Masks</code> object can be used index, manipulate and convert masks to segments. The segment conversion operation is cached.</p> <pre><code>results = model(inputs)\nmasks = results[0].masks  # Masks object\nmasks.segments  # bounding coordinates of masks, List[segment] * N\nmasks.data  # raw masks tensor, (N, H, W) or masks.masks \n</code></pre>"},{"location":"predict/#probs","title":"probs","text":"<p><code>probs</code> attribute of <code>Results</code> class is a <code>Tensor</code> containing class probabilities of a classification operation.</p> <pre><code>results = model(inputs)\nresults[0].probs  # cls prob, (num_class, )\n</code></pre> <p>Class reference documentation for <code>Results</code> module and its components can be found here</p>"},{"location":"python/","title":"Python","text":"<p>The simplest way of simply using YOLOv8 directly in a Python environment.</p> <p>Train</p> From pretrained(recommanded)From scratchResume <pre><code>from ultralytics import YOLO\nmodel = YOLO(\"yolov8n.pt\") # pass any model type\nmodel.train(epochs=5)\n</code></pre> <pre><code>from ultralytics import YOLO\nmodel = YOLO(\"yolov8n.yaml\")\nmodel.train(data=\"coco128.yaml\", epochs=5)\n</code></pre> <pre><code># TODO: Resume feature is under development and should be released soon.\nmodel = YOLO(\"last.pt\")\nmodel.train(resume=True)\n</code></pre> <p>Val</p> Val after trainingVal independently <pre><code>  from ultralytics import YOLO\nmodel = YOLO(\"yolov8n.yaml\")\nmodel.train(data=\"coco128.yaml\", epochs=5)\nmodel.val()  # It'll automatically evaluate the data you trained.\n</code></pre> <pre><code>  from ultralytics import YOLO\nmodel = YOLO(\"model.pt\")\n# It'll use the data yaml file in model.pt if you don't set data.\nmodel.val()\n# or you can set the data you want to val\nmodel.val(data=\"coco128.yaml\")\n</code></pre> <p>Predict</p> From sourceResults usage <pre><code>from ultralytics import YOLO\nfrom PIL import Image\nimport cv2\nmodel = YOLO(\"model.pt\")\n# accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\nresults = model.predict(source=\"0\")\nresults = model.predict(source=\"folder\", show=True) # Display preds. Accepts all YOLO predict arguments\n# from PIL\nim1 = Image.open(\"bus.jpg\")\nresults = model.predict(source=im1, save=True)  # save plotted images\n# from ndarray\nim2 = cv2.imread(\"bus.jpg\")\nresults = model.predict(source=im2, save=True, save_txt=True)  # save predictions as labels\n# from list of PIL/ndarray\nresults = model.predict(source=[im1, im2])\n</code></pre> <pre><code># results would be a list of Results object including all the predictions by default\n# but be careful as it could occupy a lot memory when there're many images, \n# especially the task is segmentation.\n# 1. return as a list\nresults = model.predict(source=\"folder\")\n# results would be a generator which is more friendly to memory by setting stream=True\n# 2. return as a generator\nresults = model.predict(source=0, stream=True)\nfor result in results:\n# detection\nresult.boxes.xyxy   # box with xyxy format, (N, 4)\nresult.boxes.xywh   # box with xywh format, (N, 4)\nresult.boxes.xyxyn  # box with xyxy format but normalized, (N, 4)\nresult.boxes.xywhn  # box with xywh format but normalized, (N, 4)\nresult.boxes.conf   # confidence score, (N, 1)\nresult.boxes.cls    # cls, (N, 1)\n# segmentation\nresult.masks.masks     # masks, (N, H, W)\nresult.masks.segments  # bounding coordinates of masks, List[segment] * N\n# classification\nresult.probs     # cls prob, (num_class, )\n# Each result is composed of torch.Tensor by default, \n# in which you can easily use following functionality:\nresult = result.cuda()\nresult = result.cpu()\nresult = result.to(\"cpu\")\nresult = result.numpy()\n</code></pre> <p>Export and Deployment</p> Export, Fuse &amp; infoDeployment <pre><code>from ultralytics import YOLO\nmodel = YOLO(\"model.pt\")\nmodel.fuse()  \nmodel.info(verbose=True)  # Print model information\nmodel.export(format=)  # TODO: \n</code></pre> <p>More functionality coming soon</p> <p>To know more about using <code>YOLO</code> models, refer Model class Reference</p> <p>Model reference</p>"},{"location":"python/#using-trainers","title":"Using Trainers","text":"<p><code>YOLO</code> model class is a high-level wrapper on the Trainer classes. Each YOLO task has its own trainer that inherits from <code>BaseTrainer</code>.</p> <p>Detection Trainer Example<pre><code>from ultralytics.yolo import v8 import DetectionTrainer, DetectionValidator, DetectionPredictor\n# trainer\ntrainer = DetectionTrainer(overrides={})\ntrainer.train()\ntrained_model = trainer.best\n# Validator\nval = DetectionValidator(args=...)\nval(model=trained_model)\n# predictor\npred = DetectionPredictor(overrides={})\npred(source=SOURCE, model=trained_model)\n# resume from last weight\noverrides[\"resume\"] = trainer.last\ntrainer = detect.DetectionTrainer(overrides=overrides)\n</code></pre> </p> <p>You can easily customize Trainers to support custom tasks or explore R&amp;D ideas. Learn more about Customizing <code>Trainers</code>, <code>Validators</code> and <code>Predictors</code> to suit your project needs in the Customization Section.</p> <p>Customization tutorials</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#install","title":"Install","text":"<p>Install YOLOv8 via the <code>ultralytics</code> pip package for the latest stable release or by cloning the https://github.com/ultralytics/ultralytics repository for the most up-to-date version.</p> <p>Pip install method (recommended)</p> <pre><code>pip install ultralytics\n</code></pre> <p>Git clone method (for development)</p> <p><pre><code>git clone https://github.com/ultralytics/ultralytics\ncd ultralytics\npip install -e '.[dev]'\n</code></pre> See contributing section to know more about contributing to the project</p>"},{"location":"quickstart/#use-with-cli","title":"Use with CLI","text":"<p>The YOLO command line interface (CLI) lets you simply train, validate or infer models on various tasks and versions. CLI requires no customization or code. You can simply run all tasks from the terminal with the <code>yolo</code> command.</p> <p>Example</p> SyntaxExample trainingExample Multi-GPU training <pre><code>yolo task=detect    mode=train    model=yolov8n.yaml      args...\n          classify       predict        yolov8n-cls.yaml  args...\n          segment        val            yolov8n-seg.yaml  args...\n                         export         yolov8n.pt        format=onnx  args...\n</code></pre> <pre><code>yolo detect train model=yolov8n.pt data=coco128.yaml device=0\n</code></pre> <pre><code>yolo detect train model=yolov8n.pt data=coco128.yaml device=\\'0,1,2,3\\'\n</code></pre> <p>CLI Guide</p>"},{"location":"quickstart/#use-with-python","title":"Use with Python","text":"<p>Python usage allows users to easily use YOLOv8 inside their Python projects. It provides functions for loading and running the model, as well as for processing the model's output. The interface is designed to be easy to use, so that users can quickly implement object detection in their projects.</p> <p>Overall, the Python interface is a useful tool for anyone looking to incorporate object detection, segmentation or classification into their Python projects using YOLOv8.</p> <p>Example</p> <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\nmodel = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n# Use the model\nresults = model.train(data=\"coco128.yaml\", epochs=3)  # train the model\nresults = model.val()  # evaluate model performance on the validation set\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\nsuccess = model.export(format=\"onnx\")  # export the model to ONNX format\n</code></pre> <p>Python Guide</p>"},{"location":"reference/base_pred/","title":"Predictor","text":"<p>All task Predictors are inherited from <code>BasePredictors</code> class that contains the model validation routine boilerplate. You can override any function of these Trainers to suit your needs.</p>"},{"location":"reference/base_pred/#basepredictor-api-reference","title":"BasePredictor API Reference","text":"<p>BasePredictor</p> <p>A base class for creating predictors.</p> <p>Attributes:</p> Name Type Description <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the predictor.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> <code>done_setup</code> <code>bool</code> <p>Whether the predictor has finished setup.</p> <code>model</code> <code>nn.Module</code> <p>Model used for prediction.</p> <code>data</code> <code>dict</code> <p>Data configuration.</p> <code>device</code> <code>torch.device</code> <p>Device used for prediction.</p> <code>dataset</code> <code>Dataset</code> <p>Dataset used for prediction.</p> <code>vid_path</code> <code>str</code> <p>Path to video file.</p> <code>vid_writer</code> <code>cv2.VideoWriter</code> <p>Video writer for saving video output.</p> <code>annotator</code> <code>Annotator</code> <p>Annotator used for prediction.</p> <code>data_path</code> <code>str</code> <p>Path to data.</p> Source code in <code>ultralytics/yolo/engine/predictor.py</code> <pre><code>class BasePredictor:\n\"\"\"\n    BasePredictor\n    A base class for creating predictors.\n    Attributes:\n        args (SimpleNamespace): Configuration for the predictor.\n        save_dir (Path): Directory to save results.\n        done_setup (bool): Whether the predictor has finished setup.\n        model (nn.Module): Model used for prediction.\n        data (dict): Data configuration.\n        device (torch.device): Device used for prediction.\n        dataset (Dataset): Dataset used for prediction.\n        vid_path (str): Path to video file.\n        vid_writer (cv2.VideoWriter): Video writer for saving video output.\n        annotator (Annotator): Annotator used for prediction.\n        data_path (str): Path to data.\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None):\n\"\"\"\n        Initializes the BasePredictor class.\n        Args:\n            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\nself.args = get_cfg(cfg, overrides)\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f\"{self.args.mode}\"\nself.save_dir = increment_path(Path(project) / name, exist_ok=self.args.exist_ok)\nif self.args.conf is None:\nself.args.conf = 0.25  # default conf=0.25\nself.done_warmup = False\nif self.args.show:\nself.args.show = check_imshow(warn=True)\n# Usable if setup is done\nself.model = None\nself.data = self.args.data  # data_dict\nself.bs = None\nself.imgsz = None\nself.device = None\nself.classes = self.args.classes\nself.dataset = None\nself.vid_path, self.vid_writer = None, None\nself.annotator = None\nself.data_path = None\nself.source_type = None\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\ncallbacks.add_integration_callbacks(self)\ndef preprocess(self, img):\npass\ndef get_annotator(self, img):\nraise NotImplementedError(\"get_annotator function needs to be implemented\")\ndef write_results(self, results, batch, print_string):\nraise NotImplementedError(\"print_results function needs to be implemented\")\ndef postprocess(self, preds, img, orig_img, classes=None):\nreturn preds\n@smart_inference_mode()\ndef __call__(self, source=None, model=None, stream=False):\nif stream:\nreturn self.stream_inference(source, model)\nelse:\nreturn list(self.stream_inference(source, model))  # merge list of Result into one\ndef predict_cli(self, source=None, model=None):\n# Method used for CLI prediction. It uses always generator as outputs as not required by CLI mode\ngen = self.stream_inference(source, model)\nfor _ in gen:  # running CLI inference without accumulating any outputs (do not modify)\npass\ndef setup_source(self, source):\nself.imgsz = check_imgsz(self.args.imgsz, stride=self.model.stride, min_dim=2)  # check image size\nself.dataset = load_inference_source(source=source,\ntransforms=getattr(self.model.model, 'transforms', None),\nimgsz=self.imgsz,\nvid_stride=self.args.vid_stride,\nstride=self.model.stride,\nauto=self.model.pt)\nself.source_type = self.dataset.source_type\nself.vid_path, self.vid_writer = [None] * self.dataset.bs, [None] * self.dataset.bs\ndef stream_inference(self, source=None, model=None):\nself.run_callbacks(\"on_predict_start\")\nif self.args.verbose:\nLOGGER.info(\"\")\n# setup model\nif not self.model:\nself.setup_model(model)\n# setup source every time predict is called\nself.setup_source(source if source is not None else self.args.source)\n# check if save_dir/ label file exists\nif self.args.save or self.args.save_txt:\n(self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n# warmup model\nif not self.done_warmup:\nself.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.bs, 3, *self.imgsz))\nself.done_warmup = True\nself.seen, self.windows, self.dt, self.batch = 0, [], (ops.Profile(), ops.Profile(), ops.Profile()), None\nfor batch in self.dataset:\nself.run_callbacks(\"on_predict_batch_start\")\nself.batch = batch\npath, im, im0s, vid_cap, s = batch\nvisualize = increment_path(self.save_dir / Path(path).stem, mkdir=True) if self.args.visualize else False\nwith self.dt[0]:\nim = self.preprocess(im)\nif len(im.shape) == 3:\nim = im[None]  # expand for batch dim\n# Inference\nwith self.dt[1]:\npreds = self.model(im, augment=self.args.augment, visualize=visualize)\n# postprocess\nwith self.dt[2]:\nself.results = self.postprocess(preds, im, im0s, self.classes)\nfor i in range(len(im)):\np, im0 = (path[i], im0s[i]) if self.source_type.webcam or self.source_type.from_img else (path, im0s)\np = Path(p)\nif self.args.verbose or self.args.save or self.args.save_txt or self.args.show:\ns += self.write_results(i, self.results, (p, im, im0))\nif self.args.show:\nself.show(p)\nif self.args.save:\nself.save_preds(vid_cap, i, str(self.save_dir / p.name))\nself.run_callbacks(\"on_predict_batch_end\")\nyield from self.results\n# Print time (inference-only)\nif self.args.verbose:\nLOGGER.info(f\"{s}{'' if len(preds) else '(no detections), '}{self.dt[1].dt * 1E3:.1f}ms\")\n# Release assets\nif isinstance(self.vid_writer[-1], cv2.VideoWriter):\nself.vid_writer[-1].release()  # release final video writer\n# Print results\nif self.args.verbose and self.seen:\nt = tuple(x.t / self.seen * 1E3 for x in self.dt)  # speeds per image\nLOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms postprocess per image at shape '\nf'{(1, 3, *self.imgsz)}' % t)\nif self.args.save_txt or self.args.save:\nnl = len(list(self.save_dir.glob('labels/*.txt')))  # number of labels\ns = f\"\\n{nl} label{'s' * (nl &gt; 1)} saved to {self.save_dir / 'labels'}\" if self.args.save_txt else ''\nLOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}{s}\")\nself.run_callbacks(\"on_predict_end\")\ndef setup_model(self, model):\ndevice = select_device(self.args.device)\nmodel = model or self.args.model\nself.args.half &amp;= device.type != 'cpu'  # half precision only supported on CUDA\nself.model = AutoBackend(model, device=device, dnn=self.args.dnn, data=self.args.data, fp16=self.args.half)\nself.device = device\nself.model.eval()\ndef show(self, p):\nim0 = self.annotator.result()\nif platform.system() == 'Linux' and p not in self.windows:\nself.windows.append(p)\ncv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\ncv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\ncv2.imshow(str(p), im0)\ncv2.waitKey(1)  # 1 millisecond\ndef save_preds(self, vid_cap, idx, save_path):\nim0 = self.annotator.result()\n# save imgs\nif self.dataset.mode == 'image':\ncv2.imwrite(save_path, im0)\nelse:  # 'video' or 'stream'\nif self.vid_path[idx] != save_path:  # new video\nself.vid_path[idx] = save_path\nif isinstance(self.vid_writer[idx], cv2.VideoWriter):\nself.vid_writer[idx].release()  # release previous video writer\nif vid_cap:  # video\nfps = int(vid_cap.get(cv2.CAP_PROP_FPS))  # integer required, floats produce error in MP4 codec\nw = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nh = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nelse:  # stream\nfps, w, h = 30, im0.shape[1], im0.shape[0]\nsave_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\nself.vid_writer[idx] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\nself.vid_writer[idx].write(im0)\ndef run_callbacks(self, event: str):\nfor callback in self.callbacks.get(event, []):\ncallback(self)\n</code></pre>"},{"location":"reference/base_pred/#ultralytics.yolo.engine.predictor.BasePredictor.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None)</code>","text":"<p>Initializes the BasePredictor class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str</code> <p>Path to a configuration file. Defaults to DEFAULT_CFG.</p> <code>DEFAULT_CFG</code> <code>overrides</code> <code>dict</code> <p>Configuration overrides. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/yolo/engine/predictor.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None):\n\"\"\"\n    Initializes the BasePredictor class.\n    Args:\n        cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\nself.args = get_cfg(cfg, overrides)\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f\"{self.args.mode}\"\nself.save_dir = increment_path(Path(project) / name, exist_ok=self.args.exist_ok)\nif self.args.conf is None:\nself.args.conf = 0.25  # default conf=0.25\nself.done_warmup = False\nif self.args.show:\nself.args.show = check_imshow(warn=True)\n# Usable if setup is done\nself.model = None\nself.data = self.args.data  # data_dict\nself.bs = None\nself.imgsz = None\nself.device = None\nself.classes = self.args.classes\nself.dataset = None\nself.vid_path, self.vid_writer = None, None\nself.annotator = None\nself.data_path = None\nself.source_type = None\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\ncallbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/base_trainer/","title":"Trainer","text":"<p>All task Trainers are inherited from <code>BaseTrainer</code> class that contains the model training and optimization routine boilerplate. You can override any function of these Trainers to suit your needs.</p>"},{"location":"reference/base_trainer/#basetrainer-api-reference","title":"BaseTrainer API Reference","text":"<p>BaseTrainer</p> <p>A base class for creating trainers.</p> <p>Attributes:</p> Name Type Description <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the trainer.</p> <code>check_resume</code> <code>method</code> <p>Method to check if training should be resumed from a saved checkpoint.</p> <code>console</code> <code>logging.Logger</code> <p>Logger instance.</p> <code>validator</code> <code>BaseValidator</code> <p>Validator instance.</p> <code>model</code> <code>nn.Module</code> <p>Model instance.</p> <code>callbacks</code> <code>defaultdict</code> <p>Dictionary of callbacks.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> <code>wdir</code> <code>Path</code> <p>Directory to save weights.</p> <code>last</code> <code>Path</code> <p>Path to last checkpoint.</p> <code>best</code> <code>Path</code> <p>Path to best checkpoint.</p> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>epochs</code> <code>int</code> <p>Number of epochs to train for.</p> <code>start_epoch</code> <code>int</code> <p>Starting epoch for training.</p> <code>device</code> <code>torch.device</code> <p>Device to use for training.</p> <code>amp</code> <code>bool</code> <p>Flag to enable AMP (Automatic Mixed Precision).</p> <code>scaler</code> <code>amp.GradScaler</code> <p>Gradient scaler for AMP.</p> <code>data</code> <code>str</code> <p>Path to data.</p> <code>trainset</code> <code>torch.utils.data.Dataset</code> <p>Training dataset.</p> <code>testset</code> <code>torch.utils.data.Dataset</code> <p>Testing dataset.</p> <code>ema</code> <code>nn.Module</code> <p>EMA (Exponential Moving Average) of the model.</p> <code>lf</code> <code>nn.Module</code> <p>Loss function.</p> <code>scheduler</code> <code>torch.optim.lr_scheduler._LRScheduler</code> <p>Learning rate scheduler.</p> <code>best_fitness</code> <code>float</code> <p>The best fitness value achieved.</p> <code>fitness</code> <code>float</code> <p>Current fitness value.</p> <code>loss</code> <code>float</code> <p>Current loss value.</p> <code>tloss</code> <code>float</code> <p>Total loss value.</p> <code>loss_names</code> <code>list</code> <p>List of loss names.</p> <code>csv</code> <code>Path</code> <p>Path to results CSV file.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>class BaseTrainer:\n\"\"\"\n    BaseTrainer\n    A base class for creating trainers.\n    Attributes:\n        args (SimpleNamespace): Configuration for the trainer.\n        check_resume (method): Method to check if training should be resumed from a saved checkpoint.\n        console (logging.Logger): Logger instance.\n        validator (BaseValidator): Validator instance.\n        model (nn.Module): Model instance.\n        callbacks (defaultdict): Dictionary of callbacks.\n        save_dir (Path): Directory to save results.\n        wdir (Path): Directory to save weights.\n        last (Path): Path to last checkpoint.\n        best (Path): Path to best checkpoint.\n        batch_size (int): Batch size for training.\n        epochs (int): Number of epochs to train for.\n        start_epoch (int): Starting epoch for training.\n        device (torch.device): Device to use for training.\n        amp (bool): Flag to enable AMP (Automatic Mixed Precision).\n        scaler (amp.GradScaler): Gradient scaler for AMP.\n        data (str): Path to data.\n        trainset (torch.utils.data.Dataset): Training dataset.\n        testset (torch.utils.data.Dataset): Testing dataset.\n        ema (nn.Module): EMA (Exponential Moving Average) of the model.\n        lf (nn.Module): Loss function.\n        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.\n        best_fitness (float): The best fitness value achieved.\n        fitness (float): Current fitness value.\n        loss (float): Current loss value.\n        tloss (float): Total loss value.\n        loss_names (list): List of loss names.\n        csv (Path): Path to results CSV file.\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None):\n\"\"\"\n        Initializes the BaseTrainer class.\n        Args:\n            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.device = select_device(self.args.device, self.args.batch)\nself.check_resume()\nself.console = LOGGER\nself.validator = None\nself.model = None\ninit_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)\n# Dirs\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f\"{self.args.mode}\"\nif hasattr(self.args, 'save_dir'):\nself.save_dir = Path(self.args.save_dir)\nelse:\nself.save_dir = Path(\nincrement_path(Path(project) / name, exist_ok=self.args.exist_ok if RANK in {-1, 0} else True))\nself.wdir = self.save_dir / 'weights'  # weights dir\nif RANK in {-1, 0}:\nself.wdir.mkdir(parents=True, exist_ok=True)  # make dir\nself.args.save_dir = str(self.save_dir)\nyaml_save(self.save_dir / 'args.yaml', vars(self.args))  # save run args\nself.last, self.best = self.wdir / 'last.pt', self.wdir / 'best.pt'  # checkpoint paths\nself.batch_size = self.args.batch\nself.epochs = self.args.epochs\nself.start_epoch = 0\nif RANK == -1:\nprint_args(vars(self.args))\n# Device\nself.amp = self.device.type != 'cpu'\nself.scaler = amp.GradScaler(enabled=self.amp)\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading\n# Model and Dataloaders.\nself.model = self.args.model\ntry:\nif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data)\nelif self.args.data.endswith(\".yaml\") or self.args.task in ('detect', 'segment'):\nself.data = check_det_dataset(self.args.data)\nif 'yaml_file' in self.data:\nself.args.data = self.data['yaml_file']  # for validating 'yolo train data=url.zip' usage\nexcept Exception as e:\nraise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' error \u274c {e}\")) from e\nself.trainset, self.testset = self.get_dataset(self.data)\nself.ema = None\n# Optimization utils init\nself.lf = None\nself.scheduler = None\n# Epoch level metrics\nself.best_fitness = None\nself.fitness = None\nself.loss = None\nself.tloss = None\nself.loss_names = ['Loss']\nself.csv = self.save_dir / 'results.csv'\nself.plot_idx = [0, 1, 2]\n# Callbacks\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\nif RANK in {0, -1}:\ncallbacks.add_integration_callbacks(self)\ndef add_callback(self, event: str, callback):\n\"\"\"\n        Appends the given callback.\n        \"\"\"\nself.callbacks[event].append(callback)\ndef set_callback(self, event: str, callback):\n\"\"\"\n        Overrides the existing callbacks with the given callback.\n        \"\"\"\nself.callbacks[event] = [callback]\ndef run_callbacks(self, event: str):\nfor callback in self.callbacks.get(event, []):\ncallback(self)\ndef train(self):\n# Allow device='', device=None on Multi-GPU systems to default to device=0\nif isinstance(self.args.device, int) or self.args.device:  # i.e. device=0 or device=[0,1,2,3]\nworld_size = torch.cuda.device_count()\nelif torch.cuda.is_available():  # i.e. device=None or device=''\nworld_size = 1  # default to device 0\nelse:  # i.e. device='cpu' or 'mps'\nworld_size = 0\n# Run subprocess if DDP training, else train normally\nif world_size &gt; 1 and \"LOCAL_RANK\" not in os.environ:\ncommand = generate_ddp_command(world_size, self)\ntry:\nsubprocess.run(command)\nexcept Exception as e:\nself.console(e)\nfinally:\nddp_cleanup(command, self)\nelse:\nself._do_train(int(os.getenv(\"RANK\", -1)), world_size)\ndef _setup_ddp(self, rank, world_size):\n# os.environ['MASTER_ADDR'] = 'localhost'\n# os.environ['MASTER_PORT'] = '9020'\ntorch.cuda.set_device(rank)\nself.device = torch.device('cuda', rank)\nself.console.info(f\"DDP settings: RANK {rank}, WORLD_SIZE {world_size}, DEVICE {self.device}\")\ndist.init_process_group(\"nccl\" if dist.is_nccl_available() else \"gloo\", rank=rank, world_size=world_size)\ndef _setup_train(self, rank, world_size):\n\"\"\"\n        Builds dataloaders and optimizer on correct rank process.\n        \"\"\"\n# model\nself.run_callbacks(\"on_pretrain_routine_start\")\nckpt = self.setup_model()\nself.model = self.model.to(self.device)\nself.set_model_attributes()\nif world_size &gt; 1:\nself.model = DDP(self.model, device_ids=[rank])\n# Check imgsz\ngs = max(int(self.model.stride.max() if hasattr(self.model, 'stride') else 32), 32)  # grid size (max stride)\nself.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)\n# Batch size\nif self.batch_size == -1:\nif RANK == -1:  # single-GPU only, estimate best batch size\nself.batch_size = check_train_batch_size(self.model, self.args.imgsz, self.amp)\nelse:\nSyntaxError('batch=-1 to use AutoBatch is only available in Single-GPU training. '\n'Please pass a valid batch size value for Multi-GPU DDP training, i.e. batch=16')\n# Optimizer\nself.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing\nself.args.weight_decay *= self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay\nself.optimizer = self.build_optimizer(model=self.model,\nname=self.args.optimizer,\nlr=self.args.lr0,\nmomentum=self.args.momentum,\ndecay=self.args.weight_decay)\n# Scheduler\nif self.args.cos_lr:\nself.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1-&gt;hyp['lrf']\nelse:\nself.lf = lambda x: (1 - x / self.epochs) * (1.0 - self.args.lrf) + self.args.lrf  # linear\nself.scheduler = lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)\nself.scheduler.last_epoch = self.start_epoch - 1  # do not move\nself.stopper, self.stop = EarlyStopping(patience=self.args.patience), False\n# dataloaders\nbatch_size = self.batch_size // world_size if world_size &gt; 1 else self.batch_size\nself.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=rank, mode=\"train\")\nif rank in {0, -1}:\nself.test_loader = self.get_dataloader(self.testset, batch_size=batch_size * 2, rank=-1, mode=\"val\")\nself.validator = self.get_validator()\nmetric_keys = self.validator.metrics.keys + self.label_loss_items(prefix=\"val\")\nself.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))  # TODO: init metrics for plot_results()?\nself.ema = ModelEMA(self.model)\nself.resume_training(ckpt)\nself.run_callbacks(\"on_pretrain_routine_end\")\ndef _do_train(self, rank=-1, world_size=1):\nif world_size &gt; 1:\nself._setup_ddp(rank, world_size)\nself._setup_train(rank, world_size)\nself.epoch_time = None\nself.epoch_time_start = time.time()\nself.train_time_start = time.time()\nnb = len(self.train_loader)  # number of batches\nnw = max(round(self.args.warmup_epochs * nb), 100)  # number of warmup iterations\nlast_opt_step = -1\nself.run_callbacks(\"on_train_start\")\nself.log(f\"Image sizes {self.args.imgsz} train, {self.args.imgsz} val\\n\"\nf'Using {self.train_loader.num_workers * (world_size or 1)} dataloader workers\\n'\nf\"Logging results to {colorstr('bold', self.save_dir)}\\n\"\nf\"Starting training for {self.epochs} epochs...\")\nif self.args.close_mosaic:\nbase_idx = (self.epochs - self.args.close_mosaic) * nb\nself.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])\nfor epoch in range(self.start_epoch, self.epochs):\nself.epoch = epoch\nself.run_callbacks(\"on_train_epoch_start\")\nself.model.train()\nif rank != -1:\nself.train_loader.sampler.set_epoch(epoch)\npbar = enumerate(self.train_loader)\n# Update dataloader attributes (optional)\nif epoch == (self.epochs - self.args.close_mosaic):\nself.console.info(\"Closing dataloader mosaic\")\nif hasattr(self.train_loader.dataset, 'mosaic'):\nself.train_loader.dataset.mosaic = False\nif hasattr(self.train_loader.dataset, 'close_mosaic'):\nself.train_loader.dataset.close_mosaic(hyp=self.args)\nif rank in {-1, 0}:\nself.console.info(self.progress_string())\npbar = tqdm(enumerate(self.train_loader), total=nb, bar_format=TQDM_BAR_FORMAT)\nself.tloss = None\nself.optimizer.zero_grad()\nfor i, batch in pbar:\nself.run_callbacks(\"on_train_batch_start\")\n# Warmup\nni = i + nb * epoch\nif ni &lt;= nw:\nxi = [0, nw]  # x interp\nself.accumulate = max(1, np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round())\nfor j, x in enumerate(self.optimizer.param_groups):\n# bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\nx['lr'] = np.interp(\nni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x['initial_lr'] * self.lf(epoch)])\nif 'momentum' in x:\nx['momentum'] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])\n# Forward\nwith torch.cuda.amp.autocast(self.amp):\nbatch = self.preprocess_batch(batch)\npreds = self.model(batch[\"img\"])\nself.loss, self.loss_items = self.criterion(preds, batch)\nif rank != -1:\nself.loss *= world_size\nself.tloss = (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None \\\n                        else self.loss_items\n# Backward\nself.scaler.scale(self.loss).backward()\n# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\nif ni - last_opt_step &gt;= self.accumulate:\nself.optimizer_step()\nlast_opt_step = ni\n# Log\nmem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\nloss_len = self.tloss.shape[0] if len(self.tloss.size()) else 1\nlosses = self.tloss if loss_len &gt; 1 else torch.unsqueeze(self.tloss, 0)\nif rank in {-1, 0}:\npbar.set_description(\n('%11s' * 2 + '%11.4g' * (2 + loss_len)) %\n(f'{epoch + 1}/{self.epochs}', mem, *losses, batch[\"cls\"].shape[0], batch[\"img\"].shape[-1]))\nself.run_callbacks('on_batch_end')\nif self.args.plots and ni in self.plot_idx:\nself.plot_training_samples(batch, ni)\nself.run_callbacks(\"on_train_batch_end\")\nself.lr = {f\"lr/pg{ir}\": x['lr'] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers\nself.scheduler.step()\nself.run_callbacks(\"on_train_epoch_end\")\nif rank in {-1, 0}:\n# Validation\nself.ema.update_attr(self.model, include=['yaml', 'nc', 'args', 'names', 'stride', 'class_weights'])\nfinal_epoch = (epoch + 1 == self.epochs) or self.stopper.possible_stop\nif self.args.val or final_epoch:\nself.metrics, self.fitness = self.validate()\nself.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})\nself.stop = self.stopper(epoch + 1, self.fitness)\n# Save model\nif self.args.save or (epoch + 1 == self.epochs):\nself.save_model()\nself.run_callbacks('on_model_save')\ntnow = time.time()\nself.epoch_time = tnow - self.epoch_time_start\nself.epoch_time_start = tnow\nself.run_callbacks(\"on_fit_epoch_end\")\n# Early Stopping\nif RANK != -1:  # if DDP training\nbroadcast_list = [self.stop if RANK == 0 else None]\ndist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks\nif RANK != 0:\nself.stop = broadcast_list[0]\nif self.stop:\nbreak  # must break all DDP ranks\nif rank in {-1, 0}:\n# Do final val with best.pt\nself.log(f'\\n{epoch - self.start_epoch + 1} epochs completed in '\nf'{(time.time() - self.train_time_start) / 3600:.3f} hours.')\nself.final_eval()\nif self.args.plots:\nself.plot_metrics()\nself.log(f\"Results saved to {colorstr('bold', self.save_dir)}\")\nself.run_callbacks('on_train_end')\ntorch.cuda.empty_cache()\nself.run_callbacks('teardown')\ndef save_model(self):\nckpt = {\n'epoch': self.epoch,\n'best_fitness': self.best_fitness,\n'model': deepcopy(de_parallel(self.model)).half(),\n'ema': deepcopy(self.ema.ema).half(),\n'updates': self.ema.updates,\n'optimizer': self.optimizer.state_dict(),\n'train_args': vars(self.args),  # save as dict\n'date': datetime.now().isoformat(),\n'version': __version__}\n# Save last, best and delete\ntorch.save(ckpt, self.last)\nif self.best_fitness == self.fitness:\ntorch.save(ckpt, self.best)\ndel ckpt\ndef get_dataset(self, data):\n\"\"\"\n        Get train, val path from data dict if it exists. Returns None if data format is not recognized.\n        \"\"\"\nreturn data[\"train\"], data.get(\"val\") or data.get(\"test\")\ndef setup_model(self):\n\"\"\"\n        load/create/download model for any task.\n        \"\"\"\nif isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\nreturn\nmodel, weights = self.model, None\nckpt = None\nif str(model).endswith(\".pt\"):\nweights, ckpt = attempt_load_one_weight(model)\ncfg = ckpt[\"model\"].yaml\nelse:\ncfg = model\nself.model = self.get_model(cfg=cfg, weights=weights)  # calls Model(cfg, weights)\nreturn ckpt\ndef optimizer_step(self):\nself.scaler.unscale_(self.optimizer)  # unscale gradients\ntorch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradients\nself.scaler.step(self.optimizer)\nself.scaler.update()\nself.optimizer.zero_grad()\nif self.ema:\nself.ema.update(self.model)\ndef preprocess_batch(self, batch):\n\"\"\"\n        Allows custom preprocessing model inputs and ground truths depending on task type.\n        \"\"\"\nreturn batch\ndef validate(self):\n\"\"\"\n        Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.\n        \"\"\"\nmetrics = self.validator(self)\nfitness = metrics.pop(\"fitness\", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\nif not self.best_fitness or self.best_fitness &lt; fitness:\nself.best_fitness = fitness\nreturn metrics, fitness\ndef log(self, text, rank=-1):\n\"\"\"\n        Logs the given text to given ranks process if provided, otherwise logs to all ranks.\n        Args\"\n            text (str): text to log\n            rank (List[Int]): process rank\n        \"\"\"\nif rank in {-1, 0}:\nself.console.info(text)\ndef get_model(self, cfg=None, weights=None, verbose=True):\nraise NotImplementedError(\"This task trainer doesn't support loading cfg files\")\ndef get_validator(self):\nraise NotImplementedError(\"get_validator function not implemented in trainer\")\ndef get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\"):\n\"\"\"\n        Returns dataloader derived from torch.data.Dataloader.\n        \"\"\"\nraise NotImplementedError(\"get_dataloader function not implemented in trainer\")\ndef criterion(self, preds, batch):\n\"\"\"\n        Returns loss and individual loss items as Tensor.\n        \"\"\"\nraise NotImplementedError(\"criterion function not implemented in trainer\")\ndef label_loss_items(self, loss_items=None, prefix=\"train\"):\n\"\"\"\n        Returns a loss dict with labelled training loss items tensor\n        \"\"\"\n# Not needed for classification but necessary for segmentation &amp; detection\nreturn {\"loss\": loss_items} if loss_items is not None else [\"loss\"]\ndef set_model_attributes(self):\n\"\"\"\n        To set or update model parameters before training.\n        \"\"\"\nself.model.names = self.data[\"names\"]\ndef build_targets(self, preds, targets):\npass\ndef progress_string(self):\nreturn \"\"\n# TODO: may need to put these following functions into callback\ndef plot_training_samples(self, batch, ni):\npass\ndef save_metrics(self, metrics):\nkeys, vals = list(metrics.keys()), list(metrics.values())\nn = len(metrics) + 1  # number of cols\ns = '' if self.csv.exists() else (('%23s,' * n % tuple(['epoch'] + keys)).rstrip(',') + '\\n')  # header\nwith open(self.csv, 'a') as f:\nf.write(s + ('%23.5g,' * n % tuple([self.epoch] + vals)).rstrip(',') + '\\n')\ndef plot_metrics(self):\npass\ndef final_eval(self):\nfor f in self.last, self.best:\nif f.exists():\nstrip_optimizer(f)  # strip optimizers\nif f is self.best:\nself.console.info(f'\\nValidating {f}...')\nself.metrics = self.validator(model=f)\nself.metrics.pop('fitness', None)\nself.run_callbacks('on_fit_epoch_end')\ndef check_resume(self):\nresume = self.args.resume\nif resume:\ntry:\nlast = Path(\ncheck_file(resume) if isinstance(resume, (str,\nPath)) and Path(resume).exists() else get_latest_run())\nself.args = get_cfg(attempt_load_weights(last).args)\nself.args.model, resume = str(last), True  # reinstate\nexcept Exception as e:\nraise FileNotFoundError(\"Resume checkpoint not found. Please pass a valid checkpoint to resume from, \"\n\"i.e. 'yolo train resume model=path/to/last.pt'\") from e\nself.resume = resume\ndef resume_training(self, ckpt):\nif ckpt is None:\nreturn\nbest_fitness = 0.0\nstart_epoch = ckpt['epoch'] + 1\nif ckpt['optimizer'] is not None:\nself.optimizer.load_state_dict(ckpt['optimizer'])  # optimizer\nbest_fitness = ckpt['best_fitness']\nif self.ema and ckpt.get('ema'):\nself.ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # EMA\nself.ema.updates = ckpt['updates']\nif self.resume:\nassert start_epoch &gt; 0, \\\n                f'{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\\n' \\\n                f\"Start a new training without --resume, i.e. 'yolo task=... mode=train model={self.args.model}'\"\nLOGGER.info(\nf'Resuming training from {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs')\nif self.epochs &lt; start_epoch:\nLOGGER.info(\nf\"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs.\")\nself.epochs += ckpt['epoch']  # finetune additional epochs\nself.best_fitness = best_fitness\nself.start_epoch = start_epoch\n@staticmethod\ndef build_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):\n\"\"\"\n        Builds an optimizer with the specified parameters and parameter groups.\n        Args:\n            model (nn.Module): model to optimize\n            name (str): name of the optimizer to use\n            lr (float): learning rate\n            momentum (float): momentum\n            decay (float): weight decay\n        Returns:\n            optimizer (torch.optim.Optimizer): the built optimizer\n        \"\"\"\ng = [], [], []  # optimizer parameter groups\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nfor v in model.modules():\nif hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)\ng[2].append(v.bias)\nif isinstance(v, bn):  # weight (no decay)\ng[1].append(v.weight)\nelif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\ng[0].append(v.weight)\nif name == 'Adam':\noptimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # adjust beta1 to momentum\nelif name == 'AdamW':\noptimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\nelif name == 'RMSProp':\noptimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)\nelif name == 'SGD':\noptimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\nelse:\nraise NotImplementedError(f'Optimizer {name} not implemented.')\noptimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay\noptimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)\nLOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}) with parameter groups \"\nf\"{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias\")\nreturn optimizer\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None)</code>","text":"<p>Initializes the BaseTrainer class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str</code> <p>Path to a configuration file. Defaults to DEFAULT_CFG.</p> <code>DEFAULT_CFG</code> <code>overrides</code> <code>dict</code> <p>Configuration overrides. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None):\n\"\"\"\n    Initializes the BaseTrainer class.\n    Args:\n        cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.device = select_device(self.args.device, self.args.batch)\nself.check_resume()\nself.console = LOGGER\nself.validator = None\nself.model = None\ninit_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)\n# Dirs\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f\"{self.args.mode}\"\nif hasattr(self.args, 'save_dir'):\nself.save_dir = Path(self.args.save_dir)\nelse:\nself.save_dir = Path(\nincrement_path(Path(project) / name, exist_ok=self.args.exist_ok if RANK in {-1, 0} else True))\nself.wdir = self.save_dir / 'weights'  # weights dir\nif RANK in {-1, 0}:\nself.wdir.mkdir(parents=True, exist_ok=True)  # make dir\nself.args.save_dir = str(self.save_dir)\nyaml_save(self.save_dir / 'args.yaml', vars(self.args))  # save run args\nself.last, self.best = self.wdir / 'last.pt', self.wdir / 'best.pt'  # checkpoint paths\nself.batch_size = self.args.batch\nself.epochs = self.args.epochs\nself.start_epoch = 0\nif RANK == -1:\nprint_args(vars(self.args))\n# Device\nself.amp = self.device.type != 'cpu'\nself.scaler = amp.GradScaler(enabled=self.amp)\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading\n# Model and Dataloaders.\nself.model = self.args.model\ntry:\nif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data)\nelif self.args.data.endswith(\".yaml\") or self.args.task in ('detect', 'segment'):\nself.data = check_det_dataset(self.args.data)\nif 'yaml_file' in self.data:\nself.args.data = self.data['yaml_file']  # for validating 'yolo train data=url.zip' usage\nexcept Exception as e:\nraise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' error \u274c {e}\")) from e\nself.trainset, self.testset = self.get_dataset(self.data)\nself.ema = None\n# Optimization utils init\nself.lf = None\nself.scheduler = None\n# Epoch level metrics\nself.best_fitness = None\nself.fitness = None\nself.loss = None\nself.tloss = None\nself.loss_names = ['Loss']\nself.csv = self.save_dir / 'results.csv'\nself.plot_idx = [0, 1, 2]\n# Callbacks\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\nif RANK in {0, -1}:\ncallbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.add_callback","title":"<code>add_callback(event, callback)</code>","text":"<p>Appends the given callback.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def add_callback(self, event: str, callback):\n\"\"\"\n    Appends the given callback.\n    \"\"\"\nself.callbacks[event].append(callback)\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.build_optimizer","title":"<code>build_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-05)</code>  <code>staticmethod</code>","text":"<p>Builds an optimizer with the specified parameters and parameter groups.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>model to optimize</p> required <code>name</code> <code>str</code> <p>name of the optimizer to use</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>learning rate</p> <code>0.001</code> <code>momentum</code> <code>float</code> <p>momentum</p> <code>0.9</code> <code>decay</code> <code>float</code> <p>weight decay</p> <code>1e-05</code> <p>Returns:</p> Name Type Description <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>the built optimizer</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>@staticmethod\ndef build_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):\n\"\"\"\n    Builds an optimizer with the specified parameters and parameter groups.\n    Args:\n        model (nn.Module): model to optimize\n        name (str): name of the optimizer to use\n        lr (float): learning rate\n        momentum (float): momentum\n        decay (float): weight decay\n    Returns:\n        optimizer (torch.optim.Optimizer): the built optimizer\n    \"\"\"\ng = [], [], []  # optimizer parameter groups\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nfor v in model.modules():\nif hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)\ng[2].append(v.bias)\nif isinstance(v, bn):  # weight (no decay)\ng[1].append(v.weight)\nelif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\ng[0].append(v.weight)\nif name == 'Adam':\noptimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # adjust beta1 to momentum\nelif name == 'AdamW':\noptimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\nelif name == 'RMSProp':\noptimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)\nelif name == 'SGD':\noptimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\nelse:\nraise NotImplementedError(f'Optimizer {name} not implemented.')\noptimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay\noptimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)\nLOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}) with parameter groups \"\nf\"{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias\")\nreturn optimizer\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.criterion","title":"<code>criterion(preds, batch)</code>","text":"<p>Returns loss and individual loss items as Tensor.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def criterion(self, preds, batch):\n\"\"\"\n    Returns loss and individual loss items as Tensor.\n    \"\"\"\nraise NotImplementedError(\"criterion function not implemented in trainer\")\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size=16, rank=0, mode='train')</code>","text":"<p>Returns dataloader derived from torch.data.Dataloader.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\"):\n\"\"\"\n    Returns dataloader derived from torch.data.Dataloader.\n    \"\"\"\nraise NotImplementedError(\"get_dataloader function not implemented in trainer\")\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.get_dataset","title":"<code>get_dataset(data)</code>","text":"<p>Get train, val path from data dict if it exists. Returns None if data format is not recognized.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def get_dataset(self, data):\n\"\"\"\n    Get train, val path from data dict if it exists. Returns None if data format is not recognized.\n    \"\"\"\nreturn data[\"train\"], data.get(\"val\") or data.get(\"test\")\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.label_loss_items","title":"<code>label_loss_items(loss_items=None, prefix='train')</code>","text":"<p>Returns a loss dict with labelled training loss items tensor</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def label_loss_items(self, loss_items=None, prefix=\"train\"):\n\"\"\"\n    Returns a loss dict with labelled training loss items tensor\n    \"\"\"\n# Not needed for classification but necessary for segmentation &amp; detection\nreturn {\"loss\": loss_items} if loss_items is not None else [\"loss\"]\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.log","title":"<code>log(text, rank=-1)</code>","text":"<p>Logs the given text to given ranks process if provided, otherwise logs to all ranks.</p> <p>Args\"     text (str): text to log     rank (List[Int]): process rank</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def log(self, text, rank=-1):\n\"\"\"\n    Logs the given text to given ranks process if provided, otherwise logs to all ranks.\n    Args\"\n        text (str): text to log\n        rank (List[Int]): process rank\n    \"\"\"\nif rank in {-1, 0}:\nself.console.info(text)\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.preprocess_batch","title":"<code>preprocess_batch(batch)</code>","text":"<p>Allows custom preprocessing model inputs and ground truths depending on task type.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def preprocess_batch(self, batch):\n\"\"\"\n    Allows custom preprocessing model inputs and ground truths depending on task type.\n    \"\"\"\nreturn batch\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.set_callback","title":"<code>set_callback(event, callback)</code>","text":"<p>Overrides the existing callbacks with the given callback.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def set_callback(self, event: str, callback):\n\"\"\"\n    Overrides the existing callbacks with the given callback.\n    \"\"\"\nself.callbacks[event] = [callback]\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.set_model_attributes","title":"<code>set_model_attributes()</code>","text":"<p>To set or update model parameters before training.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def set_model_attributes(self):\n\"\"\"\n    To set or update model parameters before training.\n    \"\"\"\nself.model.names = self.data[\"names\"]\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.setup_model","title":"<code>setup_model()</code>","text":"<p>load/create/download model for any task.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def setup_model(self):\n\"\"\"\n    load/create/download model for any task.\n    \"\"\"\nif isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\nreturn\nmodel, weights = self.model, None\nckpt = None\nif str(model).endswith(\".pt\"):\nweights, ckpt = attempt_load_one_weight(model)\ncfg = ckpt[\"model\"].yaml\nelse:\ncfg = model\nself.model = self.get_model(cfg=cfg, weights=weights)  # calls Model(cfg, weights)\nreturn ckpt\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.validate","title":"<code>validate()</code>","text":"<p>Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.</p> Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def validate(self):\n\"\"\"\n    Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.\n    \"\"\"\nmetrics = self.validator(self)\nfitness = metrics.pop(\"fitness\", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\nif not self.best_fitness or self.best_fitness &lt; fitness:\nself.best_fitness = fitness\nreturn metrics, fitness\n</code></pre>"},{"location":"reference/base_val/","title":"Validator","text":"<p>All task Validators are inherited from <code>BaseValidator</code> class that contains the model validation routine boilerplate. You can override any function of these Trainers to suit your needs.</p>"},{"location":"reference/base_val/#basevalidator-api-reference","title":"BaseValidator API Reference","text":"<p>BaseValidator</p> <p>A base class for creating validators.</p> <p>Attributes:</p> Name Type Description <code>dataloader</code> <code>DataLoader</code> <p>Dataloader to use for validation.</p> <code>pbar</code> <code>tqdm</code> <p>Progress bar to update during validation.</p> <code>logger</code> <code>logging.Logger</code> <p>Logger to use for validation.</p> <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the validator.</p> <code>model</code> <code>nn.Module</code> <p>Model to validate.</p> <code>data</code> <code>dict</code> <p>Data dictionary.</p> <code>device</code> <code>torch.device</code> <p>Device to use for validation.</p> <code>batch_i</code> <code>int</code> <p>Current batch index.</p> <code>training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>speed</code> <code>float</code> <p>Batch processing speed in seconds.</p> <code>jdict</code> <code>dict</code> <p>Dictionary to store validation results.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> Source code in <code>ultralytics/yolo/engine/validator.py</code> <pre><code>class BaseValidator:\n\"\"\"\n    BaseValidator\n    A base class for creating validators.\n    Attributes:\n        dataloader (DataLoader): Dataloader to use for validation.\n        pbar (tqdm): Progress bar to update during validation.\n        logger (logging.Logger): Logger to use for validation.\n        args (SimpleNamespace): Configuration for the validator.\n        model (nn.Module): Model to validate.\n        data (dict): Data dictionary.\n        device (torch.device): Device to use for validation.\n        batch_i (int): Current batch index.\n        training (bool): Whether the model is in training mode.\n        speed (float): Batch processing speed in seconds.\n        jdict (dict): Dictionary to store validation results.\n        save_dir (Path): Directory to save results.\n    \"\"\"\ndef __init__(self, dataloader=None, save_dir=None, pbar=None, logger=None, args=None):\n\"\"\"\n        Initializes a BaseValidator instance.\n        Args:\n            dataloader (torch.utils.data.DataLoader): Dataloader to be used for validation.\n            save_dir (Path): Directory to save results.\n            pbar (tqdm.tqdm): Progress bar for displaying progress.\n            logger (logging.Logger): Logger to log messages.\n            args (SimpleNamespace): Configuration for the validator.\n        \"\"\"\nself.dataloader = dataloader\nself.pbar = pbar\nself.logger = logger or LOGGER\nself.args = args or get_cfg(DEFAULT_CFG)\nself.model = None\nself.data = None\nself.device = None\nself.batch_i = None\nself.training = True\nself.speed = None\nself.jdict = None\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f\"{self.args.mode}\"\nself.save_dir = save_dir or increment_path(Path(project) / name,\nexist_ok=self.args.exist_ok if RANK in {-1, 0} else True)\n(self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\nif self.args.conf is None:\nself.args.conf = 0.001  # default conf=0.001\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\n@smart_inference_mode()\ndef __call__(self, trainer=None, model=None):\n\"\"\"\n        Supports validation of a pre-trained model if passed or a model being trained\n        if trainer is passed (trainer gets priority).\n        \"\"\"\nself.training = trainer is not None\nif self.training:\nself.device = trainer.device\nself.data = trainer.data\nmodel = trainer.ema.ema or trainer.model\nself.args.half = self.device.type != 'cpu'  # force FP16 val during training\nmodel = model.half() if self.args.half else model.float()\nself.model = model\nself.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\nself.args.plots = trainer.epoch == trainer.epochs - 1  # always plot final epoch\nmodel.eval()\nelse:\ncallbacks.add_integration_callbacks(self)\nself.run_callbacks('on_val_start')\nassert model is not None, \"Either trainer or model is needed for validation\"\nself.device = select_device(self.args.device, self.args.batch)\nself.args.half &amp;= self.device.type != 'cpu'\nmodel = AutoBackend(model, device=self.device, dnn=self.args.dnn, fp16=self.args.half)\nself.model = model\nstride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\nimgsz = check_imgsz(self.args.imgsz, stride=stride)\nif engine:\nself.args.batch = model.batch_size\nelse:\nself.device = model.device\nif not pt and not jit:\nself.args.batch = 1  # export.py models default to batch-size 1\nself.logger.info(\nf'Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')\nif isinstance(self.args.data, str) and self.args.data.endswith(\".yaml\"):\nself.data = check_det_dataset(self.args.data)\nelif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data)\nelse:\nraise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' not found \u274c\"))\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\nif not pt:\nself.args.rect = False\nself.dataloader = self.dataloader or \\\n                              self.get_dataloader(self.data.get(\"val\") or self.data.get(\"test\"), self.args.batch)\nmodel.eval()\nmodel.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\ndt = Profile(), Profile(), Profile(), Profile()\nn_batches = len(self.dataloader)\ndesc = self.get_desc()\n# NOTE: keeping `not self.training` in tqdm will eliminate pbar after segmentation evaluation during training,\n# which may affect classification task since this arg is in yolov5/classify/val.py.\n# bar = tqdm(self.dataloader, desc, n_batches, not self.training, bar_format=TQDM_BAR_FORMAT)\nbar = tqdm(self.dataloader, desc, n_batches, bar_format=TQDM_BAR_FORMAT)\nself.init_metrics(de_parallel(model))\nself.jdict = []  # empty before each val\nfor batch_i, batch in enumerate(bar):\nself.run_callbacks('on_val_batch_start')\nself.batch_i = batch_i\n# pre-process\nwith dt[0]:\nbatch = self.preprocess(batch)\n# inference\nwith dt[1]:\npreds = model(batch[\"img\"])\n# loss\nwith dt[2]:\nif self.training:\nself.loss += trainer.criterion(preds, batch)[1]\n# pre-process predictions\nwith dt[3]:\npreds = self.postprocess(preds)\nself.update_metrics(preds, batch)\nif self.args.plots and batch_i &lt; 3:\nself.plot_val_samples(batch, batch_i)\nself.plot_predictions(batch, preds, batch_i)\nself.run_callbacks('on_val_batch_end')\nstats = self.get_stats()\nself.check_stats(stats)\nself.print_results()\nself.speed = tuple(x.t / len(self.dataloader.dataset) * 1E3 for x in dt)  # speeds per image\nself.run_callbacks('on_val_end')\nif self.training:\nmodel.float()\nresults = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix=\"val\")}\nreturn {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\nelse:\nself.logger.info('Speed: %.1fms pre-process, %.1fms inference, %.1fms loss, %.1fms post-process per image' %\nself.speed)\nif self.args.save_json and self.jdict:\nwith open(str(self.save_dir / \"predictions.json\"), 'w') as f:\nself.logger.info(f\"Saving {f.name}...\")\njson.dump(self.jdict, f)  # flatten and save\nstats = self.eval_json(stats)  # update stats\nreturn stats\ndef run_callbacks(self, event: str):\nfor callback in self.callbacks.get(event, []):\ncallback(self)\ndef get_dataloader(self, dataset_path, batch_size):\nraise NotImplementedError(\"get_dataloader function not implemented for this validator\")\ndef preprocess(self, batch):\nreturn batch\ndef postprocess(self, preds):\nreturn preds\ndef init_metrics(self, model):\npass\ndef update_metrics(self, preds, batch):\npass\ndef get_stats(self):\nreturn {}\ndef check_stats(self, stats):\npass\ndef print_results(self):\npass\ndef get_desc(self):\npass\n@property\ndef metric_keys(self):\nreturn []\n# TODO: may need to put these following functions into callback\ndef plot_val_samples(self, batch, ni):\npass\ndef plot_predictions(self, batch, preds, ni):\npass\ndef pred_to_json(self, preds, batch):\npass\ndef eval_json(self, stats):\npass\n</code></pre>"},{"location":"reference/base_val/#ultralytics.yolo.engine.validator.BaseValidator.__call__","title":"<code>__call__(trainer=None, model=None)</code>","text":"<p>Supports validation of a pre-trained model if passed or a model being trained if trainer is passed (trainer gets priority).</p> Source code in <code>ultralytics/yolo/engine/validator.py</code> <pre><code>@smart_inference_mode()\ndef __call__(self, trainer=None, model=None):\n\"\"\"\n    Supports validation of a pre-trained model if passed or a model being trained\n    if trainer is passed (trainer gets priority).\n    \"\"\"\nself.training = trainer is not None\nif self.training:\nself.device = trainer.device\nself.data = trainer.data\nmodel = trainer.ema.ema or trainer.model\nself.args.half = self.device.type != 'cpu'  # force FP16 val during training\nmodel = model.half() if self.args.half else model.float()\nself.model = model\nself.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\nself.args.plots = trainer.epoch == trainer.epochs - 1  # always plot final epoch\nmodel.eval()\nelse:\ncallbacks.add_integration_callbacks(self)\nself.run_callbacks('on_val_start')\nassert model is not None, \"Either trainer or model is needed for validation\"\nself.device = select_device(self.args.device, self.args.batch)\nself.args.half &amp;= self.device.type != 'cpu'\nmodel = AutoBackend(model, device=self.device, dnn=self.args.dnn, fp16=self.args.half)\nself.model = model\nstride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\nimgsz = check_imgsz(self.args.imgsz, stride=stride)\nif engine:\nself.args.batch = model.batch_size\nelse:\nself.device = model.device\nif not pt and not jit:\nself.args.batch = 1  # export.py models default to batch-size 1\nself.logger.info(\nf'Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')\nif isinstance(self.args.data, str) and self.args.data.endswith(\".yaml\"):\nself.data = check_det_dataset(self.args.data)\nelif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data)\nelse:\nraise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' not found \u274c\"))\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\nif not pt:\nself.args.rect = False\nself.dataloader = self.dataloader or \\\n                          self.get_dataloader(self.data.get(\"val\") or self.data.get(\"test\"), self.args.batch)\nmodel.eval()\nmodel.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\ndt = Profile(), Profile(), Profile(), Profile()\nn_batches = len(self.dataloader)\ndesc = self.get_desc()\n# NOTE: keeping `not self.training` in tqdm will eliminate pbar after segmentation evaluation during training,\n# which may affect classification task since this arg is in yolov5/classify/val.py.\n# bar = tqdm(self.dataloader, desc, n_batches, not self.training, bar_format=TQDM_BAR_FORMAT)\nbar = tqdm(self.dataloader, desc, n_batches, bar_format=TQDM_BAR_FORMAT)\nself.init_metrics(de_parallel(model))\nself.jdict = []  # empty before each val\nfor batch_i, batch in enumerate(bar):\nself.run_callbacks('on_val_batch_start')\nself.batch_i = batch_i\n# pre-process\nwith dt[0]:\nbatch = self.preprocess(batch)\n# inference\nwith dt[1]:\npreds = model(batch[\"img\"])\n# loss\nwith dt[2]:\nif self.training:\nself.loss += trainer.criterion(preds, batch)[1]\n# pre-process predictions\nwith dt[3]:\npreds = self.postprocess(preds)\nself.update_metrics(preds, batch)\nif self.args.plots and batch_i &lt; 3:\nself.plot_val_samples(batch, batch_i)\nself.plot_predictions(batch, preds, batch_i)\nself.run_callbacks('on_val_batch_end')\nstats = self.get_stats()\nself.check_stats(stats)\nself.print_results()\nself.speed = tuple(x.t / len(self.dataloader.dataset) * 1E3 for x in dt)  # speeds per image\nself.run_callbacks('on_val_end')\nif self.training:\nmodel.float()\nresults = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix=\"val\")}\nreturn {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\nelse:\nself.logger.info('Speed: %.1fms pre-process, %.1fms inference, %.1fms loss, %.1fms post-process per image' %\nself.speed)\nif self.args.save_json and self.jdict:\nwith open(str(self.save_dir / \"predictions.json\"), 'w') as f:\nself.logger.info(f\"Saving {f.name}...\")\njson.dump(self.jdict, f)  # flatten and save\nstats = self.eval_json(stats)  # update stats\nreturn stats\n</code></pre>"},{"location":"reference/base_val/#ultralytics.yolo.engine.validator.BaseValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, logger=None, args=None)</code>","text":"<p>Initializes a BaseValidator instance.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>torch.utils.data.DataLoader</code> <p>Dataloader to be used for validation.</p> <code>None</code> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> <code>None</code> <code>pbar</code> <code>tqdm.tqdm</code> <p>Progress bar for displaying progress.</p> <code>None</code> <code>logger</code> <code>logging.Logger</code> <p>Logger to log messages.</p> <code>None</code> <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the validator.</p> <code>None</code> Source code in <code>ultralytics/yolo/engine/validator.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, logger=None, args=None):\n\"\"\"\n    Initializes a BaseValidator instance.\n    Args:\n        dataloader (torch.utils.data.DataLoader): Dataloader to be used for validation.\n        save_dir (Path): Directory to save results.\n        pbar (tqdm.tqdm): Progress bar for displaying progress.\n        logger (logging.Logger): Logger to log messages.\n        args (SimpleNamespace): Configuration for the validator.\n    \"\"\"\nself.dataloader = dataloader\nself.pbar = pbar\nself.logger = logger or LOGGER\nself.args = args or get_cfg(DEFAULT_CFG)\nself.model = None\nself.data = None\nself.device = None\nself.batch_i = None\nself.training = True\nself.speed = None\nself.jdict = None\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f\"{self.args.mode}\"\nself.save_dir = save_dir or increment_path(Path(project) / name,\nexist_ok=self.args.exist_ok if RANK in {-1, 0} else True)\n(self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\nif self.args.conf is None:\nself.args.conf = 0.001  # default conf=0.001\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\n</code></pre>"},{"location":"reference/exporter/","title":"Exporter","text":""},{"location":"reference/exporter/#exporter-api-reference","title":"Exporter API Reference","text":"<p>Exporter</p> <p>A class for exporting a model.</p> <p>Attributes:</p> Name Type Description <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the exporter.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> Source code in <code>ultralytics/yolo/engine/exporter.py</code> <pre><code>class Exporter:\n\"\"\"\n    Exporter\n    A class for exporting a model.\n    Attributes:\n        args (SimpleNamespace): Configuration for the exporter.\n        save_dir (Path): Directory to save results.\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None):\n\"\"\"\n        Initializes the Exporter class.\n        Args:\n            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\ncallbacks.add_integration_callbacks(self)\n@smart_inference_mode()\ndef __call__(self, model=None):\nself.run_callbacks(\"on_export_start\")\nt = time.time()\nformat = self.args.format.lower()  # to lowercase\nif format in {'tensorrt', 'trt'}:  # engine aliases\nformat = 'engine'\nfmts = tuple(export_formats()['Argument'][1:])  # available export formats\nflags = [x == format for x in fmts]\nif sum(flags) != 1:\nraise ValueError(f\"Invalid export format='{format}'. Valid formats are {fmts}\")\njit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n# Load PyTorch model\nself.device = select_device('cpu' if self.args.device is None else self.args.device)\nif self.args.half:\nif self.device.type == 'cpu' and not coreml and not xml:\nLOGGER.info('half=True only compatible with GPU or CoreML export, i.e. use device=0 or format=coreml')\nself.args.half = False\nassert not self.args.dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic'\n# Checks\nmodel.names = check_class_names(model.names)\n# if self.args.batch == model.args['batch_size']:  # user has not modified training batch_size\nself.args.batch = 1\nself.imgsz = check_imgsz(self.args.imgsz, stride=model.stride, min_dim=2)  # check image size\nif model.task == 'classify':\nself.args.nms = self.args.agnostic_nms = False\nif self.args.optimize:\nassert self.device.type == 'cpu', '--optimize not compatible with cuda devices, i.e. use --device cpu'\n# Input\nim = torch.zeros(self.args.batch, 3, *self.imgsz).to(self.device)\nfile = Path(getattr(model, 'pt_path', None) or getattr(model, 'yaml_file', None) or model.yaml['yaml_file'])\nif file.suffix == '.yaml':\nfile = Path(file.name)\n# Update model\nmodel = deepcopy(model).to(self.device)\nfor p in model.parameters():\np.requires_grad = False\nmodel.eval()\nmodel.float()\nmodel = model.fuse()\nfor k, m in model.named_modules():\nif isinstance(m, (Detect, Segment)):\nm.dynamic = self.args.dynamic\nm.export = True\ny = None\nfor _ in range(2):\ny = model(im)  # dry runs\nif self.args.half and not coreml and not xml:\nim, model = im.half(), model.half()  # to FP16\n# Warnings\nwarnings.filterwarnings('ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\nwarnings.filterwarnings('ignore', category=UserWarning)  # suppress shape prim::Constant missing ONNX warning\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # suppress CoreML np.bool deprecation warning\n# Assign\nself.im = im\nself.model = model\nself.file = file\nself.output_shape = tuple(y.shape) if isinstance(y, torch.Tensor) else tuple(tuple(x.shape) for x in y)\nself.pretty_name = self.file.stem.replace('yolo', 'YOLO')\nself.metadata = {\n'description': f\"Ultralytics {self.pretty_name} model trained on {self.model.args['data']}\",\n'author': 'Ultralytics',\n'license': 'GPL-3.0 https://ultralytics.com/license',\n'version': ultralytics.__version__,\n'stride': int(max(model.stride)),\n'names': model.names}  # model metadata\nLOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from {file} with input shape {tuple(im.shape)} BCHW and \"\nf\"output shape(s) {self.output_shape} ({file_size(file):.1f} MB)\")\n# Exports\nf = [''] * len(fmts)  # exported filenames\nif jit:  # TorchScript\nf[0], _ = self._export_torchscript()\nif engine:  # TensorRT required before ONNX\nf[1], _ = self._export_engine()\nif onnx or xml:  # OpenVINO requires ONNX\nf[2], _ = self._export_onnx()\nif xml:  # OpenVINO\nf[3], _ = self._export_openvino()\nif coreml:  # CoreML\nf[4], _ = self._export_coreml()\nif any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\nLOGGER.warning('WARNING \u26a0\ufe0f YOLOv8 TensorFlow export support is still under development. '\n'Please consider contributing to the effort if you have TF expertise. Thank you!')\nnms = False\nf[5], s_model = self._export_saved_model(nms=nms or self.args.agnostic_nms or tfjs,\nagnostic_nms=self.args.agnostic_nms or tfjs)\ndebug = False\nif debug:\nif pb or tfjs:  # pb prerequisite to tfjs\nf[6], _ = self._export_pb(s_model)\nif tflite or edgetpu:\nf[7], _ = self._export_tflite(s_model,\nint8=self.args.int8 or edgetpu,\ndata=self.args.data,\nnms=nms,\nagnostic_nms=self.args.agnostic_nms)\nif edgetpu:\nf[8], _ = self._export_edgetpu()\nself._add_tflite_metadata(f[8] or f[7], num_outputs=len(self.output_shape))\nif tfjs:\nf[9], _ = self._export_tfjs()\nif paddle:  # PaddlePaddle\nf[10], _ = self._export_paddle()\n# Finish\nf = [str(x) for x in f if x]  # filter out '' and None\nif any(f):\ns = \"-WARNING \u26a0\ufe0f not yet supported for YOLOv8 exported models\"\nLOGGER.info(f'\\nExport complete ({time.time() - t:.1f}s)'\nf\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\nf\"\\nPredict:         yolo task={model.task} mode=predict model={f[-1]} {s}\"\nf\"\\nValidate:        yolo task={model.task} mode=val model={f[-1]} {s}\"\nf\"\\nVisualize:       https://netron.app\")\nself.run_callbacks(\"on_export_end\")\nreturn f  # return list of exported files/dirs\n@try_export\ndef _export_torchscript(self, prefix=colorstr('TorchScript:')):\n# YOLOv8 TorchScript model export\nLOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\nf = self.file.with_suffix('.torchscript')\nts = torch.jit.trace(self.model, self.im, strict=False)\nd = {\"shape\": self.im.shape, \"stride\": int(max(self.model.stride)), \"names\": self.model.names}\nextra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\nif self.args.optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\nLOGGER.info(f'{prefix} optimizing for mobile...')\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\noptimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\nelse:\nts.save(str(f), _extra_files=extra_files)\nreturn f, None\n@try_export\ndef _export_onnx(self, prefix=colorstr('ONNX:')):\n# YOLOv8 ONNX export\ncheck_requirements('onnx&gt;=1.12.0')\nimport onnx  # noqa\nLOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__}...')\nf = str(self.file.with_suffix('.onnx'))\noutput_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']\ndynamic = self.args.dynamic\nif dynamic:\ndynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\nif isinstance(self.model, SegmentationModel):\ndynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\ndynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\nelif isinstance(self.model, DetectionModel):\ndynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\ntorch.onnx.export(\nself.model.cpu() if dynamic else self.model,  # --dynamic only compatible with cpu\nself.im.cpu() if dynamic else self.im,\nf,\nverbose=False,\nopset_version=self.args.opset,\ndo_constant_folding=True,  # WARNING: DNN inference with torch&gt;=1.12 may require do_constant_folding=False\ninput_names=['images'],\noutput_names=output_names,\ndynamic_axes=dynamic or None)\n# Checks\nmodel_onnx = onnx.load(f)  # load onnx model\nonnx.checker.check_model(model_onnx)  # check onnx model\n# Metadata\nd = {'stride': int(max(self.model.stride)), 'names': self.model.names}\nfor k, v in d.items():\nmeta = model_onnx.metadata_props.add()\nmeta.key, meta.value = k, str(v)\nonnx.save(model_onnx, f)\n# Simplify\nif self.args.simplify:\ntry:\ncheck_requirements('onnxsim')\nimport onnxsim\nLOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')\nsubprocess.run(f'onnxsim {f} {f}', shell=True)\nexcept Exception as e:\nLOGGER.info(f'{prefix} simplifier failure: {e}')\nreturn f, model_onnx\n@try_export\ndef _export_openvino(self, prefix=colorstr('OpenVINO:')):\n# YOLOv8 OpenVINO export\ncheck_requirements('openvino-dev&gt;=2022.3')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\nimport openvino.runtime as ov  # noqa\nfrom openvino.tools import mo  # noqa\nLOGGER.info(f'\\n{prefix} starting export with openvino {ov.__version__}...')\nf = str(self.file).replace(self.file.suffix, f'_openvino_model{os.sep}')\nf_onnx = self.file.with_suffix('.onnx')\nf_ov = str(Path(f) / self.file.with_suffix('.xml').name)\nov_model = mo.convert_model(f_onnx,\nmodel_name=self.pretty_name,\nframework=\"onnx\",\ncompress_to_fp16=self.args.half)  # export\nov.serialize(ov_model, f_ov)  # save\nyaml_save(Path(f) / self.file.with_suffix('.yaml').name, self.metadata)  # add metadata.yaml\nreturn f, None\n@try_export\ndef _export_paddle(self, prefix=colorstr('PaddlePaddle:')):\n# YOLOv8 Paddle export\ncheck_requirements(('paddlepaddle', 'x2paddle'))\nimport x2paddle  # noqa\nfrom x2paddle.convert import pytorch2paddle  # noqa\nLOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\nf = str(self.file).replace(self.file.suffix, f'_paddle_model{os.sep}')\npytorch2paddle(module=self.model, save_dir=f, jit_type='trace', input_examples=[self.im])  # export\nyaml_save(Path(f) / self.file.with_suffix('.yaml').name, self.metadata)  # add metadata.yaml\nreturn f, None\n@try_export\ndef _export_coreml(self, prefix=colorstr('CoreML:')):\n# YOLOv8 CoreML export\ncheck_requirements('coremltools&gt;=6.0')\nimport coremltools as ct  # noqa\nclass iOSModel(torch.nn.Module):\n# Wrap an Ultralytics YOLO model for iOS export\ndef __init__(self, model, im):\nsuper().__init__()\nb, c, h, w = im.shape  # batch, channel, height, width\nself.model = model\nself.nc = len(model.names)  # number of classes\nif w == h:\nself.normalize = 1.0 / w  # scalar\nelse:\nself.normalize = torch.tensor([1.0 / w, 1.0 / h, 1.0 / w, 1.0 / h])  # broadcast (slower, smaller)\ndef forward(self, x):\nxywh, cls = self.model(x)[0].transpose(0, 1).split((4, self.nc), 1)\nreturn cls, xywh * self.normalize  # confidence (3780, 80), coordinates (3780, 4)\nLOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\nf = self.file.with_suffix('.mlmodel')\nif self.model.task == 'classify':\nbias = [-x for x in IMAGENET_MEAN]\nscale = 1 / 255 / (sum(IMAGENET_STD) / 3)\nclassifier_config = ct.ClassifierConfig(list(self.model.names.values()))\nelse:\nbias = [0.0, 0.0, 0.0]\nscale = 1 / 255\nclassifier_config = None\nmodel = iOSModel(self.model, self.im).eval() if self.args.nms else self.model\nts = torch.jit.trace(model, self.im, strict=False)  # TorchScript model\nct_model = ct.convert(ts,\ninputs=[ct.ImageType('image', shape=self.im.shape, scale=scale, bias=bias)],\nclassifier_config=classifier_config)\nbits, mode = (8, 'kmeans_lut') if self.args.int8 else (16, 'linear') if self.args.half else (32, None)\nif bits &lt; 32:\nif MACOS:  # quantization only supported on macOS\nct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\nelse:\nLOGGER.info(f'{prefix} quantization only supported on macOS, skipping...')\nif self.args.nms:\nct_model = self._pipeline_coreml(ct_model)\nct_model.short_description = self.metadata['description']\nct_model.author = self.metadata['author']\nct_model.license = self.metadata['license']\nct_model.version = self.metadata['version']\nct_model.save(str(f))\nreturn f, ct_model\n@try_export\ndef _export_engine(self, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n# YOLOv8 TensorRT export https://developer.nvidia.com/tensorrt\nassert self.im.device.type != 'cpu', \"export running on CPU but must be on GPU, i.e. use 'device=0'\"\ntry:\nimport tensorrt as trt  # noqa\nexcept ImportError:\nif platform.system() == 'Linux':\ncheck_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\nimport tensorrt as trt  # noqa\ncheck_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=8.0.0\nself._export_onnx()\nonnx = self.file.with_suffix('.onnx')\nLOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\nassert onnx.exists(), f'failed to export ONNX file: {onnx}'\nf = self.file.with_suffix('.engine')  # TensorRT engine file\nlogger = trt.Logger(trt.Logger.INFO)\nif verbose:\nlogger.min_severity = trt.Logger.Severity.VERBOSE\nbuilder = trt.Builder(logger)\nconfig = builder.create_builder_config()\nconfig.max_workspace_size = workspace * 1 &lt;&lt; 30\n# config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace &lt;&lt; 30)  # fix TRT 8.4 deprecation notice\nflag = (1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\nnetwork = builder.create_network(flag)\nparser = trt.OnnxParser(network, logger)\nif not parser.parse_from_file(str(onnx)):\nraise RuntimeError(f'failed to load ONNX file: {onnx}')\ninputs = [network.get_input(i) for i in range(network.num_inputs)]\noutputs = [network.get_output(i) for i in range(network.num_outputs)]\nfor inp in inputs:\nLOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\nfor out in outputs:\nLOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\nif self.args.dynamic:\nshape = self.im.shape\nif shape[0] &lt;= 1:\nLOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f --dynamic model requires maximum --batch-size argument\")\nprofile = builder.create_optimization_profile()\nfor inp in inputs:\nprofile.set_shape(inp.name, (1, *shape[1:]), (max(1, shape[0] // 2), *shape[1:]), shape)\nconfig.add_optimization_profile(profile)\nLOGGER.info(\nf'{prefix} building FP{16 if builder.platform_has_fast_fp16 and self.args.half else 32} engine as {f}')\nif builder.platform_has_fast_fp16 and self.args.half:\nconfig.set_flag(trt.BuilderFlag.FP16)\nwith builder.build_engine(network, config) as engine, open(f, 'wb') as t:\nt.write(engine.serialize())\nreturn f, None\n@try_export\ndef _export_saved_model(self,\nnms=False,\nagnostic_nms=False,\ntopk_per_class=100,\ntopk_all=100,\niou_thres=0.45,\nconf_thres=0.25,\nprefix=colorstr('TensorFlow SavedModel:')):\n# YOLOv8 TensorFlow SavedModel export\ntry:\nimport tensorflow as tf  # noqa\nexcept ImportError:\ncheck_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\nimport tensorflow as tf  # noqa\ncheck_requirements((\"onnx\", \"onnx2tf\", \"sng4onnx\", \"onnxsim\", \"onnx_graphsurgeon\"),\ncmds=\"--extra-index-url https://pypi.ngc.nvidia.com \")\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nf = str(self.file).replace(self.file.suffix, '_saved_model')\n# Export to ONNX\nself._export_onnx()\nonnx = self.file.with_suffix('.onnx')\n# Export to TF SavedModel\nsubprocess.run(f'onnx2tf -i {onnx} --output_signaturedefs -o {f}', shell=True)\n# Load saved_model\nkeras_model = tf.saved_model.load(f, tags=None, options=None)\nreturn f, keras_model\n@try_export\ndef _export_saved_model_OLD(self,\nnms=False,\nagnostic_nms=False,\ntopk_per_class=100,\ntopk_all=100,\niou_thres=0.45,\nconf_thres=0.25,\nprefix=colorstr('TensorFlow SavedModel:')):\n# YOLOv8 TensorFlow SavedModel export\ntry:\nimport tensorflow as tf  # noqa\nexcept ImportError:\ncheck_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\nimport tensorflow as tf  # noqa\n# from models.tf import TFModel\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nf = str(self.file).replace(self.file.suffix, '_saved_model')\nbatch_size, ch, *imgsz = list(self.im.shape)  # BCHW\ntf_models = None  # TODO: no TF modules available\ntf_model = tf_models.TFModel(cfg=self.model.yaml, model=self.model.cpu(), nc=self.model.nc, imgsz=imgsz)\nim = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n_ = tf_model.predict(im, nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\ninputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if self.args.dynamic else batch_size)\noutputs = tf_model.predict(inputs, nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\nkeras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\nkeras_model.trainable = False\nkeras_model.summary()\nif self.args.keras:\nkeras_model.save(f, save_format='tf')\nelse:\nspec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\nm = tf.function(lambda x: keras_model(x))  # full model\nm = m.get_concrete_function(spec)\nfrozen_func = convert_variables_to_constants_v2(m)\ntfm = tf.Module()\ntfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if nms else frozen_func(x), [spec])\ntfm.__call__(im)\ntf.saved_model.save(tfm,\nf,\noptions=tf.saved_model.SaveOptions(experimental_custom_gradients=False)\nif check_version(tf.__version__, '2.6') else tf.saved_model.SaveOptions())\nreturn f, keras_model\n@try_export\ndef _export_pb(self, keras_model, prefix=colorstr('TensorFlow GraphDef:')):\n# YOLOv8 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\nimport tensorflow as tf  # noqa\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nf = self.file.with_suffix('.pb')\nm = tf.function(lambda x: keras_model(x))  # full model\nm = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\nfrozen_func = convert_variables_to_constants_v2(m)\nfrozen_func.graph.as_graph_def()\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\nreturn f, None\n@try_export\ndef _export_tflite(self, keras_model, int8, data, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n# YOLOv8 TensorFlow Lite export\nimport tensorflow as tf  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nbatch_size, ch, *imgsz = list(self.im.shape)  # BCHW\nf = str(self.file).replace(self.file.suffix, '-fp16.tflite')\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\nconverter.target_spec.supported_types = [tf.float16]\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nif int8:\ndef representative_dataset_gen(dataset, n_images=100):\n# Dataset generator for use with converter.representative_dataset, returns a generator of np arrays\nfor n, (path, img, im0s, vid_cap, string) in enumerate(dataset):\nim = np.transpose(img, [1, 2, 0])\nim = np.expand_dims(im, axis=0).astype(np.float32)\nim /= 255\nyield [im]\nif n &gt;= n_images:\nbreak\ndataset = LoadImages(check_det_dataset(check_yaml(data))['train'], imgsz=imgsz, auto=False)\nconverter.representative_dataset = lambda: representative_dataset_gen(dataset, n_images=100)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.target_spec.supported_types = []\nconverter.inference_input_type = tf.uint8  # or tf.int8\nconverter.inference_output_type = tf.uint8  # or tf.int8\nconverter.experimental_new_quantizer = True\nf = str(self.file).replace(self.file.suffix, '-int8.tflite')\nif nms or agnostic_nms:\nconverter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\ntflite_model = converter.convert()\nopen(f, \"wb\").write(tflite_model)\nreturn f, None\n@try_export\ndef _export_edgetpu(self, prefix=colorstr('Edge TPU:')):\n# YOLOv8 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\ncmd = 'edgetpu_compiler --version'\nhelp_url = 'https://coral.ai/docs/edgetpu/compiler/'\nassert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\nif subprocess.run(f'{cmd} &gt;/dev/null', shell=True).returncode != 0:\nLOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\nsudo = subprocess.run('sudo --version &gt;/dev/null', shell=True).returncode == 0  # sudo installed on system\nfor c in (\n'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | '  # no comma\n'sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n'sudo apt-get update',\n'sudo apt-get install edgetpu-compiler'):\nsubprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\nver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\nLOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\nf = str(self.file).replace(self.file.suffix, '-int8_edgetpu.tflite')  # Edge TPU model\nf_tfl = str(self.file).replace(self.file.suffix, '-int8.tflite')  # TFLite model\ncmd = f\"edgetpu_compiler -s -d -k 10 --out_dir {self.file.parent} {f_tfl}\"\nsubprocess.run(cmd.split(), check=True)\nreturn f, None\n@try_export\ndef _export_tfjs(self, prefix=colorstr('TensorFlow.js:')):\n# YOLOv8 TensorFlow.js export\ncheck_requirements('tensorflowjs')\nimport tensorflowjs as tfjs  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\nf = str(self.file).replace(self.file.suffix, '_web_model')  # js dir\nf_pb = self.file.with_suffix('.pb')  # *.pb path\nf_json = Path(f) / 'model.json'  # *.json path\ncmd = f'tensorflowjs_converter --input_format=tf_frozen_model ' \\\n              f'--output_node_names=Identity,Identity_1,Identity_2,Identity_3 {f_pb} {f}'\nsubprocess.run(cmd.split())\nwith open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\nsubst = re.sub(\nr'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\nr'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\nr'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\nr'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}', r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\nr'\"Identity_1\": {\"name\": \"Identity_1\"}, '\nr'\"Identity_2\": {\"name\": \"Identity_2\"}, '\nr'\"Identity_3\": {\"name\": \"Identity_3\"}}}', f_json.read_text())\nj.write(subst)\nreturn f, None\ndef _add_tflite_metadata(self, file, num_outputs):\n# Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata\nwith contextlib.suppress(ImportError):\n# check_requirements('tflite_support')\nfrom tflite_support import flatbuffers  # noqa\nfrom tflite_support import metadata as _metadata  # noqa\nfrom tflite_support import metadata_schema_py_generated as _metadata_fb  # noqa\ntmp_file = Path('/tmp/meta.txt')\nwith open(tmp_file, 'w') as meta_f:\nmeta_f.write(str(self.metadata))\nmodel_meta = _metadata_fb.ModelMetadataT()\nlabel_file = _metadata_fb.AssociatedFileT()\nlabel_file.name = tmp_file.name\nmodel_meta.associatedFiles = [label_file]\nsubgraph = _metadata_fb.SubGraphMetadataT()\nsubgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\nsubgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\nmodel_meta.subgraphMetadata = [subgraph]\nb = flatbuffers.Builder(0)\nb.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\nmetadata_buf = b.Output()\npopulator = _metadata.MetadataPopulator.with_model_file(file)\npopulator.load_metadata_buffer(metadata_buf)\npopulator.load_associated_files([str(tmp_file)])\npopulator.populate()\ntmp_file.unlink()\ndef _pipeline_coreml(self, model, prefix=colorstr('CoreML Pipeline:')):\n# YOLOv8 CoreML pipeline\nimport coremltools as ct  # noqa\nLOGGER.info(f'{prefix} starting pipeline with coremltools {ct.__version__}...')\nbatch_size, ch, h, w = list(self.im.shape)  # BCHW\n# Output shapes\nspec = model.get_spec()\nout0, out1 = iter(spec.description.output)\nif MACOS:\nfrom PIL import Image\nimg = Image.new('RGB', (w, h))  # img(192 width, 320 height)\n# img = torch.zeros((*opt.img_size, 3)).numpy()  # img size(320,192,3) iDetection\nout = model.predict({'image': img})\nout0_shape = out[out0.name].shape\nout1_shape = out[out1.name].shape\nelse:  # linux and windows can not run model.predict(), get sizes from pytorch output y\nout0_shape = self.output_shape[2], self.output_shape[1] - 4  # (3780, 80)\nout1_shape = self.output_shape[2], 4  # (3780, 4)\n# Checks\nnames = self.metadata['names']\nnx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\nna, nc = out0_shape\n# na, nc = out0.type.multiArrayType.shape  # number anchors, classes\nassert len(names) == nc, f'{len(names)} names found for nc={nc}'  # check\n# Define output shapes (missing)\nout0.type.multiArrayType.shape[:] = out0_shape  # (3780, 80)\nout1.type.multiArrayType.shape[:] = out1_shape  # (3780, 4)\n# spec.neuralNetwork.preprocessing[0].featureName = '0'\n# Flexible input shapes\n# from coremltools.models.neural_network import flexible_shape_utils\n# s = [] # shapes\n# s.append(flexible_shape_utils.NeuralNetworkImageSize(320, 192))\n# s.append(flexible_shape_utils.NeuralNetworkImageSize(640, 384))  # (height, width)\n# flexible_shape_utils.add_enumerated_image_sizes(spec, feature_name='image', sizes=s)\n# r = flexible_shape_utils.NeuralNetworkImageSizeRange()  # shape ranges\n# r.add_height_range((192, 640))\n# r.add_width_range((192, 640))\n# flexible_shape_utils.update_image_size_range(spec, feature_name='image', size_range=r)\n# Print\n# print(spec.description)\n# Model from spec\nmodel = ct.models.MLModel(spec)\n# 3. Create NMS protobuf\nnms_spec = ct.proto.Model_pb2.Model()\nnms_spec.specificationVersion = 5\nfor i in range(2):\ndecoder_output = model._spec.description.output[i].SerializeToString()\nnms_spec.description.input.add()\nnms_spec.description.input[i].ParseFromString(decoder_output)\nnms_spec.description.output.add()\nnms_spec.description.output[i].ParseFromString(decoder_output)\nnms_spec.description.output[0].name = 'confidence'\nnms_spec.description.output[1].name = 'coordinates'\noutput_sizes = [nc, 4]\nfor i in range(2):\nma_type = nms_spec.description.output[i].type.multiArrayType\nma_type.shapeRange.sizeRanges.add()\nma_type.shapeRange.sizeRanges[0].lowerBound = 0\nma_type.shapeRange.sizeRanges[0].upperBound = -1\nma_type.shapeRange.sizeRanges.add()\nma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\nma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\ndel ma_type.shape[:]\nnms = nms_spec.nonMaximumSuppression\nnms.confidenceInputFeatureName = out0.name  # 1x507x80\nnms.coordinatesInputFeatureName = out1.name  # 1x507x4\nnms.confidenceOutputFeatureName = 'confidence'\nnms.coordinatesOutputFeatureName = 'coordinates'\nnms.iouThresholdInputFeatureName = 'iouThreshold'\nnms.confidenceThresholdInputFeatureName = 'confidenceThreshold'\nnms.iouThreshold = 0.45\nnms.confidenceThreshold = 0.25\nnms.pickTop.perClass = True\nnms.stringClassLabels.vector.extend(names.values())\nnms_model = ct.models.MLModel(nms_spec)\n# 4. Pipeline models together\npipeline = ct.models.pipeline.Pipeline(input_features=[('image', ct.models.datatypes.Array(3, ny, nx)),\n('iouThreshold', ct.models.datatypes.Double()),\n('confidenceThreshold', ct.models.datatypes.Double())],\noutput_features=['confidence', 'coordinates'])\npipeline.add_model(model)\npipeline.add_model(nms_model)\n# Correct datatypes\npipeline.spec.description.input[0].ParseFromString(model._spec.description.input[0].SerializeToString())\npipeline.spec.description.output[0].ParseFromString(nms_model._spec.description.output[0].SerializeToString())\npipeline.spec.description.output[1].ParseFromString(nms_model._spec.description.output[1].SerializeToString())\n# Update metadata\npipeline.spec.specificationVersion = 5\npipeline.spec.description.metadata.userDefined.update({\n'IoU threshold': str(nms.iouThreshold),\n'Confidence threshold': str(nms.confidenceThreshold)})\n# Save the model\nmodel = ct.models.MLModel(pipeline.spec)\nmodel.input_description['image'] = 'Input image'\nmodel.input_description['iouThreshold'] = f'(optional) IOU threshold override (default: {nms.iouThreshold})'\nmodel.input_description['confidenceThreshold'] = \\\n            f'(optional) Confidence threshold override (default: {nms.confidenceThreshold})'\nmodel.output_description['confidence'] = 'Boxes \u00d7 Class confidence (see user-defined metadata \"classes\")'\nmodel.output_description['coordinates'] = 'Boxes \u00d7 [x, y, width, height] (relative to image size)'\nLOGGER.info(f'{prefix} pipeline success')\nreturn model\ndef run_callbacks(self, event: str):\nfor callback in self.callbacks.get(event, []):\ncallback(self)\n</code></pre>"},{"location":"reference/exporter/#ultralytics.yolo.engine.exporter.Exporter.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None)</code>","text":"<p>Initializes the Exporter class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str</code> <p>Path to a configuration file. Defaults to DEFAULT_CFG.</p> <code>DEFAULT_CFG</code> <code>overrides</code> <code>dict</code> <p>Configuration overrides. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/yolo/engine/exporter.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None):\n\"\"\"\n    Initializes the Exporter class.\n    Args:\n        cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks\ncallbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/model/","title":"Model","text":""},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO","title":"<code>YOLO</code>","text":"<p>YOLO</p> <p>A python interface which emulates a model-like behaviour by wrapping trainers.</p> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>class YOLO:\n\"\"\"\n    YOLO\n    A python interface which emulates a model-like behaviour by wrapping trainers.\n    \"\"\"\ndef __init__(self, model='yolov8n.yaml', type=\"v8\") -&gt; None:\n\"\"\"\n        Initializes the YOLO object.\n        Args:\n            model (str, Path): model to load or create\n            type (str): Type/version of models to use. Defaults to \"v8\".\n        \"\"\"\nself.type = type\nself.ModelClass = None  # model class\nself.TrainerClass = None  # trainer class\nself.ValidatorClass = None  # validator class\nself.PredictorClass = None  # predictor class\nself.predictor = None  # reuse predictor\nself.model = None  # model object\nself.trainer = None  # trainer object\nself.task = None  # task type\nself.ckpt = None  # if loaded from *.pt\nself.cfg = None  # if loaded from *.yaml\nself.ckpt_path = None\nself.overrides = {}  # overrides for trainer object\n# Load or create new YOLO model\nload_methods = {'.pt': self._load, '.yaml': self._new}\nsuffix = Path(model).suffix\nif suffix in load_methods:\n{'.pt': self._load, '.yaml': self._new}[suffix](model)\nelse:\nraise NotImplementedError(f\"'{suffix}' model loading not implemented\")\ndef __call__(self, source=None, stream=False, **kwargs):\nreturn self.predict(source, stream, **kwargs)\ndef _new(self, cfg: str, verbose=True):\n\"\"\"\n        Initializes a new model and infers the task type from the model definitions.\n        Args:\n            cfg (str): model configuration file\n            verbose (bool): display model info on load\n        \"\"\"\ncfg = check_yaml(cfg)  # check YAML\ncfg_dict = yaml_load(cfg, append_filename=True)  # model dict\nself.task = guess_model_task(cfg_dict)\nself.ModelClass, self.TrainerClass, self.ValidatorClass, self.PredictorClass = \\\n            self._assign_ops_from_task(self.task)\nself.model = self.ModelClass(cfg_dict, verbose=verbose)  # initialize\nself.cfg = cfg\ndef _load(self, weights: str):\n\"\"\"\n        Initializes a new model and infers the task type from the model head.\n        Args:\n            weights (str): model checkpoint to be loaded\n        \"\"\"\nself.model, self.ckpt = attempt_load_one_weight(weights)\nself.ckpt_path = weights\nself.task = self.model.args[\"task\"]\nself.overrides = self.model.args\nself._reset_ckpt_args(self.overrides)\nself.ModelClass, self.TrainerClass, self.ValidatorClass, self.PredictorClass = \\\n            self._assign_ops_from_task(self.task)\ndef reset(self):\n\"\"\"\n        Resets the model modules.\n        \"\"\"\nfor m in self.model.modules():\nif hasattr(m, 'reset_parameters'):\nm.reset_parameters()\nfor p in self.model.parameters():\np.requires_grad = True\ndef info(self, verbose=False):\n\"\"\"\n        Logs model info.\n        Args:\n            verbose (bool): Controls verbosity.\n        \"\"\"\nself.model.info(verbose=verbose)\ndef fuse(self):\nself.model.fuse()\ndef predict(self, source=None, stream=False, **kwargs):\n\"\"\"\n        Perform prediction using the YOLO model.\n        Args:\n            source (str | int | PIL | np.ndarray): The source of the image to make predictions on.\n                          Accepts all source types accepted by the YOLO model.\n            stream (bool): Whether to stream the predictions or not. Defaults to False.\n            **kwargs : Additional keyword arguments passed to the predictor.\n                       Check the 'configuration' section in the documentation for all available options.\n        Returns:\n            (List[ultralytics.yolo.engine.results.Results]): The prediction results.\n        \"\"\"\noverrides = self.overrides.copy()\noverrides[\"conf\"] = 0.25\noverrides.update(kwargs)\noverrides[\"mode\"] = \"predict\"\noverrides[\"save\"] = kwargs.get(\"save\", False)  # not save files by default\nif not self.predictor:\nself.predictor = self.PredictorClass(overrides=overrides)\nself.predictor.setup_model(model=self.model)\nelse:  # only update args if predictor is already setup\nself.predictor.args = get_cfg(self.predictor.args, overrides)\nis_cli = sys.argv[0].endswith('yolo') or sys.argv[0].endswith('ultralytics')\nreturn self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n@smart_inference_mode()\ndef val(self, data=None, **kwargs):\n\"\"\"\n        Validate a model on a given dataset .\n        Args:\n            data (str): The dataset to validate on. Accepts all formats accepted by yolo\n            **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n        \"\"\"\noverrides = self.overrides.copy()\noverrides[\"rect\"] = True  # rect batches as default\noverrides.update(kwargs)\noverrides[\"mode\"] = \"val\"\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.data = data or args.data\nargs.task = self.task\nif args.imgsz == DEFAULT_CFG.imgsz:\nargs.imgsz = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nargs.imgsz = check_imgsz(args.imgsz, max_dim=1)\nvalidator = self.ValidatorClass(args=args)\nvalidator(model=self.model)\nreturn validator.metrics\n@smart_inference_mode()\ndef export(self, **kwargs):\n\"\"\"\n        Export model.\n        Args:\n            **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n        \"\"\"\noverrides = self.overrides.copy()\noverrides.update(kwargs)\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.task = self.task\nif args.imgsz == DEFAULT_CFG.imgsz:\nargs.imgsz = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nexporter = Exporter(overrides=args)\nexporter(model=self.model)\ndef train(self, **kwargs):\n\"\"\"\n        Trains the model on a given dataset.\n        Args:\n            **kwargs (Any): Any number of arguments representing the training configuration.\n        \"\"\"\noverrides = self.overrides.copy()\noverrides.update(kwargs)\nif kwargs.get(\"cfg\"):\nLOGGER.info(f\"cfg file passed. Overriding default params with {kwargs['cfg']}.\")\noverrides = yaml_load(check_yaml(kwargs[\"cfg\"]), append_filename=True)\noverrides[\"task\"] = self.task\noverrides[\"mode\"] = \"train\"\nif not overrides.get(\"data\"):\nraise AttributeError(\"Dataset required but missing, i.e. pass 'data=coco128.yaml'\")\nif overrides.get(\"resume\"):\noverrides[\"resume\"] = self.ckpt_path\nself.trainer = self.TrainerClass(overrides=overrides)\nif not overrides.get(\"resume\"):  # manually set model only if not resuming\nself.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\nself.model = self.trainer.model\nself.trainer.train()\n# update model and cfg after training\nif RANK in {0, -1}:\nself.model, _ = attempt_load_one_weight(str(self.trainer.best))\nself.overrides = self.model.args\ndef to(self, device):\n\"\"\"\n        Sends the model to the given device.\n        Args:\n            device (str): device\n        \"\"\"\nself.model.to(device)\ndef _assign_ops_from_task(self, task):\nmodel_class, train_lit, val_lit, pred_lit = MODEL_MAP[task]\n# warning: eval is unsafe. Use with caution\ntrainer_class = eval(train_lit.replace(\"TYPE\", f\"{self.type}\"))\nvalidator_class = eval(val_lit.replace(\"TYPE\", f\"{self.type}\"))\npredictor_class = eval(pred_lit.replace(\"TYPE\", f\"{self.type}\"))\nreturn model_class, trainer_class, validator_class, predictor_class\n@property\ndef names(self):\n\"\"\"\n         Returns class names of the loaded model.\n        \"\"\"\nreturn self.model.names\n@property\ndef transforms(self):\n\"\"\"\n         Returns transform of the loaded model.\n        \"\"\"\nreturn self.model.transforms if hasattr(self.model, 'transforms') else None\n@staticmethod\ndef add_callback(event: str, func):\n\"\"\"\n        Add callback\n        \"\"\"\ncallbacks.default_callbacks[event].append(func)\n@staticmethod\ndef _reset_ckpt_args(args):\nfor arg in 'augment', 'verbose', 'project', 'name', 'exist_ok', 'resume', 'batch', 'epochs', 'cache', \\\n                'save_json', 'half', 'v5loader', 'device', 'cfg', 'save', 'rect', 'plots':\nargs.pop(arg, None)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.names","title":"<code>names</code>  <code>property</code>","text":"<p>Returns class names of the loaded model.</p>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.transforms","title":"<code>transforms</code>  <code>property</code>","text":"<p>Returns transform of the loaded model.</p>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.__init__","title":"<code>__init__(model='yolov8n.yaml', type='v8')</code>","text":"<p>Initializes the YOLO object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str, Path</code> <p>model to load or create</p> <code>'yolov8n.yaml'</code> <code>type</code> <code>str</code> <p>Type/version of models to use. Defaults to \"v8\".</p> <code>'v8'</code> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def __init__(self, model='yolov8n.yaml', type=\"v8\") -&gt; None:\n\"\"\"\n    Initializes the YOLO object.\n    Args:\n        model (str, Path): model to load or create\n        type (str): Type/version of models to use. Defaults to \"v8\".\n    \"\"\"\nself.type = type\nself.ModelClass = None  # model class\nself.TrainerClass = None  # trainer class\nself.ValidatorClass = None  # validator class\nself.PredictorClass = None  # predictor class\nself.predictor = None  # reuse predictor\nself.model = None  # model object\nself.trainer = None  # trainer object\nself.task = None  # task type\nself.ckpt = None  # if loaded from *.pt\nself.cfg = None  # if loaded from *.yaml\nself.ckpt_path = None\nself.overrides = {}  # overrides for trainer object\n# Load or create new YOLO model\nload_methods = {'.pt': self._load, '.yaml': self._new}\nsuffix = Path(model).suffix\nif suffix in load_methods:\n{'.pt': self._load, '.yaml': self._new}[suffix](model)\nelse:\nraise NotImplementedError(f\"'{suffix}' model loading not implemented\")\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.add_callback","title":"<code>add_callback(event, func)</code>  <code>staticmethod</code>","text":"<p>Add callback</p> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>@staticmethod\ndef add_callback(event: str, func):\n\"\"\"\n    Add callback\n    \"\"\"\ncallbacks.default_callbacks[event].append(func)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.export","title":"<code>export(**kwargs)</code>","text":"<p>Export model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Any other args accepted by the predictors. To see all args check 'configuration' section in docs</p> <code>{}</code> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef export(self, **kwargs):\n\"\"\"\n    Export model.\n    Args:\n        **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n    \"\"\"\noverrides = self.overrides.copy()\noverrides.update(kwargs)\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.task = self.task\nif args.imgsz == DEFAULT_CFG.imgsz:\nargs.imgsz = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nexporter = Exporter(overrides=args)\nexporter(model=self.model)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.info","title":"<code>info(verbose=False)</code>","text":"<p>Logs model info.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Controls verbosity.</p> <code>False</code> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def info(self, verbose=False):\n\"\"\"\n    Logs model info.\n    Args:\n        verbose (bool): Controls verbosity.\n    \"\"\"\nself.model.info(verbose=verbose)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.predict","title":"<code>predict(source=None, stream=False, **kwargs)</code>","text":"<p>Perform prediction using the YOLO model.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | int | PIL | np.ndarray</code> <p>The source of the image to make predictions on.           Accepts all source types accepted by the YOLO model.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the predictions or not. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the predictor.        Check the 'configuration' section in the documentation for all available options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ultralytics.yolo.engine.results.Results]</code> <p>The prediction results.</p> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def predict(self, source=None, stream=False, **kwargs):\n\"\"\"\n    Perform prediction using the YOLO model.\n    Args:\n        source (str | int | PIL | np.ndarray): The source of the image to make predictions on.\n                      Accepts all source types accepted by the YOLO model.\n        stream (bool): Whether to stream the predictions or not. Defaults to False.\n        **kwargs : Additional keyword arguments passed to the predictor.\n                   Check the 'configuration' section in the documentation for all available options.\n    Returns:\n        (List[ultralytics.yolo.engine.results.Results]): The prediction results.\n    \"\"\"\noverrides = self.overrides.copy()\noverrides[\"conf\"] = 0.25\noverrides.update(kwargs)\noverrides[\"mode\"] = \"predict\"\noverrides[\"save\"] = kwargs.get(\"save\", False)  # not save files by default\nif not self.predictor:\nself.predictor = self.PredictorClass(overrides=overrides)\nself.predictor.setup_model(model=self.model)\nelse:  # only update args if predictor is already setup\nself.predictor.args = get_cfg(self.predictor.args, overrides)\nis_cli = sys.argv[0].endswith('yolo') or sys.argv[0].endswith('ultralytics')\nreturn self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.reset","title":"<code>reset()</code>","text":"<p>Resets the model modules.</p> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def reset(self):\n\"\"\"\n    Resets the model modules.\n    \"\"\"\nfor m in self.model.modules():\nif hasattr(m, 'reset_parameters'):\nm.reset_parameters()\nfor p in self.model.parameters():\np.requires_grad = True\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.to","title":"<code>to(device)</code>","text":"<p>Sends the model to the given device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>device</p> required Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def to(self, device):\n\"\"\"\n    Sends the model to the given device.\n    Args:\n        device (str): device\n    \"\"\"\nself.model.to(device)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.train","title":"<code>train(**kwargs)</code>","text":"<p>Trains the model on a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Any number of arguments representing the training configuration.</p> <code>{}</code> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def train(self, **kwargs):\n\"\"\"\n    Trains the model on a given dataset.\n    Args:\n        **kwargs (Any): Any number of arguments representing the training configuration.\n    \"\"\"\noverrides = self.overrides.copy()\noverrides.update(kwargs)\nif kwargs.get(\"cfg\"):\nLOGGER.info(f\"cfg file passed. Overriding default params with {kwargs['cfg']}.\")\noverrides = yaml_load(check_yaml(kwargs[\"cfg\"]), append_filename=True)\noverrides[\"task\"] = self.task\noverrides[\"mode\"] = \"train\"\nif not overrides.get(\"data\"):\nraise AttributeError(\"Dataset required but missing, i.e. pass 'data=coco128.yaml'\")\nif overrides.get(\"resume\"):\noverrides[\"resume\"] = self.ckpt_path\nself.trainer = self.TrainerClass(overrides=overrides)\nif not overrides.get(\"resume\"):  # manually set model only if not resuming\nself.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\nself.model = self.trainer.model\nself.trainer.train()\n# update model and cfg after training\nif RANK in {0, -1}:\nself.model, _ = attempt_load_one_weight(str(self.trainer.best))\nself.overrides = self.model.args\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.val","title":"<code>val(data=None, **kwargs)</code>","text":"<p>Validate a model on a given dataset .</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The dataset to validate on. Accepts all formats accepted by yolo</p> <code>None</code> <code>**kwargs</code> <p>Any other args accepted by the validators. To see all args check 'configuration' section in docs</p> <code>{}</code> Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef val(self, data=None, **kwargs):\n\"\"\"\n    Validate a model on a given dataset .\n    Args:\n        data (str): The dataset to validate on. Accepts all formats accepted by yolo\n        **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n    \"\"\"\noverrides = self.overrides.copy()\noverrides[\"rect\"] = True  # rect batches as default\noverrides.update(kwargs)\noverrides[\"mode\"] = \"val\"\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.data = data or args.data\nargs.task = self.task\nif args.imgsz == DEFAULT_CFG.imgsz:\nargs.imgsz = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nargs.imgsz = check_imgsz(args.imgsz, max_dim=1)\nvalidator = self.ValidatorClass(args=args)\nvalidator(model=self.model)\nreturn validator.metrics\n</code></pre>"},{"location":"reference/nn/","title":"nn Module","text":"<p>Ultralytics nn module contains 3 main components:</p> <ol> <li>AutoBackend: A module that can run inference on all popular model formats</li> <li>BaseModel: <code>BaseModel</code> class defines the operations supported by tasks like Detection and Segmentation</li> <li>modules: Optimized and reusable neural network blocks built on PyTorch.</li> </ol>"},{"location":"reference/nn/#autobackend","title":"AutoBackend","text":"<p>         Bases: <code>nn.Module</code></p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>class AutoBackend(nn.Module):\ndef __init__(self, weights='yolov8n.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):\n\"\"\"\n        MultiBackend class for python inference on various platforms using Ultralytics YOLO.\n        Args:\n            weights (str): The path to the weights file. Default: 'yolov8n.pt'\n            device (torch.device): The device to run the model on.\n            dnn (bool): Use OpenCV's DNN module for inference if True, defaults to False.\n            data (dict): Additional data, optional\n            fp16 (bool): If True, use half precision. Default: False\n            fuse (bool): Whether to fuse the model or not. Default: True\n        Supported formats and their naming conventions:\n            | Format                | Suffix           |\n            |-----------------------|------------------|\n            | PyTorch               | *.pt             |\n            | TorchScript           | *.torchscript    |\n            | ONNX Runtime          | *.onnx           |\n            | ONNX OpenCV DNN       | *.onnx --dnn     |\n            | OpenVINO              | *.xml            |\n            | CoreML                | *.mlmodel        |\n            | TensorRT              | *.engine         |\n            | TensorFlow SavedModel | *_saved_model    |\n            | TensorFlow GraphDef   | *.pb             |\n            | TensorFlow Lite       | *.tflite         |\n            | TensorFlow Edge TPU   | *_edgetpu.tflite |\n            | PaddlePaddle          | *_paddle_model   |\n        \"\"\"\nsuper().__init__()\nw = str(weights[0] if isinstance(weights, list) else weights)\nnn_module = isinstance(weights, torch.nn.Module)\npt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)\nfp16 &amp;= pt or jit or onnx or engine or nn_module  # FP16\nnhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\nstride = 32  # default stride\nmodel = None  # TODO: resolves ONNX inference, verify effect on other backends\ncuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA\nif not (pt or triton or nn_module):\nw = attempt_download_asset(w)  # download if not local\n# NOTE: special case: in-memory pytorch model\nif nn_module:\nmodel = weights.to(device)\nmodel = model.fuse() if fuse else model\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nstride = max(int(model.stride.max()), 32)  # model stride\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\npt = True\nelif pt:  # PyTorch\nfrom ultralytics.nn.tasks import attempt_load_weights\nmodel = attempt_load_weights(weights if isinstance(weights, list) else w,\ndevice=device,\ninplace=True,\nfuse=fuse)\nstride = max(int(model.stride.max()), 32)  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\nelif jit:  # TorchScript\nLOGGER.info(f'Loading {w} for TorchScript inference...')\nextra_files = {'config.txt': ''}  # model metadata\nmodel = torch.jit.load(w, _extra_files=extra_files, map_location=device)\nmodel.half() if fp16 else model.float()\nif extra_files['config.txt']:  # load metadata dict\nd = json.loads(extra_files['config.txt'],\nobject_hook=lambda d: {int(k) if k.isdigit() else k: v\nfor k, v in d.items()})\nstride, names = int(d['stride']), d['names']\nelif dnn:  # ONNX OpenCV DNN\nLOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')\ncheck_requirements('opencv-python&gt;=4.5.4')\nnet = cv2.dnn.readNetFromONNX(w)\nelif onnx:  # ONNX Runtime\nLOGGER.info(f'Loading {w} for ONNX Runtime inference...')\ncheck_requirements(('onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'))\nimport onnxruntime\nproviders = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\nsession = onnxruntime.InferenceSession(w, providers=providers)\noutput_names = [x.name for x in session.get_outputs()]\nmeta = session.get_modelmeta().custom_metadata_map  # metadata\nif 'stride' in meta:\nstride, names = int(meta['stride']), eval(meta['names'])\nelif xml:  # OpenVINO\nLOGGER.info(f'Loading {w} for OpenVINO inference...')\ncheck_requirements('openvino')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\nfrom openvino.runtime import Core, Layout, get_batch  # noqa\nie = Core()\nif not Path(w).is_file():  # if not *.xml\nw = next(Path(w).glob('*.xml'))  # get *.xml file from *_openvino_model dir\nnetwork = ie.read_model(model=w, weights=Path(w).with_suffix('.bin'))\nif network.get_parameters()[0].get_layout().empty:\nnetwork.get_parameters()[0].set_layout(Layout(\"NCHW\"))\nbatch_dim = get_batch(network)\nif batch_dim.is_static:\nbatch_size = batch_dim.get_length()\nexecutable_network = ie.compile_model(network, device_name=\"CPU\")  # device_name=\"MYRIAD\" for Intel NCS2\nstride, names = self._load_metadata(Path(w).with_suffix('.yaml'))  # load metadata\nelif engine:  # TensorRT\nLOGGER.info(f'Loading {w} for TensorRT inference...')\nimport tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\ncheck_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\nif device.type == 'cpu':\ndevice = torch.device('cuda:0')\nBinding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\nlogger = trt.Logger(trt.Logger.INFO)\nwith open(w, 'rb') as f, trt.Runtime(logger) as runtime:\nmodel = runtime.deserialize_cuda_engine(f.read())\ncontext = model.create_execution_context()\nbindings = OrderedDict()\noutput_names = []\nfp16 = False  # default updated below\ndynamic = False\nfor i in range(model.num_bindings):\nname = model.get_binding_name(i)\ndtype = trt.nptype(model.get_binding_dtype(i))\nif model.binding_is_input(i):\nif -1 in tuple(model.get_binding_shape(i)):  # dynamic\ndynamic = True\ncontext.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\nif dtype == np.float16:\nfp16 = True\nelse:  # output\noutput_names.append(name)\nshape = tuple(context.get_binding_shape(i))\nim = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\nbindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\nbinding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\nbatch_size = bindings['images'].shape[0]  # if dynamic, this is instead max batch size\nelif coreml:  # CoreML\nLOGGER.info(f'Loading {w} for CoreML inference...')\nimport coremltools as ct\nmodel = ct.models.MLModel(w)\nelif saved_model:  # TF SavedModel\nLOGGER.info(f'Loading {w} for TensorFlow SavedModel inference...')\nimport tensorflow as tf\nkeras = False  # assume TF1 saved_model\nmodel = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\nelif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\nLOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')\nimport tensorflow as tf\ndef wrap_frozen_graph(gd, inputs, outputs):\nx = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped\nge = x.graph.as_graph_element\nreturn x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\ndef gd_outputs(gd):\nname_list, input_list = [], []\nfor node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef\nname_list.append(node.name)\ninput_list.extend(node.input)\nreturn sorted(f'{x}:0' for x in list(set(name_list) - set(input_list)) if not x.startswith('NoOp'))\ngd = tf.Graph().as_graph_def()  # TF GraphDef\nwith open(w, 'rb') as f:\ngd.ParseFromString(f.read())\nfrozen_func = wrap_frozen_graph(gd, inputs=\"x:0\", outputs=gd_outputs(gd))\nelif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\ntry:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nexcept ImportError:\nimport tensorflow as tf\nInterpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate,\nif edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\nLOGGER.info(f'Loading {w} for TensorFlow Lite Edge TPU inference...')\ndelegate = {\n'Linux': 'libedgetpu.so.1',\n'Darwin': 'libedgetpu.1.dylib',\n'Windows': 'edgetpu.dll'}[platform.system()]\ninterpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\nelse:  # TFLite\nLOGGER.info(f'Loading {w} for TensorFlow Lite inference...')\ninterpreter = Interpreter(model_path=w)  # load TFLite model\ninterpreter.allocate_tensors()  # allocate\ninput_details = interpreter.get_input_details()  # inputs\noutput_details = interpreter.get_output_details()  # outputs\nelif tfjs:  # TF.js\nraise NotImplementedError('ERROR: YOLOv8 TF.js inference is not supported')\nelif paddle:  # PaddlePaddle\nLOGGER.info(f'Loading {w} for PaddlePaddle inference...')\ncheck_requirements('paddlepaddle-gpu' if cuda else 'paddlepaddle')\nimport paddle.inference as pdi\nif not Path(w).is_file():  # if not *.pdmodel\nw = next(Path(w).rglob('*.pdmodel'))  # get *.xml file from *_openvino_model dir\nweights = Path(w).with_suffix('.pdiparams')\nconfig = pdi.Config(str(w), str(weights))\nif cuda:\nconfig.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\npredictor = pdi.create_predictor(config)\ninput_handle = predictor.get_input_handle(predictor.get_input_names()[0])\noutput_names = predictor.get_output_names()\nelif triton:  # NVIDIA Triton Inference Server\nLOGGER.info('Triton Inference Server not supported...')\n'''\n            TODO:\n            check_requirements('tritonclient[all]')\n            from utils.triton import TritonRemoteModel\n            model = TritonRemoteModel(url=w)\n            nhwc = model.runtime.startswith(\"tensorflow\")\n            '''\nelse:\nraise NotImplementedError(f\"ERROR: '{w}' is not a supported format. For supported formats see \"\nf\"https://docs.ultralytics.com/reference/nn/\")\n# class names\nif 'names' not in locals():  # names missing\nnames = yaml_load(data)['names'] if data else {i: f'class{i}' for i in range(999)}  # assign default\nnames = check_class_names(names)\nself.__dict__.update(locals())  # assign all variables to self\ndef forward(self, im, augment=False, visualize=False):\n\"\"\"\n        Runs inference on the YOLOv8 MultiBackend model.\n        Args:\n            im (torch.Tensor): The image tensor to perform inference on.\n            augment (bool): whether to perform data augmentation during inference, defaults to False\n            visualize (bool): whether to visualize the output predictions, defaults to False\n        Returns:\n            (tuple): Tuple containing the raw output tensor, and the processed output for visualization (if visualize=True)\n        \"\"\"\nb, ch, h, w = im.shape  # batch, channel, height, width\nif self.fp16 and im.dtype != torch.float16:\nim = im.half()  # to FP16\nif self.nhwc:\nim = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\nif self.pt or self.nn_module:  # PyTorch\ny = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\nelif self.jit:  # TorchScript\ny = self.model(im)\nelif self.dnn:  # ONNX OpenCV DNN\nim = im.cpu().numpy()  # torch to numpy\nself.net.setInput(im)\ny = self.net.forward()\nelif self.onnx:  # ONNX Runtime\nim = im.cpu().numpy()  # torch to numpy\ny = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\nelif self.xml:  # OpenVINO\nim = im.cpu().numpy()  # FP32\ny = list(self.executable_network([im]).values())\nelif self.engine:  # TensorRT\nif self.dynamic and im.shape != self.bindings['images'].shape:\ni = self.model.get_binding_index('images')\nself.context.set_binding_shape(i, im.shape)  # reshape if dynamic\nself.bindings['images'] = self.bindings['images']._replace(shape=im.shape)\nfor name in self.output_names:\ni = self.model.get_binding_index(name)\nself.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\ns = self.bindings['images'].shape\nassert im.shape == s, f\"input size {im.shape} {'&gt;' if self.dynamic else 'not equal to'} max model size {s}\"\nself.binding_addrs['images'] = int(im.data_ptr())\nself.context.execute_v2(list(self.binding_addrs.values()))\ny = [self.bindings[x].data for x in sorted(self.output_names)]\nelif self.coreml:  # CoreML\nim = im.cpu().numpy()\nim = Image.fromarray((im[0] * 255).astype('uint8'))\n# im = im.resize((192, 320), Image.ANTIALIAS)\ny = self.model.predict({'image': im})  # coordinates are xywh normalized\nif 'confidence' in y:\nbox = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\nconf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)\ny = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\nelse:\ny = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\nelif self.paddle:  # PaddlePaddle\nim = im.cpu().numpy().astype(np.float32)\nself.input_handle.copy_from_cpu(im)\nself.predictor.run()\ny = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\nelif self.triton:  # NVIDIA Triton Inference Server\ny = self.model(im)\nelse:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\nim = im.cpu().numpy()\nif self.saved_model:  # SavedModel\ny = self.model(im, training=False) if self.keras else self.model(im)\nelif self.pb:  # GraphDef\ny = self.frozen_func(x=self.tf.constant(im))\nelse:  # Lite or Edge TPU\ninput = self.input_details[0]\nint8 = input['dtype'] == np.uint8  # is TFLite quantized uint8 model\nif int8:\nscale, zero_point = input['quantization']\nim = (im / scale + zero_point).astype(np.uint8)  # de-scale\nself.interpreter.set_tensor(input['index'], im)\nself.interpreter.invoke()\ny = []\nfor output in self.output_details:\nx = self.interpreter.get_tensor(output['index'])\nif int8:\nscale, zero_point = output['quantization']\nx = (x.astype(np.float32) - zero_point) * scale  # re-scale\ny.append(x)\ny = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\ny[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\nif isinstance(y, (list, tuple)):\nreturn self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\nelse:\nreturn self.from_numpy(y)\ndef from_numpy(self, x):\n\"\"\"\n         Convert a numpy array to a tensor.\n         Args:\n             x (np.ndarray): The array to be converted.\n         Returns:\n             (torch.Tensor): The converted tensor\n         \"\"\"\nreturn torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x\ndef warmup(self, imgsz=(1, 3, 640, 640)):\n\"\"\"\n        Warm up the model by running one forward pass with a dummy input.\n        Args:\n            imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\n        Returns:\n            (None): This method runs the forward pass and don't return any value\n        \"\"\"\nwarmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module\nif any(warmup_types) and (self.device.type != 'cpu' or self.triton):\nim = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\nfor _ in range(2 if self.jit else 1):  #\nself.forward(im)  # warmup\n@staticmethod\ndef _model_type(p='path/to/model.pt'):\n\"\"\"\n        This function takes a path to a model file and returns the model type\n        Args:\n            p: path to the model file. Defaults to path/to/model.pt\n        \"\"\"\n# Return model type from model path, i.e. path='path/to/model.onnx' -&gt; type=onnx\n# types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]\nfrom ultralytics.yolo.engine.exporter import export_formats\nsf = list(export_formats().Suffix)  # export suffixes\nif not is_url(p, check=False) and not isinstance(p, str):\ncheck_suffix(p, sf)  # checks\nurl = urlparse(p)  # if url may be Triton inference server\ntypes = [s in Path(p).name for s in sf]\ntypes[8] &amp;= not types[9]  # tflite &amp;= not edgetpu\ntriton = not any(types) and all([any(s in url.scheme for s in [\"http\", \"grpc\"]), url.netloc])\nreturn types + [triton]\n@staticmethod\ndef _load_metadata(f=Path('path/to/meta.yaml')):\n\"\"\"\n        Loads the metadata from a yaml file\n        Args:\n            f: The path to the metadata file.\n        \"\"\"\n# Load metadata from meta.yaml if it exists\nif f.exists():\nd = yaml_load(f)\nreturn d['stride'], d['names']  # assign stride, names\nreturn None, None\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.__init__","title":"<code>__init__(weights='yolov8n.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True)</code>","text":"<p>MultiBackend class for python inference on various platforms using Ultralytics YOLO.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>The path to the weights file. Default: 'yolov8n.pt'</p> <code>'yolov8n.pt'</code> <code>device</code> <code>torch.device</code> <p>The device to run the model on.</p> <code>torch.device('cpu')</code> <code>dnn</code> <code>bool</code> <p>Use OpenCV's DNN module for inference if True, defaults to False.</p> <code>False</code> <code>data</code> <code>dict</code> <p>Additional data, optional</p> <code>None</code> <code>fp16</code> <code>bool</code> <p>If True, use half precision. Default: False</p> <code>False</code> <code>fuse</code> <code>bool</code> <p>Whether to fuse the model or not. Default: True</p> <code>True</code> Supported formats and their naming conventions Format Suffix PyTorch *.pt TorchScript *.torchscript ONNX Runtime *.onnx ONNX OpenCV DNN *.onnx --dnn OpenVINO *.xml CoreML *.mlmodel TensorRT *.engine TensorFlow SavedModel *_saved_model TensorFlow GraphDef *.pb TensorFlow Lite *.tflite TensorFlow Edge TPU *_edgetpu.tflite PaddlePaddle *_paddle_model Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def __init__(self, weights='yolov8n.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):\n\"\"\"\n    MultiBackend class for python inference on various platforms using Ultralytics YOLO.\n    Args:\n        weights (str): The path to the weights file. Default: 'yolov8n.pt'\n        device (torch.device): The device to run the model on.\n        dnn (bool): Use OpenCV's DNN module for inference if True, defaults to False.\n        data (dict): Additional data, optional\n        fp16 (bool): If True, use half precision. Default: False\n        fuse (bool): Whether to fuse the model or not. Default: True\n    Supported formats and their naming conventions:\n        | Format                | Suffix           |\n        |-----------------------|------------------|\n        | PyTorch               | *.pt             |\n        | TorchScript           | *.torchscript    |\n        | ONNX Runtime          | *.onnx           |\n        | ONNX OpenCV DNN       | *.onnx --dnn     |\n        | OpenVINO              | *.xml            |\n        | CoreML                | *.mlmodel        |\n        | TensorRT              | *.engine         |\n        | TensorFlow SavedModel | *_saved_model    |\n        | TensorFlow GraphDef   | *.pb             |\n        | TensorFlow Lite       | *.tflite         |\n        | TensorFlow Edge TPU   | *_edgetpu.tflite |\n        | PaddlePaddle          | *_paddle_model   |\n    \"\"\"\nsuper().__init__()\nw = str(weights[0] if isinstance(weights, list) else weights)\nnn_module = isinstance(weights, torch.nn.Module)\npt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)\nfp16 &amp;= pt or jit or onnx or engine or nn_module  # FP16\nnhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\nstride = 32  # default stride\nmodel = None  # TODO: resolves ONNX inference, verify effect on other backends\ncuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA\nif not (pt or triton or nn_module):\nw = attempt_download_asset(w)  # download if not local\n# NOTE: special case: in-memory pytorch model\nif nn_module:\nmodel = weights.to(device)\nmodel = model.fuse() if fuse else model\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nstride = max(int(model.stride.max()), 32)  # model stride\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\npt = True\nelif pt:  # PyTorch\nfrom ultralytics.nn.tasks import attempt_load_weights\nmodel = attempt_load_weights(weights if isinstance(weights, list) else w,\ndevice=device,\ninplace=True,\nfuse=fuse)\nstride = max(int(model.stride.max()), 32)  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\nelif jit:  # TorchScript\nLOGGER.info(f'Loading {w} for TorchScript inference...')\nextra_files = {'config.txt': ''}  # model metadata\nmodel = torch.jit.load(w, _extra_files=extra_files, map_location=device)\nmodel.half() if fp16 else model.float()\nif extra_files['config.txt']:  # load metadata dict\nd = json.loads(extra_files['config.txt'],\nobject_hook=lambda d: {int(k) if k.isdigit() else k: v\nfor k, v in d.items()})\nstride, names = int(d['stride']), d['names']\nelif dnn:  # ONNX OpenCV DNN\nLOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')\ncheck_requirements('opencv-python&gt;=4.5.4')\nnet = cv2.dnn.readNetFromONNX(w)\nelif onnx:  # ONNX Runtime\nLOGGER.info(f'Loading {w} for ONNX Runtime inference...')\ncheck_requirements(('onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'))\nimport onnxruntime\nproviders = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\nsession = onnxruntime.InferenceSession(w, providers=providers)\noutput_names = [x.name for x in session.get_outputs()]\nmeta = session.get_modelmeta().custom_metadata_map  # metadata\nif 'stride' in meta:\nstride, names = int(meta['stride']), eval(meta['names'])\nelif xml:  # OpenVINO\nLOGGER.info(f'Loading {w} for OpenVINO inference...')\ncheck_requirements('openvino')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\nfrom openvino.runtime import Core, Layout, get_batch  # noqa\nie = Core()\nif not Path(w).is_file():  # if not *.xml\nw = next(Path(w).glob('*.xml'))  # get *.xml file from *_openvino_model dir\nnetwork = ie.read_model(model=w, weights=Path(w).with_suffix('.bin'))\nif network.get_parameters()[0].get_layout().empty:\nnetwork.get_parameters()[0].set_layout(Layout(\"NCHW\"))\nbatch_dim = get_batch(network)\nif batch_dim.is_static:\nbatch_size = batch_dim.get_length()\nexecutable_network = ie.compile_model(network, device_name=\"CPU\")  # device_name=\"MYRIAD\" for Intel NCS2\nstride, names = self._load_metadata(Path(w).with_suffix('.yaml'))  # load metadata\nelif engine:  # TensorRT\nLOGGER.info(f'Loading {w} for TensorRT inference...')\nimport tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\ncheck_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\nif device.type == 'cpu':\ndevice = torch.device('cuda:0')\nBinding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\nlogger = trt.Logger(trt.Logger.INFO)\nwith open(w, 'rb') as f, trt.Runtime(logger) as runtime:\nmodel = runtime.deserialize_cuda_engine(f.read())\ncontext = model.create_execution_context()\nbindings = OrderedDict()\noutput_names = []\nfp16 = False  # default updated below\ndynamic = False\nfor i in range(model.num_bindings):\nname = model.get_binding_name(i)\ndtype = trt.nptype(model.get_binding_dtype(i))\nif model.binding_is_input(i):\nif -1 in tuple(model.get_binding_shape(i)):  # dynamic\ndynamic = True\ncontext.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\nif dtype == np.float16:\nfp16 = True\nelse:  # output\noutput_names.append(name)\nshape = tuple(context.get_binding_shape(i))\nim = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\nbindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\nbinding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\nbatch_size = bindings['images'].shape[0]  # if dynamic, this is instead max batch size\nelif coreml:  # CoreML\nLOGGER.info(f'Loading {w} for CoreML inference...')\nimport coremltools as ct\nmodel = ct.models.MLModel(w)\nelif saved_model:  # TF SavedModel\nLOGGER.info(f'Loading {w} for TensorFlow SavedModel inference...')\nimport tensorflow as tf\nkeras = False  # assume TF1 saved_model\nmodel = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\nelif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\nLOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')\nimport tensorflow as tf\ndef wrap_frozen_graph(gd, inputs, outputs):\nx = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped\nge = x.graph.as_graph_element\nreturn x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\ndef gd_outputs(gd):\nname_list, input_list = [], []\nfor node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef\nname_list.append(node.name)\ninput_list.extend(node.input)\nreturn sorted(f'{x}:0' for x in list(set(name_list) - set(input_list)) if not x.startswith('NoOp'))\ngd = tf.Graph().as_graph_def()  # TF GraphDef\nwith open(w, 'rb') as f:\ngd.ParseFromString(f.read())\nfrozen_func = wrap_frozen_graph(gd, inputs=\"x:0\", outputs=gd_outputs(gd))\nelif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\ntry:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nexcept ImportError:\nimport tensorflow as tf\nInterpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate,\nif edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\nLOGGER.info(f'Loading {w} for TensorFlow Lite Edge TPU inference...')\ndelegate = {\n'Linux': 'libedgetpu.so.1',\n'Darwin': 'libedgetpu.1.dylib',\n'Windows': 'edgetpu.dll'}[platform.system()]\ninterpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\nelse:  # TFLite\nLOGGER.info(f'Loading {w} for TensorFlow Lite inference...')\ninterpreter = Interpreter(model_path=w)  # load TFLite model\ninterpreter.allocate_tensors()  # allocate\ninput_details = interpreter.get_input_details()  # inputs\noutput_details = interpreter.get_output_details()  # outputs\nelif tfjs:  # TF.js\nraise NotImplementedError('ERROR: YOLOv8 TF.js inference is not supported')\nelif paddle:  # PaddlePaddle\nLOGGER.info(f'Loading {w} for PaddlePaddle inference...')\ncheck_requirements('paddlepaddle-gpu' if cuda else 'paddlepaddle')\nimport paddle.inference as pdi\nif not Path(w).is_file():  # if not *.pdmodel\nw = next(Path(w).rglob('*.pdmodel'))  # get *.xml file from *_openvino_model dir\nweights = Path(w).with_suffix('.pdiparams')\nconfig = pdi.Config(str(w), str(weights))\nif cuda:\nconfig.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\npredictor = pdi.create_predictor(config)\ninput_handle = predictor.get_input_handle(predictor.get_input_names()[0])\noutput_names = predictor.get_output_names()\nelif triton:  # NVIDIA Triton Inference Server\nLOGGER.info('Triton Inference Server not supported...')\n'''\n        TODO:\n        check_requirements('tritonclient[all]')\n        from utils.triton import TritonRemoteModel\n        model = TritonRemoteModel(url=w)\n        nhwc = model.runtime.startswith(\"tensorflow\")\n        '''\nelse:\nraise NotImplementedError(f\"ERROR: '{w}' is not a supported format. For supported formats see \"\nf\"https://docs.ultralytics.com/reference/nn/\")\n# class names\nif 'names' not in locals():  # names missing\nnames = yaml_load(data)['names'] if data else {i: f'class{i}' for i in range(999)}  # assign default\nnames = check_class_names(names)\nself.__dict__.update(locals())  # assign all variables to self\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.forward","title":"<code>forward(im, augment=False, visualize=False)</code>","text":"<p>Runs inference on the YOLOv8 MultiBackend model.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>torch.Tensor</code> <p>The image tensor to perform inference on.</p> required <code>augment</code> <code>bool</code> <p>whether to perform data augmentation during inference, defaults to False</p> <code>False</code> <code>visualize</code> <code>bool</code> <p>whether to visualize the output predictions, defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing the raw output tensor, and the processed output for visualization (if visualize=True)</p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def forward(self, im, augment=False, visualize=False):\n\"\"\"\n    Runs inference on the YOLOv8 MultiBackend model.\n    Args:\n        im (torch.Tensor): The image tensor to perform inference on.\n        augment (bool): whether to perform data augmentation during inference, defaults to False\n        visualize (bool): whether to visualize the output predictions, defaults to False\n    Returns:\n        (tuple): Tuple containing the raw output tensor, and the processed output for visualization (if visualize=True)\n    \"\"\"\nb, ch, h, w = im.shape  # batch, channel, height, width\nif self.fp16 and im.dtype != torch.float16:\nim = im.half()  # to FP16\nif self.nhwc:\nim = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\nif self.pt or self.nn_module:  # PyTorch\ny = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\nelif self.jit:  # TorchScript\ny = self.model(im)\nelif self.dnn:  # ONNX OpenCV DNN\nim = im.cpu().numpy()  # torch to numpy\nself.net.setInput(im)\ny = self.net.forward()\nelif self.onnx:  # ONNX Runtime\nim = im.cpu().numpy()  # torch to numpy\ny = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\nelif self.xml:  # OpenVINO\nim = im.cpu().numpy()  # FP32\ny = list(self.executable_network([im]).values())\nelif self.engine:  # TensorRT\nif self.dynamic and im.shape != self.bindings['images'].shape:\ni = self.model.get_binding_index('images')\nself.context.set_binding_shape(i, im.shape)  # reshape if dynamic\nself.bindings['images'] = self.bindings['images']._replace(shape=im.shape)\nfor name in self.output_names:\ni = self.model.get_binding_index(name)\nself.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\ns = self.bindings['images'].shape\nassert im.shape == s, f\"input size {im.shape} {'&gt;' if self.dynamic else 'not equal to'} max model size {s}\"\nself.binding_addrs['images'] = int(im.data_ptr())\nself.context.execute_v2(list(self.binding_addrs.values()))\ny = [self.bindings[x].data for x in sorted(self.output_names)]\nelif self.coreml:  # CoreML\nim = im.cpu().numpy()\nim = Image.fromarray((im[0] * 255).astype('uint8'))\n# im = im.resize((192, 320), Image.ANTIALIAS)\ny = self.model.predict({'image': im})  # coordinates are xywh normalized\nif 'confidence' in y:\nbox = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\nconf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)\ny = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\nelse:\ny = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\nelif self.paddle:  # PaddlePaddle\nim = im.cpu().numpy().astype(np.float32)\nself.input_handle.copy_from_cpu(im)\nself.predictor.run()\ny = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\nelif self.triton:  # NVIDIA Triton Inference Server\ny = self.model(im)\nelse:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\nim = im.cpu().numpy()\nif self.saved_model:  # SavedModel\ny = self.model(im, training=False) if self.keras else self.model(im)\nelif self.pb:  # GraphDef\ny = self.frozen_func(x=self.tf.constant(im))\nelse:  # Lite or Edge TPU\ninput = self.input_details[0]\nint8 = input['dtype'] == np.uint8  # is TFLite quantized uint8 model\nif int8:\nscale, zero_point = input['quantization']\nim = (im / scale + zero_point).astype(np.uint8)  # de-scale\nself.interpreter.set_tensor(input['index'], im)\nself.interpreter.invoke()\ny = []\nfor output in self.output_details:\nx = self.interpreter.get_tensor(output['index'])\nif int8:\nscale, zero_point = output['quantization']\nx = (x.astype(np.float32) - zero_point) * scale  # re-scale\ny.append(x)\ny = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\ny[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\nif isinstance(y, (list, tuple)):\nreturn self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\nelse:\nreturn self.from_numpy(y)\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.from_numpy","title":"<code>from_numpy(x)</code>","text":"<p>Convert a numpy array to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>The array to be converted.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The converted tensor</p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def from_numpy(self, x):\n\"\"\"\n     Convert a numpy array to a tensor.\n     Args:\n         x (np.ndarray): The array to be converted.\n     Returns:\n         (torch.Tensor): The converted tensor\n     \"\"\"\nreturn torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.warmup","title":"<code>warmup(imgsz=(1, 3, 640, 640))</code>","text":"<p>Warm up the model by running one forward pass with a dummy input.</p> <p>Parameters:</p> Name Type Description Default <code>imgsz</code> <code>tuple</code> <p>The shape of the dummy input tensor in the format (batch_size, channels, height, width)</p> <code>(1, 3, 640, 640)</code> <p>Returns:</p> Type Description <code>None</code> <p>This method runs the forward pass and don't return any value</p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def warmup(self, imgsz=(1, 3, 640, 640)):\n\"\"\"\n    Warm up the model by running one forward pass with a dummy input.\n    Args:\n        imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\n    Returns:\n        (None): This method runs the forward pass and don't return any value\n    \"\"\"\nwarmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module\nif any(warmup_types) and (self.device.type != 'cpu' or self.triton):\nim = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\nfor _ in range(2 if self.jit else 1):  #\nself.forward(im)  # warmup\n</code></pre>"},{"location":"reference/nn/#basemodel","title":"BaseModel","text":"<p>         Bases: <code>nn.Module</code></p> <p>The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class BaseModel(nn.Module):\n\"\"\"\n    The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.\n    \"\"\"\ndef forward(self, x, profile=False, visualize=False):\n\"\"\"\n        Forward pass of the model on a single scale.\n        Wrapper for `_forward_once` method.\n        Args:\n            x (torch.Tensor): The input image tensor\n            profile (bool): Whether to profile the model, defaults to False\n            visualize (bool): Whether to return the intermediate feature maps, defaults to False\n        Returns:\n            (torch.Tensor): The output of the network.\n        \"\"\"\nreturn self._forward_once(x, profile, visualize)\ndef _forward_once(self, x, profile=False, visualize=False):\n\"\"\"\n        Perform a forward pass through the network.\n        Args:\n            x (torch.Tensor): The input tensor to the model\n            profile (bool):  Print the computation time of each layer if True, defaults to False.\n            visualize (bool): Save the feature maps of the model if True, defaults to False\n        Returns:\n            (torch.Tensor): The last output of the model.\n        \"\"\"\ny, dt = [], []  # outputs\nfor m in self.model:\nif m.f != -1:  # if not from previous layer\nx = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\nif profile:\nself._profile_one_layer(m, x, dt)\nx = m(x)  # run\ny.append(x if m.i in self.save else None)  # save output\nif visualize:\nLOGGER.info('visualize feature not yet supported')\n# TODO: feature_visualization(x, m.type, m.i, save_dir=visualize)\nreturn x\ndef _profile_one_layer(self, m, x, dt):\n\"\"\"\n        Profile the computation time and FLOPs of a single layer of the model on a given input.\n        Appends the results to the provided list.\n        Args:\n            m (nn.Module): The layer to be profiled.\n            x (torch.Tensor): The input data to the layer.\n            dt (list): A list to store the computation time of the layer.\n        Returns:\n            None\n        \"\"\"\nc = m == self.model[-1]  # is final layer, copy input as inplace fix\no = thop.profile(m, inputs=(x.clone() if c else x,), verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPs\nt = time_sync()\nfor _ in range(10):\nm(x.clone() if c else x)\ndt.append((time_sync() - t) * 100)\nif m == self.model[0]:\nLOGGER.info(f\"{'time (ms)':&gt;10s} {'GFLOPs':&gt;10s} {'params':&gt;10s}  module\")\nLOGGER.info(f'{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}')\nif c:\nLOGGER.info(f\"{sum(dt):10.2f} {'-':&gt;10s} {'-':&gt;10s}  Total\")\ndef fuse(self):\n\"\"\"\n        Fuse the `Conv2d()` and `BatchNorm2d()` layers of the model into a single layer, in order to improve the\n        computation efficiency.\n        Returns:\n            (nn.Module): The fused model is returned.\n        \"\"\"\nif not self.is_fused():\nfor m in self.model.modules():\nif isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):\nm.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nif isinstance(m, ConvTranspose) and hasattr(m, 'bn'):\nm.conv_transpose = fuse_deconv_and_bn(m.conv_transpose, m.bn)\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nself.info()\nreturn self\ndef is_fused(self, thresh=10):\n\"\"\"\n        Check if the model has less than a certain threshold of BatchNorm layers.\n        Args:\n            thresh (int, optional): The threshold number of BatchNorm layers. Default is 10.\n        Returns:\n            (bool): True if the number of BatchNorm layers in the model is less than the threshold, False otherwise.\n        \"\"\"\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nreturn sum(isinstance(v, bn) for v in self.modules()) &lt; thresh  # True if &lt; 'thresh' BatchNorm layers in model\ndef info(self, verbose=False, imgsz=640):\n\"\"\"\n        Prints model information\n        Args:\n            verbose (bool): if True, prints out the model information. Defaults to False\n            imgsz (int): the size of the image that the model will be trained on. Defaults to 640\n        \"\"\"\nmodel_info(self, verbose, imgsz)\ndef _apply(self, fn):\n\"\"\"\n        `_apply()` is a function that applies a function to all the tensors in the model that are not\n        parameters or registered buffers\n        Args:\n            fn: the function to apply to the model\n        Returns:\n            A model that is a Detect() object.\n        \"\"\"\nself = super()._apply(fn)\nm = self.model[-1]  # Detect()\nif isinstance(m, (Detect, Segment)):\nm.stride = fn(m.stride)\nm.anchors = fn(m.anchors)\nm.strides = fn(m.strides)\nreturn self\ndef load(self, weights):\n\"\"\"\n        This function loads the weights of the model from a file\n        Args:\n            weights (str): The weights to load into the model.\n        \"\"\"\n# Force all tasks to implement this function\nraise NotImplementedError(\"This function needs to be implemented by derived classes!\")\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.forward","title":"<code>forward(x, profile=False, visualize=False)</code>","text":"<p>Forward pass of the model on a single scale. Wrapper for <code>_forward_once</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>The input image tensor</p> required <code>profile</code> <code>bool</code> <p>Whether to profile the model, defaults to False</p> <code>False</code> <code>visualize</code> <code>bool</code> <p>Whether to return the intermediate feature maps, defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The output of the network.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def forward(self, x, profile=False, visualize=False):\n\"\"\"\n    Forward pass of the model on a single scale.\n    Wrapper for `_forward_once` method.\n    Args:\n        x (torch.Tensor): The input image tensor\n        profile (bool): Whether to profile the model, defaults to False\n        visualize (bool): Whether to return the intermediate feature maps, defaults to False\n    Returns:\n        (torch.Tensor): The output of the network.\n    \"\"\"\nreturn self._forward_once(x, profile, visualize)\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.fuse","title":"<code>fuse()</code>","text":"<p>Fuse the <code>Conv2d()</code> and <code>BatchNorm2d()</code> layers of the model into a single layer, in order to improve the computation efficiency.</p> <p>Returns:</p> Type Description <code>nn.Module</code> <p>The fused model is returned.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def fuse(self):\n\"\"\"\n    Fuse the `Conv2d()` and `BatchNorm2d()` layers of the model into a single layer, in order to improve the\n    computation efficiency.\n    Returns:\n        (nn.Module): The fused model is returned.\n    \"\"\"\nif not self.is_fused():\nfor m in self.model.modules():\nif isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):\nm.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nif isinstance(m, ConvTranspose) and hasattr(m, 'bn'):\nm.conv_transpose = fuse_deconv_and_bn(m.conv_transpose, m.bn)\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nself.info()\nreturn self\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.info","title":"<code>info(verbose=False, imgsz=640)</code>","text":"<p>Prints model information</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>if True, prints out the model information. Defaults to False</p> <code>False</code> <code>imgsz</code> <code>int</code> <p>the size of the image that the model will be trained on. Defaults to 640</p> <code>640</code> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def info(self, verbose=False, imgsz=640):\n\"\"\"\n    Prints model information\n    Args:\n        verbose (bool): if True, prints out the model information. Defaults to False\n        imgsz (int): the size of the image that the model will be trained on. Defaults to 640\n    \"\"\"\nmodel_info(self, verbose, imgsz)\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.is_fused","title":"<code>is_fused(thresh=10)</code>","text":"<p>Check if the model has less than a certain threshold of BatchNorm layers.</p> <p>Parameters:</p> Name Type Description Default <code>thresh</code> <code>int</code> <p>The threshold number of BatchNorm layers. Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the number of BatchNorm layers in the model is less than the threshold, False otherwise.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def is_fused(self, thresh=10):\n\"\"\"\n    Check if the model has less than a certain threshold of BatchNorm layers.\n    Args:\n        thresh (int, optional): The threshold number of BatchNorm layers. Default is 10.\n    Returns:\n        (bool): True if the number of BatchNorm layers in the model is less than the threshold, False otherwise.\n    \"\"\"\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nreturn sum(isinstance(v, bn) for v in self.modules()) &lt; thresh  # True if &lt; 'thresh' BatchNorm layers in model\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.load","title":"<code>load(weights)</code>","text":"<p>This function loads the weights of the model from a file</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>The weights to load into the model.</p> required Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def load(self, weights):\n\"\"\"\n    This function loads the weights of the model from a file\n    Args:\n        weights (str): The weights to load into the model.\n    \"\"\"\n# Force all tasks to implement this function\nraise NotImplementedError(\"This function needs to be implemented by derived classes!\")\n</code></pre>"},{"location":"reference/nn/#modules","title":"Modules","text":"<p>TODO</p>"},{"location":"reference/ops/","title":"Operations","text":"<p>This module contains optimized deep learning related operations used in the Ultralytics YOLO framework</p>"},{"location":"reference/ops/#non-max-suppression","title":"Non-max suppression","text":"<p>Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>A tensor of shape (batch_size, num_boxes, num_classes + 4 + num_masks) containing the predicted boxes, classes, and masks. The tensor should be in the format output by a model, such as YOLO.</p> required <code>conf_thres</code> <code>float</code> <p>The confidence threshold below which boxes will be filtered out. Valid values are between 0.0 and 1.0.</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>The IoU threshold below which boxes will be filtered out during NMS. Valid values are between 0.0 and 1.0.</p> <code>0.45</code> <code>classes</code> <code>List[int]</code> <p>A list of class indices to consider. If None, all classes will be considered.</p> <code>None</code> <code>agnostic</code> <code>bool</code> <p>If True, the model is agnostic to the number of classes, and all classes will be considered as one.</p> <code>False</code> <code>multi_label</code> <code>bool</code> <p>If True, each box may have multiple labels.</p> <code>False</code> <code>labels</code> <code>List[List[Union[int, float, torch.Tensor]]]</code> <p>A list of lists, where each inner list contains the apriori labels for a given image. The list should be in the format output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).</p> <code>()</code> <code>max_det</code> <code>int</code> <p>The maximum number of boxes to keep after NMS.</p> <code>300</code> <code>nm</code> <code>int</code> <p>The number of masks output by the model.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[torch.Tensor]</code> <p>A list of length batch_size, where each element is a tensor of shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def non_max_suppression(\nprediction,\nconf_thres=0.25,\niou_thres=0.45,\nclasses=None,\nagnostic=False,\nmulti_label=False,\nlabels=(),\nmax_det=300,\nnm=0,  # number of masks\n):\n\"\"\"\n    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.\n    Arguments:\n        prediction (torch.Tensor): A tensor of shape (batch_size, num_boxes, num_classes + 4 + num_masks)\n            containing the predicted boxes, classes, and masks. The tensor should be in the format\n            output by a model, such as YOLO.\n        conf_thres (float): The confidence threshold below which boxes will be filtered out.\n            Valid values are between 0.0 and 1.0.\n        iou_thres (float): The IoU threshold below which boxes will be filtered out during NMS.\n            Valid values are between 0.0 and 1.0.\n        classes (List[int]): A list of class indices to consider. If None, all classes will be considered.\n        agnostic (bool): If True, the model is agnostic to the number of classes, and all\n            classes will be considered as one.\n        multi_label (bool): If True, each box may have multiple labels.\n        labels (List[List[Union[int, float, torch.Tensor]]]): A list of lists, where each inner\n            list contains the apriori labels for a given image. The list should be in the format\n            output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).\n        max_det (int): The maximum number of boxes to keep after NMS.\n        nm (int): The number of masks output by the model.\n    Returns:\n        (List[torch.Tensor]): A list of length batch_size, where each element is a tensor of\n            shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns\n            (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).\n    \"\"\"\n# Checks\nassert 0 &lt;= conf_thres &lt;= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\nassert 0 &lt;= iou_thres &lt;= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\nif isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)\nprediction = prediction[0]  # select only inference output\ndevice = prediction.device\nmps = 'mps' in device.type  # Apple MPS\nif mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\nprediction = prediction.cpu()\nbs = prediction.shape[0]  # batch size\nnc = prediction.shape[1] - nm - 4  # number of classes\nmi = 4 + nc  # mask start index\nxc = prediction[:, 4:mi].amax(1) &gt; conf_thres  # candidates\n# Settings\n# min_wh = 2  # (pixels) minimum box width and height\nmax_wh = 7680  # (pixels) maximum box width and height\nmax_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\ntime_limit = 0.5 + 0.05 * bs  # seconds to quit after\nredundant = True  # require redundant detections\nmulti_label &amp;= nc &gt; 1  # multiple labels per box (adds 0.5ms/img)\nmerge = False  # use merge-NMS\nt = time.time()\noutput = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\nfor xi, x in enumerate(prediction):  # image index, image inference\n# Apply constraints\n# x[((x[:, 2:4] &lt; min_wh) | (x[:, 2:4] &gt; max_wh)).any(1), 4] = 0  # width-height\nx = x.transpose(0, -1)[xc[xi]]  # confidence\n# Cat apriori labels if autolabelling\nif labels and len(labels[xi]):\nlb = labels[xi]\nv = torch.zeros((len(lb), nc + nm + 5), device=x.device)\nv[:, :4] = lb[:, 1:5]  # box\nv[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\nx = torch.cat((x, v), 0)\n# If none remain process next image\nif not x.shape[0]:\ncontinue\n# Detections matrix nx6 (xyxy, conf, cls)\nbox, cls, mask = x.split((4, nc, nm), 1)\nbox = xywh2xyxy(box)  # center_x, center_y, width, height) to (x1, y1, x2, y2)\nif multi_label:\ni, j = (cls &gt; conf_thres).nonzero(as_tuple=False).T\nx = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\nelse:  # best class only\nconf, j = cls.max(1, keepdim=True)\nx = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) &gt; conf_thres]\n# Filter by class\nif classes is not None:\nx = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n# Apply finite constraint\n# if not torch.isfinite(x).all():\n#     x = x[torch.isfinite(x).all(1)]\n# Check shape\nn = x.shape[0]  # number of boxes\nif not n:  # no boxes\ncontinue\nx = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n# Batched NMS\nc = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\nboxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\ni = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\ni = i[:max_det]  # limit detections\nif merge and (1 &lt; n &lt; 3E3):  # Merge NMS (boxes merged using weighted mean)\n# update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\niou = box_iou(boxes[i], boxes) &gt; iou_thres  # iou matrix\nweights = iou * scores[None]  # box weights\nx[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\nif redundant:\ni = i[iou.sum(1) &gt; 1]  # require redundancy\noutput[xi] = x[i]\nif mps:\noutput[xi] = output[xi].to(device)\nif (time.time() - t) &gt; time_limit:\nLOGGER.warning(f'WARNING \u26a0\ufe0f NMS time limit {time_limit:.3f}s exceeded')\nbreak  # time limit exceeded\nreturn output\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#scale-boxes","title":"Scale boxes","text":"<p>Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in (img1_shape) to the shape of a different image (img0_shape).</p> <p>Parameters:</p> Name Type Description Default <code>img1_shape</code> <code>tuple</code> <p>The shape of the image that the bounding boxes are for, in the format of (height, width).</p> required <code>boxes</code> <code>torch.Tensor</code> <p>the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)</p> required <code>img0_shape</code> <code>tuple</code> <p>the shape of the target image, in the format of (height, width).</p> required <code>ratio_pad</code> <code>tuple</code> <p>a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be                  calculated based on the size difference between the two images.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>boxes</code> <code>torch.Tensor</code> <p>The scaled bounding boxes, in the format of (x1, y1, x2, y2)</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n\"\"\"\n    Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in\n    (img1_shape) to the shape of a different image (img0_shape).\n    Args:\n      img1_shape (tuple): The shape of the image that the bounding boxes are for, in the format of (height, width).\n      boxes (torch.Tensor): the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)\n      img0_shape (tuple): the shape of the target image, in the format of (height, width).\n      ratio_pad (tuple): a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be\n                         calculated based on the size difference between the two images.\n    Returns:\n      boxes (torch.Tensor): The scaled bounding boxes, in the format of (x1, y1, x2, y2)\n    \"\"\"\nif ratio_pad is None:  # calculate from img0_shape\ngain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\npad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\nelse:\ngain = ratio_pad[0][0]\npad = ratio_pad[1]\nboxes[..., [0, 2]] -= pad[0]  # x padding\nboxes[..., [1, 3]] -= pad[1]  # y padding\nboxes[..., :4] /= gain\nclip_boxes(boxes, img0_shape)\nreturn boxes\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#scale-image","title":"Scale image","text":"<p>Takes a mask, and resizes it to the original image size</p> <p>Parameters:</p> Name Type Description Default <code>im1_shape</code> <code>tuple</code> <p>model input shape, [h, w]</p> required <code>masks</code> <code>torch.Tensor</code> <p>[h, w, num]</p> required <code>im0_shape</code> <code>tuple</code> <p>the original image shape</p> required <code>ratio_pad</code> <code>tuple</code> <p>the ratio of the padding to the original image.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>masks</code> <code>torch.Tensor</code> <p>The masks that are being returned.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def scale_image(im1_shape, masks, im0_shape, ratio_pad=None):\n\"\"\"\n    Takes a mask, and resizes it to the original image size\n    Args:\n      im1_shape (tuple): model input shape, [h, w]\n      masks (torch.Tensor): [h, w, num]\n      im0_shape (tuple): the original image shape\n      ratio_pad (tuple): the ratio of the padding to the original image.\n    Returns:\n      masks (torch.Tensor): The masks that are being returned.\n    \"\"\"\n# Rescale coordinates (xyxy) from im1_shape to im0_shape\nif ratio_pad is None:  # calculate from im0_shape\ngain = min(im1_shape[0] / im0_shape[0], im1_shape[1] / im0_shape[1])  # gain  = old / new\npad = (im1_shape[1] - im0_shape[1] * gain) / 2, (im1_shape[0] - im0_shape[0] * gain) / 2  # wh padding\nelse:\npad = ratio_pad[1]\ntop, left = int(pad[1]), int(pad[0])  # y, x\nbottom, right = int(im1_shape[0] - pad[1]), int(im1_shape[1] - pad[0])\nif len(masks.shape) &lt; 2:\nraise ValueError(f'\"len of masks shape\" should be 2 or 3, but got {len(masks.shape)}')\nmasks = masks[top:bottom, left:right]\n# masks = masks.permute(2, 0, 1).contiguous()\n# masks = F.interpolate(masks[None], im0_shape[:2], mode='bilinear', align_corners=False)[0]\n# masks = masks.permute(1, 2, 0).contiguous()\nmasks = cv2.resize(masks, (im0_shape[1], im0_shape[0]))\nif len(masks.shape) == 2:\nmasks = masks[:, :, None]\nreturn masks\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#clip-boxes","title":"clip boxes","text":"<p>It takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>torch.Tensor</code> <p>the bounding boxes to clip</p> required <code>shape</code> <code>tuple</code> <p>the shape of the image</p> required Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def clip_boxes(boxes, shape):\n\"\"\"\n    It takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the\n    shape\n    Args:\n      boxes (torch.Tensor): the bounding boxes to clip\n      shape (tuple): the shape of the image\n    \"\"\"\nif isinstance(boxes, torch.Tensor):  # faster individually\nboxes[..., 0].clamp_(0, shape[1])  # x1\nboxes[..., 1].clamp_(0, shape[0])  # y1\nboxes[..., 2].clamp_(0, shape[1])  # x2\nboxes[..., 3].clamp_(0, shape[0])  # y2\nelse:  # np.array (faster grouped)\nboxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\nboxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#box-format-conversion","title":"Box Format Conversion","text":""},{"location":"reference/ops/#xyxy2xywh","title":"xyxy2xywh","text":"<p>Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>The input bounding box coordinates in (x1, y1, x2, y2) format.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>The bounding box coordinates in (x, y, width, height) format.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def xyxy2xywh(x):\n\"\"\"\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n       y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\ny[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\ny[..., 2] = x[..., 2] - x[..., 0]  # width\ny[..., 3] = x[..., 3] - x[..., 1]  # height\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#xywh2xyxy","title":"xywh2xyxy","text":"<p>Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>The input bounding box coordinates in (x, y, width, height) format.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>The bounding box coordinates in (x1, y1, x2, y2) format.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def xywh2xyxy(x):\n\"\"\"\n    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n    Returns:\n        y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\ny[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\ny[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\ny[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#xywhn2xyxy","title":"xywhn2xyxy","text":"<p>Convert normalized bounding box coordinates to pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>The bounding box coordinates.</p> required <code>w</code> <code>int</code> <p>Width of the image. Defaults to 640</p> <code>640</code> <code>h</code> <code>int</code> <p>Height of the image. Defaults to 640</p> <code>640</code> <code>padw</code> <code>int</code> <p>Padding width. Defaults to 0</p> <code>0</code> <code>padh</code> <code>int</code> <p>Padding height. Defaults to 0</p> <code>0</code> <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>The coordinates of the bounding box in the format [x1, y1, x2, y2] where x1,y1 is the top-left corner, x2,y2 is the bottom-right corner of the bounding box.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n\"\"\"\n    Convert normalized bounding box coordinates to pixel coordinates.\n    Args:\n        x (np.ndarray) or (torch.Tensor): The bounding box coordinates.\n        w (int): Width of the image. Defaults to 640\n        h (int): Height of the image. Defaults to 640\n        padw (int): Padding width. Defaults to 0\n        padh (int): Padding height. Defaults to 0\n    Returns:\n        y (np.ndarray) or (torch.Tensor): The coordinates of the bounding box in the format [x1, y1, x2, y2] where\n            x1,y1 is the top-left corner, x2,y2 is the bottom-right corner of the bounding box.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 0] = w * (x[..., 0] - x[..., 2] / 2) + padw  # top left x\ny[..., 1] = h * (x[..., 1] - x[..., 3] / 2) + padh  # top left y\ny[..., 2] = w * (x[..., 0] + x[..., 2] / 2) + padw  # bottom right x\ny[..., 3] = h * (x[..., 1] + x[..., 3] / 2) + padh  # bottom right y\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#xyxy2xywhn","title":"xyxy2xywhn","text":"<p>Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format. x, y, width and height are normalized to image dimensions</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>The input bounding box coordinates in (x1, y1, x2, y2) format.</p> required <code>w</code> <code>int</code> <p>The width of the image. Defaults to 640</p> <code>640</code> <code>h</code> <code>int</code> <p>The height of the image. Defaults to 640</p> <code>640</code> <code>clip</code> <code>bool</code> <p>If True, the boxes will be clipped to the image boundaries. Defaults to False</p> <code>False</code> <code>eps</code> <code>float</code> <p>The minimum value of the box's width and height. Defaults to 0.0</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>The bounding box coordinates in (x, y, width, height, normalized) format</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n\"\"\"\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format.\n    x, y, width and height are normalized to image dimensions\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n        w (int): The width of the image. Defaults to 640\n        h (int): The height of the image. Defaults to 640\n        clip (bool): If True, the boxes will be clipped to the image boundaries. Defaults to False\n        eps (float): The minimum value of the box's width and height. Defaults to 0.0\n    Returns:\n        y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x, y, width, height, normalized) format\n    \"\"\"\nif clip:\nclip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\ny[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\ny[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\ny[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#xyn2xy","title":"xyn2xy","text":"<p>Convert normalized coordinates to pixel coordinates of shape (n,2)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>The input tensor of normalized bounding box coordinates</p> required <code>w</code> <code>int</code> <p>The width of the image. Defaults to 640</p> <code>640</code> <code>h</code> <code>int</code> <p>The height of the image. Defaults to 640</p> <code>640</code> <code>padw</code> <code>int</code> <p>The width of the padding. Defaults to 0</p> <code>0</code> <code>padh</code> <code>int</code> <p>The height of the padding. Defaults to 0</p> <code>0</code> <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>The x and y coordinates of the top left corner of the bounding box</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def xyn2xy(x, w=640, h=640, padw=0, padh=0):\n\"\"\"\n    Convert normalized coordinates to pixel coordinates of shape (n,2)\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input tensor of normalized bounding box coordinates\n        w (int): The width of the image. Defaults to 640\n        h (int): The height of the image. Defaults to 640\n        padw (int): The width of the padding. Defaults to 0\n        padh (int): The height of the padding. Defaults to 0\n    Returns:\n        y (np.ndarray) or (torch.Tensor): The x and y coordinates of the top left corner of the bounding box\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 0] = w * x[..., 0] + padw  # top left x\ny[..., 1] = h * x[..., 1] + padh  # top left y\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#xywh2ltwh","title":"xywh2ltwh","text":"<p>Convert the bounding box format from [x, y, w, h] to [x1, y1, w, h], where x1, y1 are the top-left coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>The input tensor with the bounding box coordinates in the xywh format</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>The bounding box coordinates in the xyltwh format</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def xywh2ltwh(x):\n\"\"\"\n    Convert the bounding box format from [x, y, w, h] to [x1, y1, w, h], where x1, y1 are the top-left coordinates.\n    Args:\n        x (np.ndarray) or (torch.Tensor): The input tensor with the bounding box coordinates in the xywh format\n    Returns:\n        y (np.ndarray) or (torch.Tensor): The bounding box coordinates in the xyltwh format\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\ny[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#xyxy2ltwh","title":"xyxy2ltwh","text":"<p>Convert nx4 bounding boxes from [x1, y1, x2, y2] to [x1, y1, w, h], where xy1=top-left, xy2=bottom-right</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>The input tensor with the bounding boxes coordinates in the xyxy format</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>The bounding box coordinates in the xyltwh format.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def xyxy2ltwh(x):\n\"\"\"\n    Convert nx4 bounding boxes from [x1, y1, x2, y2] to [x1, y1, w, h], where xy1=top-left, xy2=bottom-right\n    Args:\n      x (np.ndarray) or (torch.Tensor): The input tensor with the bounding boxes coordinates in the xyxy format\n    Returns:\n      y (np.ndarray) or (torch.Tensor): The bounding box coordinates in the xyltwh format.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[:, 2] = x[:, 2] - x[:, 0]  # width\ny[:, 3] = x[:, 3] - x[:, 1]  # height\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#ltwh2xywh","title":"ltwh2xywh","text":"<p>Convert nx4 boxes from [x1, y1, w, h] to [x, y, w, h] where xy1=top-left, xy=center</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>the input tensor</p> required Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def ltwh2xywh(x):\n\"\"\"\n    Convert nx4 boxes from [x1, y1, w, h] to [x, y, w, h] where xy1=top-left, xy=center\n    Args:\n      x (torch.Tensor): the input tensor\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[:, 0] = x[:, 0] + x[:, 2] / 2  # center x\ny[:, 1] = x[:, 1] + x[:, 3] / 2  # center y\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#ltwh2xyxy","title":"ltwh2xyxy","text":"<p>It converts the bounding box from [x1, y1, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray) or (torch.Tensor</code> <p>the input image</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>np.ndarray) or (torch.Tensor</code> <p>the xyxy coordinates of the bounding boxes.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def ltwh2xyxy(x):\n\"\"\"\n    It converts the bounding box from [x1, y1, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    Args:\n      x (np.ndarray) or (torch.Tensor): the input image\n    Returns:\n      y (np.ndarray) or (torch.Tensor): the xyxy coordinates of the bounding boxes.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[:, 2] = x[:, 2] + x[:, 0]  # width\ny[:, 3] = x[:, 3] + x[:, 1]  # height\nreturn y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#segment2box","title":"segment2box","text":"<p>Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <code>torch.Tensor</code> <p>the segment label</p> required <code>width</code> <code>int</code> <p>the width of the image. Defaults to 640</p> <code>640</code> <code>height</code> <code>int</code> <p>The height of the image. Defaults to 640</p> <code>640</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>the minimum and maximum x and y values of the segment.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def segment2box(segment, width=640, height=640):\n\"\"\"\n    Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\n    Args:\n      segment (torch.Tensor): the segment label\n      width (int): the width of the image. Defaults to 640\n      height (int): The height of the image. Defaults to 640\n    Returns:\n      (np.ndarray): the minimum and maximum x and y values of the segment.\n    \"\"\"\n# Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\nx, y = segment.T  # segment xy\ninside = (x &gt;= 0) &amp; (y &gt;= 0) &amp; (x &lt;= width) &amp; (y &lt;= height)\nx, y, = x[inside], y[inside]\nreturn np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros(4)  # xyxy\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#mask-operations","title":"Mask Operations","text":""},{"location":"reference/ops/#resample_segments","title":"resample_segments","text":"<p>Inputs a list of segments (n,2) and returns a list of segments (n,2) up-sampled to n points each.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>list</code> <p>a list of (n,2) arrays, where n is the number of points in the segment.</p> required <code>n</code> <code>int</code> <p>number of points to resample the segment to. Defaults to 1000</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>segments</code> <code>list</code> <p>the resampled segments.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def resample_segments(segments, n=1000):\n\"\"\"\n    Inputs a list of segments (n,2) and returns a list of segments (n,2) up-sampled to n points each.\n    Args:\n      segments (list): a list of (n,2) arrays, where n is the number of points in the segment.\n      n (int): number of points to resample the segment to. Defaults to 1000\n    Returns:\n      segments (list): the resampled segments.\n    \"\"\"\nfor i, s in enumerate(segments):\ns = np.concatenate((s, s[0:1, :]), axis=0)\nx = np.linspace(0, len(s) - 1, n)\nxp = np.arange(len(s))\nsegments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy\nreturn segments\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#crop_mask","title":"crop_mask","text":"<p>It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>torch.Tensor</code> <p>[h, w, n] tensor of masks</p> required <code>boxes</code> <code>torch.Tensor</code> <p>[n, 4] tensor of bbox coordinates in relative point form</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The masks are being cropped to the bounding box.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def crop_mask(masks, boxes):\n\"\"\"\n    It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box\n    Args:\n      masks (torch.Tensor): [h, w, n] tensor of masks\n      boxes (torch.Tensor): [n, 4] tensor of bbox coordinates in relative point form\n    Returns:\n      (torch.Tensor): The masks are being cropped to the bounding box.\n    \"\"\"\nn, h, w = masks.shape\nx1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(1,1,n)\nr = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # rows shape(1,w,1)\nc = torch.arange(h, device=masks.device, dtype=x1.dtype)[None, :, None]  # cols shape(h,1,1)\nreturn masks * ((r &gt;= x1) * (r &lt; x2) * (c &gt;= y1) * (c &lt; y2))\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#process_mask_upsample","title":"process_mask_upsample","text":"<p>It takes the output of the mask head, and applies the mask to the bounding boxes. This produces masks of higher quality but is slower.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>torch.Tensor</code> <p>[mask_dim, mask_h, mask_w]</p> required <code>masks_in</code> <code>torch.Tensor</code> <p>[n, mask_dim], n is number of masks after nms</p> required <code>bboxes</code> <code>torch.Tensor</code> <p>[n, 4], n is number of masks after nms</p> required <code>shape</code> <code>tuple</code> <p>the size of the input image (h,w)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The upsampled masks.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def process_mask_upsample(protos, masks_in, bboxes, shape):\n\"\"\"\n    It takes the output of the mask head, and applies the mask to the bounding boxes. This produces masks of higher\n    quality but is slower.\n    Args:\n      protos (torch.Tensor): [mask_dim, mask_h, mask_w]\n      masks_in (torch.Tensor): [n, mask_dim], n is number of masks after nms\n      bboxes (torch.Tensor): [n, 4], n is number of masks after nms\n      shape (tuple): the size of the input image (h,w)\n    Returns:\n      (torch.Tensor): The upsampled masks.\n    \"\"\"\nc, mh, mw = protos.shape  # CHW\nmasks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)\nmasks = F.interpolate(masks[None], shape, mode='bilinear', align_corners=False)[0]  # CHW\nmasks = crop_mask(masks, bboxes)  # CHW\nreturn masks.gt_(0.5)\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#process_mask","title":"process_mask","text":"<p>It takes the output of the mask head, and applies the mask to the bounding boxes. This is faster but produces downsampled quality of mask</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>torch.Tensor</code> <p>[mask_dim, mask_h, mask_w]</p> required <code>masks_in</code> <code>torch.Tensor</code> <p>[n, mask_dim], n is number of masks after nms</p> required <code>bboxes</code> <code>torch.Tensor</code> <p>[n, 4], n is number of masks after nms</p> required <code>shape</code> <code>tuple</code> <p>the size of the input image (h,w)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The processed masks.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def process_mask(protos, masks_in, bboxes, shape, upsample=False):\n\"\"\"\n    It takes the output of the mask head, and applies the mask to the bounding boxes. This is faster but produces\n    downsampled quality of mask\n    Args:\n      protos (torch.Tensor): [mask_dim, mask_h, mask_w]\n      masks_in (torch.Tensor): [n, mask_dim], n is number of masks after nms\n      bboxes (torch.Tensor): [n, 4], n is number of masks after nms\n      shape (tuple): the size of the input image (h,w)\n    Returns:\n      (torch.Tensor): The processed masks.\n    \"\"\"\nc, mh, mw = protos.shape  # CHW\nih, iw = shape\nmasks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)  # CHW\ndownsampled_bboxes = bboxes.clone()\ndownsampled_bboxes[:, 0] *= mw / iw\ndownsampled_bboxes[:, 2] *= mw / iw\ndownsampled_bboxes[:, 3] *= mh / ih\ndownsampled_bboxes[:, 1] *= mh / ih\nmasks = crop_mask(masks, downsampled_bboxes)  # CHW\nif upsample:\nmasks = F.interpolate(masks[None], shape, mode='bilinear', align_corners=False)[0]  # CHW\nreturn masks.gt_(0.5)\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#process_mask_native","title":"process_mask_native","text":"<p>It takes the output of the mask head, and crops it after upsampling to the bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>torch.Tensor</code> <p>[mask_dim, mask_h, mask_w]</p> required <code>masks_in</code> <code>torch.Tensor</code> <p>[n, mask_dim], n is number of masks after nms</p> required <code>bboxes</code> <code>torch.Tensor</code> <p>[n, 4], n is number of masks after nms</p> required <code>shape</code> <code>tuple</code> <p>the size of the input image (h,w)</p> required <p>Returns:</p> Name Type Description <code>masks</code> <code>torch.Tensor</code> <p>The returned masks with dimensions [h, w, n]</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def process_mask_native(protos, masks_in, bboxes, shape):\n\"\"\"\n    It takes the output of the mask head, and crops it after upsampling to the bounding boxes.\n    Args:\n      protos (torch.Tensor): [mask_dim, mask_h, mask_w]\n      masks_in (torch.Tensor): [n, mask_dim], n is number of masks after nms\n      bboxes (torch.Tensor): [n, 4], n is number of masks after nms\n      shape (tuple): the size of the input image (h,w)\n    Returns:\n      masks (torch.Tensor): The returned masks with dimensions [h, w, n]\n    \"\"\"\nc, mh, mw = protos.shape  # CHW\nmasks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)\ngain = min(mh / shape[0], mw / shape[1])  # gain  = old / new\npad = (mw - shape[1] * gain) / 2, (mh - shape[0] * gain) / 2  # wh padding\ntop, left = int(pad[1]), int(pad[0])  # y, x\nbottom, right = int(mh - pad[1]), int(mw - pad[0])\nmasks = masks[:, top:bottom, left:right]\nmasks = F.interpolate(masks[None], shape, mode='bilinear', align_corners=False)[0]  # CHW\nmasks = crop_mask(masks, bboxes)  # CHW\nreturn masks.gt_(0.5)\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#scale_segments","title":"scale_segments","text":"<p>Rescale segment coordinates (xyxy) from img1_shape to img0_shape</p> <p>Parameters:</p> Name Type Description Default <code>img1_shape</code> <code>tuple</code> <p>The shape of the image that the segments are from.</p> required <code>segments</code> <code>torch.Tensor</code> <p>the segments to be scaled</p> required <code>img0_shape</code> <code>tuple</code> <p>the shape of the image that the segmentation is being applied to</p> required <code>ratio_pad</code> <code>tuple</code> <p>the ratio of the image size to the padded image size.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, the coordinates will be normalized to the range [0, 1]. Defaults to False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>segments</code> <code>torch.Tensor</code> <p>the segmented image.</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def scale_segments(img1_shape, segments, img0_shape, ratio_pad=None, normalize=False):\n\"\"\"\n    Rescale segment coordinates (xyxy) from img1_shape to img0_shape\n    Args:\n      img1_shape (tuple): The shape of the image that the segments are from.\n      segments (torch.Tensor): the segments to be scaled\n      img0_shape (tuple): the shape of the image that the segmentation is being applied to\n      ratio_pad (tuple): the ratio of the image size to the padded image size.\n      normalize (bool): If True, the coordinates will be normalized to the range [0, 1]. Defaults to False\n    Returns:\n      segments (torch.Tensor): the segmented image.\n    \"\"\"\nif ratio_pad is None:  # calculate from img0_shape\ngain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\npad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\nelse:\ngain = ratio_pad[0][0]\npad = ratio_pad[1]\nsegments[:, 0] -= pad[0]  # x padding\nsegments[:, 1] -= pad[1]  # y padding\nsegments /= gain\nclip_segments(segments, img0_shape)\nif normalize:\nsegments[:, 0] /= img0_shape[1]  # width\nsegments[:, 1] /= img0_shape[0]  # height\nreturn segments\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#masks2segments","title":"masks2segments","text":"<p>It takes a list of masks(n,h,w) and returns a list of segments(n,xy)</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>torch.Tensor</code> <p>the output of the model, which is a tensor of shape (batch_size, 160, 160)</p> required <code>strategy</code> <code>str</code> <p>'concat' or 'largest'. Defaults to largest</p> <code>'largest'</code> <p>Returns:</p> Name Type Description <code>segments</code> <code>List</code> <p>list of segment masks</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def masks2segments(masks, strategy='largest'):\n\"\"\"\n    It takes a list of masks(n,h,w) and returns a list of segments(n,xy)\n    Args:\n      masks (torch.Tensor): the output of the model, which is a tensor of shape (batch_size, 160, 160)\n      strategy (str): 'concat' or 'largest'. Defaults to largest\n    Returns:\n      segments (List): list of segment masks\n    \"\"\"\nsegments = []\nfor x in masks.int().cpu().numpy().astype('uint8'):\nc = cv2.findContours(x, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\nif c:\nif strategy == 'concat':  # concatenate all segments\nc = np.concatenate([x.reshape(-1, 2) for x in c])\nelif strategy == 'largest':  # select largest segment\nc = np.array(c[np.array([len(x) for x in c]).argmax()]).reshape(-1, 2)\nelse:\nc = np.zeros((0, 2))  # no segments found\nsegments.append(c.astype('float32'))\nreturn segments\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/ops/#clip_segments","title":"clip_segments","text":"<p>It takes a list of line segments (x1,y1,x2,y2) and clips them to the image shape (height, width)</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>list</code> <p>a list of segments, each segment is a list of points, each point is a list of x,y</p> required <p>coordinates   shape (tuple): the shape of the image</p> Source code in <code>ultralytics/yolo/utils/ops.py</code> <pre><code>def clip_segments(segments, shape):\n\"\"\"\n    It takes a list of line segments (x1,y1,x2,y2) and clips them to the image shape (height, width)\n    Args:\n      segments (list): a list of segments, each segment is a list of points, each point is a list of x,y\n    coordinates\n      shape (tuple): the shape of the image\n    \"\"\"\nif isinstance(segments, torch.Tensor):  # faster individually\nsegments[:, 0].clamp_(0, shape[1])  # x\nsegments[:, 1].clamp_(0, shape[0])  # y\nelse:  # np.array (faster grouped)\nsegments[:, 0] = segments[:, 0].clip(0, shape[1])  # x\nsegments[:, 1] = segments[:, 1].clip(0, shape[0])  # y\n</code></pre> <p>handler: python options: show_source: false show_root_toc_entry: false</p>"},{"location":"reference/results/","title":"Results","text":""},{"location":"reference/results/#results-api-reference","title":"Results API Reference","text":"<p>A class for storing and manipulating inference results.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>Boxes</code> <p>A Boxes object containing the detection bounding boxes.</p> <code>None</code> <code>masks</code> <code>Masks</code> <p>A Masks object containing the detection masks.</p> <code>None</code> <code>probs</code> <code>torch.Tensor</code> <p>A tensor containing the detection class probabilities.</p> <code>None</code> <code>orig_shape</code> <code>tuple</code> <p>Original image size.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>boxes</code> <code>Boxes</code> <p>A Boxes object containing the detection bounding boxes.</p> <code>masks</code> <code>Masks</code> <p>A Masks object containing the detection masks.</p> <code>probs</code> <code>torch.Tensor</code> <p>A tensor containing the detection class probabilities.</p> <code>orig_shape</code> <code>tuple</code> <p>Original image size.</p> <code>data</code> <code>torch.Tensor</code> <p>The raw masks tensor</p> Source code in <code>ultralytics/yolo/engine/results.py</code> <pre><code>class Results:\n\"\"\"\n        A class for storing and manipulating inference results.\n        Args:\n            boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n            masks (Masks, optional): A Masks object containing the detection masks.\n            probs (torch.Tensor, optional): A tensor containing the detection class probabilities.\n            orig_shape (tuple, optional): Original image size.\n        Attributes:\n            boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n            masks (Masks, optional): A Masks object containing the detection masks.\n            probs (torch.Tensor, optional): A tensor containing the detection class probabilities.\n            orig_shape (tuple, optional): Original image size.\n            data (torch.Tensor): The raw masks tensor\n        \"\"\"\ndef __init__(self, boxes=None, masks=None, probs=None, orig_shape=None) -&gt; None:\nself.boxes = Boxes(boxes, orig_shape) if boxes is not None else None  # native size boxes\nself.masks = Masks(masks, orig_shape) if masks is not None else None  # native size or imgsz masks\nself.probs = probs if probs is not None else None\nself.orig_shape = orig_shape\nself.comp = [\"boxes\", \"masks\", \"probs\"]\ndef pandas(self):\npass\n# TODO masks.pandas + boxes.pandas + cls.pandas\ndef __getitem__(self, idx):\nr = Results(orig_shape=self.orig_shape)\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nsetattr(r, item, getattr(self, item)[idx])\nreturn r\ndef cpu(self):\nr = Results(orig_shape=self.orig_shape)\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nsetattr(r, item, getattr(self, item).cpu())\nreturn r\ndef numpy(self):\nr = Results(orig_shape=self.orig_shape)\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nsetattr(r, item, getattr(self, item).numpy())\nreturn r\ndef cuda(self):\nr = Results(orig_shape=self.orig_shape)\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nsetattr(r, item, getattr(self, item).cuda())\nreturn r\ndef to(self, *args, **kwargs):\nr = Results(orig_shape=self.orig_shape)\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nsetattr(r, item, getattr(self, item).to(*args, **kwargs))\nreturn r\ndef __len__(self):\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nreturn len(getattr(self, item))\ndef __str__(self):\nstr_out = \"\"\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nstr_out = str_out + getattr(self, item).__str__()\nreturn str_out\ndef __repr__(self):\nstr_out = \"\"\nfor item in self.comp:\nif getattr(self, item) is None:\ncontinue\nstr_out = str_out + getattr(self, item).__repr__()\nreturn str_out\ndef __getattr__(self, attr):\nname = self.__class__.__name__\nraise AttributeError(f\"\"\"\n            '{name}' object has no attribute '{attr}'. Valid '{name}' object attributes and properties are:\n            Attributes:\n                boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n                masks (Masks, optional): A Masks object containing the detection masks.\n                probs (torch.Tensor, optional): A tensor containing the detection class probabilities.\n                orig_shape (tuple, optional): Original image size.\n            \"\"\")\n</code></pre>"},{"location":"reference/results/#boxes-api-reference","title":"Boxes API Reference","text":"<p>A class for storing and manipulating detection boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>torch.Tensor) or (numpy.ndarray</code> <p>A tensor or numpy array containing the detection boxes, with shape (num_boxes, 6). The last two columns should contain confidence and class values.</p> required <code>orig_shape</code> <code>tuple</code> <p>Original image size, in the format (height, width).</p> required <p>Attributes:</p> Name Type Description <code>boxes</code> <code>torch.Tensor) or (numpy.ndarray</code> <p>A tensor or numpy array containing the detection boxes, with shape (num_boxes, 6).</p> <code>orig_shape</code> <code>torch.Tensor) or (numpy.ndarray</code> <p>Original image size, in the format (height, width).</p> Properties <p>xyxy (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format. conf (torch.Tensor) or (numpy.ndarray): The confidence values of the boxes. cls (torch.Tensor) or (numpy.ndarray): The class values of the boxes. xywh (torch.Tensor) or (numpy.ndarray): The boxes in xywh format. xyxyn (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format normalized by original image size. xywhn (torch.Tensor) or (numpy.ndarray): The boxes in xywh format normalized by original image size. data (torch.Tensor): The raw bboxes tensor</p> Source code in <code>ultralytics/yolo/engine/results.py</code> <pre><code>class Boxes:\n\"\"\"\n    A class for storing and manipulating detection boxes.\n    Args:\n        boxes (torch.Tensor) or (numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6). The last two columns should contain confidence and class values.\n        orig_shape (tuple): Original image size, in the format (height, width).\n    Attributes:\n        boxes (torch.Tensor) or (numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6).\n        orig_shape (torch.Tensor) or (numpy.ndarray): Original image size, in the format (height, width).\n    Properties:\n        xyxy (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format.\n        conf (torch.Tensor) or (numpy.ndarray): The confidence values of the boxes.\n        cls (torch.Tensor) or (numpy.ndarray): The class values of the boxes.\n        xywh (torch.Tensor) or (numpy.ndarray): The boxes in xywh format.\n        xyxyn (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format normalized by original image size.\n        xywhn (torch.Tensor) or (numpy.ndarray): The boxes in xywh format normalized by original image size.\n        data (torch.Tensor): The raw bboxes tensor\n    \"\"\"\ndef __init__(self, boxes, orig_shape) -&gt; None:\nif boxes.ndim == 1:\nboxes = boxes[None, :]\nassert boxes.shape[-1] == 6  # xyxy, conf, cls\nself.boxes = boxes\nself.orig_shape = torch.as_tensor(orig_shape, device=boxes.device) if isinstance(boxes, torch.Tensor) \\\n            else np.asarray(orig_shape)\n@property\ndef xyxy(self):\nreturn self.boxes[:, :4]\n@property\ndef conf(self):\nreturn self.boxes[:, -2]\n@property\ndef cls(self):\nreturn self.boxes[:, -1]\n@property\n@lru_cache(maxsize=2)  # maxsize 1 should suffice\ndef xywh(self):\nreturn ops.xyxy2xywh(self.xyxy)\n@property\n@lru_cache(maxsize=2)\ndef xyxyn(self):\nreturn self.xyxy / self.orig_shape[[1, 0, 1, 0]]\n@property\n@lru_cache(maxsize=2)\ndef xywhn(self):\nreturn self.xywh / self.orig_shape[[1, 0, 1, 0]]\ndef cpu(self):\nboxes = self.boxes.cpu()\nreturn Boxes(boxes, self.orig_shape)\ndef numpy(self):\nboxes = self.boxes.numpy()\nreturn Boxes(boxes, self.orig_shape)\ndef cuda(self):\nboxes = self.boxes.cuda()\nreturn Boxes(boxes, self.orig_shape)\ndef to(self, *args, **kwargs):\nboxes = self.boxes.to(*args, **kwargs)\nreturn Boxes(boxes, self.orig_shape)\ndef pandas(self):\nLOGGER.info('results.pandas() method not yet implemented')\n'''\n        new = copy(self)  # return copy\n        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns\n        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns\n        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):\n            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update\n            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])\n        return new\n        '''\n@property\ndef shape(self):\nreturn self.boxes.shape\n@property\ndef data(self):\nreturn self.boxes\ndef __len__(self):  # override len(results)\nreturn len(self.boxes)\ndef __str__(self):\nreturn self.boxes.__str__()\ndef __repr__(self):\nreturn (f\"Ultralytics YOLO {self.__class__} masks\\n\" + f\"type: {type(self.boxes)}\\n\" +\nf\"shape: {self.boxes.shape}\\n\" + f\"dtype: {self.boxes.dtype}\\n + {self.boxes.__repr__()}\")\ndef __getitem__(self, idx):\nboxes = self.boxes[idx]\nreturn Boxes(boxes, self.orig_shape)\ndef __getattr__(self, attr):\nname = self.__class__.__name__\nraise AttributeError(f\"\"\"\n            '{name}' object has no attribute '{attr}'. Valid '{name}' object attributes and properties are:\n            Attributes:\n                boxes (torch.Tensor) or (numpy.ndarray): A tensor or numpy array containing the detection boxes,\n                    with shape (num_boxes, 6).\n                orig_shape (torch.Tensor) or (numpy.ndarray): Original image size, in the format (height, width).\n            Properties:\n                xyxy (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format.\n                conf (torch.Tensor) or (numpy.ndarray): The confidence values of the boxes.\n                cls (torch.Tensor) or (numpy.ndarray): The class values of the boxes.\n                xywh (torch.Tensor) or (numpy.ndarray): The boxes in xywh format.\n                xyxyn (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format normalized by original image size.\n                xywhn (torch.Tensor) or (numpy.ndarray): The boxes in xywh format normalized by original image size.\n            \"\"\")\n</code></pre>"},{"location":"reference/results/#masks-api-reference","title":"Masks API Reference","text":"<p>A class for storing and manipulating detection masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>torch.Tensor</code> <p>A tensor containing the detection masks, with shape (num_masks, height, width).</p> required <code>orig_shape</code> <code>tuple</code> <p>Original image size, in the format (height, width).</p> required <p>Attributes:</p> Name Type Description <code>masks</code> <code>torch.Tensor</code> <p>A tensor containing the detection masks, with shape (num_masks, height, width).</p> <code>orig_shape</code> <code>tuple</code> <p>Original image size, in the format (height, width).</p> Properties <p>segments (list): A list of segments which includes x,y,w,h,label,confidence, and mask of each detection masks.</p> Source code in <code>ultralytics/yolo/engine/results.py</code> <pre><code>class Masks:\n\"\"\"\n    A class for storing and manipulating detection masks.\n    Args:\n        masks (torch.Tensor): A tensor containing the detection masks, with shape (num_masks, height, width).\n        orig_shape (tuple): Original image size, in the format (height, width).\n    Attributes:\n        masks (torch.Tensor): A tensor containing the detection masks, with shape (num_masks, height, width).\n        orig_shape (tuple): Original image size, in the format (height, width).\n    Properties:\n        segments (list): A list of segments which includes x,y,w,h,label,confidence, and mask of each detection masks.\n    \"\"\"\ndef __init__(self, masks, orig_shape) -&gt; None:\nself.masks = masks  # N, h, w\nself.orig_shape = orig_shape\n@property\n@lru_cache(maxsize=1)\ndef segments(self):\nreturn [\nops.scale_segments(self.masks.shape[1:], x, self.orig_shape, normalize=True)\nfor x in ops.masks2segments(self.masks)]\n@property\ndef shape(self):\nreturn self.masks.shape\n@property\ndef data(self):\nreturn self.masks\ndef cpu(self):\nmasks = self.masks.cpu()\nreturn Masks(masks, self.orig_shape)\ndef numpy(self):\nmasks = self.masks.numpy()\nreturn Masks(masks, self.orig_shape)\ndef cuda(self):\nmasks = self.masks.cuda()\nreturn Masks(masks, self.orig_shape)\ndef to(self, *args, **kwargs):\nmasks = self.masks.to(*args, **kwargs)\nreturn Masks(masks, self.orig_shape)\ndef __len__(self):  # override len(results)\nreturn len(self.masks)\ndef __str__(self):\nreturn self.masks.__str__()\ndef __repr__(self):\nreturn (f\"Ultralytics YOLO {self.__class__} masks\\n\" + f\"type: {type(self.masks)}\\n\" +\nf\"shape: {self.masks.shape}\\n\" + f\"dtype: {self.masks.dtype}\\n + {self.masks.__repr__()}\")\ndef __getitem__(self, idx):\nmasks = self.masks[idx]\nreturn Masks(masks, self.im_shape, self.orig_shape)\ndef __getattr__(self, attr):\nname = self.__class__.__name__\nraise AttributeError(f\"\"\"\n            '{name}' object has no attribute '{attr}'. Valid '{name}' object attributes and properties are:\n            Attributes:\n                masks (torch.Tensor): A tensor containing the detection masks, with shape (num_masks, height, width).\n                orig_shape (tuple): Original image size, in the format (height, width).\n            Properties:\n                segments (list): A list of segments which includes x,y,w,h,label,confidence, and mask of each detection masks.\n            \"\"\")\n</code></pre>"},{"location":"tasks/classification/","title":"Classification","text":"<p>Image classification is the simplest of the three tasks and involves classifying an entire image into one of a set of predefined classes.</p> <p></p> <p>The output of an image classifier is a single class label and a confidence score. Image classification is useful when you need to know only what class an image belongs to and don't need to know where objects of that class are located or what their exact shape is.</p> <p>Tip</p> <p>YOLOv8 classification models use the <code>-cls</code> suffix, i.e. <code>yolov8n-cls.pt</code> and are pretrained on ImageNet.</p> <p>Models</p>"},{"location":"tasks/classification/#train","title":"Train","text":"<p>Train YOLOv8n-cls on the MNIST160 dataset for 100 epochs at image size 64. For a full list of available arguments see the Configuration page.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-cls.yaml\")  # build a new model from scratch\nmodel = YOLO(\"yolov8n-cls.pt\")  # load a pretrained model (recommended for training)\n# Train the model\nmodel.train(data=\"mnist160\", epochs=100, imgsz=64)\n</code></pre> <pre><code>yolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classification/#val","title":"Val","text":"<p>Validate trained YOLOv8n-cls model accuracy on the MNIST160 dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-cls.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.top1   # top1 accuracy\nmetrics.top5   # top5 accuracy\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # val official model\nyolo classify val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"tasks/classification/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n-cls model to run predictions on images.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-cls.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with official model\nyolo classify predict model=path/to/best.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with custom model\n</code></pre> <p>Read more details of <code>predict</code> in our Predict page.</p>"},{"location":"tasks/classification/#export","title":"Export","text":"<p>Export a YOLOv8n-cls model to a different format like ONNX, CoreML, etc.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-cls.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained\n# Export the model\nmodel.export(format=\"onnx\")\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8-cls export formats include:</p> Format <code>format=</code> Model PyTorch - <code>yolov8n-cls.pt</code> TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlmodel</code> TensorFlow SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> TensorFlow GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> TensorFlow Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> TensorFlow Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> TensorFlow.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code>"},{"location":"tasks/detection/","title":"Detection","text":"<p>Object detection is a task that involves identifying the location and class of objects in an image or video stream.</p> <p></p> <p>The output of an object detector is a set of bounding boxes that enclose the objects in the image, along with class labels and confidence scores for each box. Object detection is a good choice when you need to identify objects of interest in a scene, but don't need to know exactly where the object is or its exact shape.</p> <p>Tip</p> <p>YOLOv8 detection models have no suffix and are the default YOLOv8 models, i.e. <code>yolov8n.pt</code> and are pretrained on COCO.</p> <p>Models</p>"},{"location":"tasks/detection/#train","title":"Train","text":"<p>Train YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see the Configuration page.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\nmodel = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n# Train the model\nmodel.train(data=\"coco128.yaml\", epochs=100, imgsz=640)\n</code></pre> <pre><code>yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detection/#val","title":"Val","text":"<p>Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # a list contains map50-95 of each category\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # val official model\nyolo detect val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"tasks/detection/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n model to run predictions on images.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with official model\nyolo detect predict model=path/to/best.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with custom model\n</code></pre> <p>Read more details of <code>predict</code> in our Predict page.</p>"},{"location":"tasks/detection/#export","title":"Export","text":"<p>Export a YOLOv8n model to a different format like ONNX, CoreML, etc.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained\n# Export the model\nmodel.export(format=\"onnx\")\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8 export formats include:</p> Format <code>format=</code> Model PyTorch - <code>yolov8n.pt</code> TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> CoreML <code>coreml</code> <code>yolov8n.mlmodel</code> TensorFlow SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> TensorFlow GraphDef <code>pb</code> <code>yolov8n.pb</code> TensorFlow Lite <code>tflite</code> <code>yolov8n.tflite</code> TensorFlow Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> TensorFlow.js <code>tfjs</code> <code>yolov8n_web_model/</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code>"},{"location":"tasks/segmentation/","title":"Segmentation","text":"<p>Instance segmentation goes a step further than object detection and involves identifying individual objects in an image and segmenting them from the rest of the image.</p> <p></p> <p>The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence scores for each object. Instance segmentation is useful when you need to know not only where objects are in an image, but also what their exact shape is.</p> <p>Tip</p> <p>YOLOv8 segmentation models use the <code>-seg</code> suffix, i.e. <code>yolov8n-seg.pt</code> and are pretrained on COCO.</p> <p>Models</p>"},{"location":"tasks/segmentation/#train","title":"Train","text":"<p>Train YOLOv8n-seg on the COCO128-seg dataset for 100 epochs at image size 640. For a full list of available arguments see the Configuration page.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-seg.yaml\")  # build a new model from scratch\nmodel = YOLO(\"yolov8n-seg.pt\")  # load a pretrained model (recommended for training)\n# Train the model\nmodel.train(data=\"coco128-seg.yaml\", epochs=100, imgsz=640)\n</code></pre> <pre><code>yolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segmentation/#val","title":"Val","text":"<p>Validate trained YOLOv8n-seg model accuracy on the COCO128-seg dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-seg.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map    # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps   # a list contains map50-95(B) of each category\nmetrics.seg.map    # map50-95(M)\nmetrics.seg.map50  # map50(M)\nmetrics.seg.map75  # map75(M)\nmetrics.seg.maps   # a list contains map50-95(M) of each category\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # val official model\nyolo segment val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"tasks/segmentation/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n-seg model to run predictions on images.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-seg.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with official model\nyolo segment predict model=path/to/best.pt source=\"https://ultralytics.com/images/bus.jpg\"  # predict with custom model\n</code></pre> <p>Read more details of <code>predict</code> in our Predict page.</p>"},{"location":"tasks/segmentation/#export","title":"Export","text":"<p>Export a YOLOv8n-seg model to a different format like ONNX, CoreML, etc.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolov8n-seg.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained\n# Export the model\nmodel.export(format=\"onnx\")\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8-seg export formats include:</p> Format <code>format=</code> Model PyTorch - <code>yolov8n-seg.pt</code> TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlmodel</code> TensorFlow SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> TensorFlow GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> TensorFlow Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> TensorFlow Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> TensorFlow.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code>"}]}